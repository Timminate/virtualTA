{"conversations": [
    ["number inherits methods instance variables number implements comparable integer, implements interface compiler checks, methods class implements, java interface specifies, ternary operator, methods example source code comparable interface, integer returns int class, object parameter returns int example source code class, type parameter, package interface definition, interface aside implementation compareto", "A Java \"interface\" specifies a set of methods; any class that implements this \"interface\" has to provide these methods. For example, here is the source code for \"Comparable\", which is an \"interface\" defined in the package \"java.lang\": This \"interface\" definition uses a type parameter, \"T\", which makes \"Comparable\" a **generic type**.  In order to implement this \"interface\", a class has to\n- Specify the type \"T\" refers to, and\n- Provide a method named \"compareTo\" that takes an object as a parameter and returns an \"int\".\nFor example, here's the source code for \"java.lang.Integer\": This class extends \"Number\", so it inherits the methods and instance variables of \"Number\"; and it implements \"Comparable<Integer>\", so it provides a method named \"compareTo\" that takes an \"Integer\" and returns an \"int\". When a class declares that it implements an \"interface\", the compiler checks that it provides all methods defined by the \"interface\". As an aside, this implementation of \"compareTo\" uses the \"ternary operator\", sometimes written \"?:\".  If you are not familiar with it, you can read about it at http://thinkdast.com/ternary."], 
    ["general idea interface java interface use library code, internal list object main contains lines, point work, new linkedlist getter method, list class implements", "The Java Collections Framework (JCF) defines an \"interface\" called \"List\" and provides two implementations, \"ArrayList\" and \"LinkedList\". The \"interface\" defines what it means to be a \"List\"; any class that implements this \"interface\" has to provide a particular set of methods, including \"add\", \"get\", \"remove\", and about 20 more. \"ArrayList\" and \"LinkedList\" provide these methods, so they can be used interchangeably. A method written to work with a \"List\" will work with an \"ArrayList\", \"LinkedList\", or any other object that implements \"List\". Here's a contrived example that demonstrates the point: \"ListClientExample\" doesn't do anything useful, but it has the essential elements of a class that **encapsulates** a \"List\"; that is, it contains a \"List\" as an instance variable.  I'll use this class to make a point, and then you'll work with it in the first exercise. The \"ListClientExample\" constructor initializes \"list\" by **instantiating** (that is, creating) a new \"LinkedList\"; the getter method called \"getList\" returns a reference to the internal \"List\" object; and \"main\" contains a few lines of code to test these methods. The important thing about this example is that it uses \"List\" whenever possible and avoids specifying \"LinkedList\" or \"ArrayList\" unless it is necessary. For example, the instance variable is declared to be a \"List\", and \"getList\" returns a \"List\", but neither specifies which kind of list. If you change your mind and decide to use an \"ArrayList\", you only have to change the constructor; you don't have to make any other changes. This style is called **interface-based programming**, or more casually, \"programming to an interface\" (see http://thinkdast.com/interbaseprog). Here we are talking about the general idea of an interface, not a Java \"interface\". When you use a library, your code should only depend on the interface, like \"List\".  It should not depend on a specific implementation, like \"ArrayList\". That way, if the implementation changes in the future, the code that uses it will still work. On the other hand, if the interface changes, the code that depends on it has to change, too.  That's why library developers avoid changing interfaces unless absolutely necessary."], 
    ["element array, addition constant time operations, example implementation simple algorithm, simple let count number comparisons, indexlowest traverses entire array total number comparisons", "For example, here's an implementation of a simple algorithm called selection sort (see http://thinkdast.com/selectsort): The first method, \"swapElements\", swaps two elements of the array. Reading and writing elements are constant time operations, because if we know the size of the elements and the location of the first, we can compute the location of any other element with one multiplication and one addition, and those are constant time operations. Since everything in \"swapElements\" is constant time, the whole method is constant time. The second method, \"indexLowest\", finds the index of the smallest element of the array starting at a given index, \"start\". Each time through the loop, it accesses two elements of the array and performs one comparison. Since these are all constant time operations, it doesn't really matter which ones we count. To keep it simple, let's count the number of comparisons.\n- If \"start\" is 0, \"indexLowest\" traverses the entire array, and the total number of comparisons is the length of the array, which I'll call n.\n- If \"start\" is 1, the number of comparisons is n-1.\n- In general, the number of comparisons is n - \"start\", so \"indexLowest\" is linear.\nThe third method, \"selectionSort\", sorts the array. It loops from 0 to n-1, so the loop executes n times. Each time, it calls \"indexLowest\" and then performs a constant time operation, \"swapElements\". The first time \"indexLowest\" is called, it performs n comparisons. The second time, it performs n-1 comparisons, and so on. The total number of comparisons is n + n-1 + n-2 + ... + 1 + 0. The sum of this series is n(n+1)/2, which is proportional to n^2; and that means that \"selectionSort\" is quadratic. To get to the same result a different way, we can think of \"indexLowest\" as a nested loop. Each time we call \"indexLowest\", the number of operations is proportional to n. We call it n times, so the total number of operations is proportional to n^2."], ["efficient algorithms, linear f n g n n fact, linear operations, constant algorithm total run time linear using mean member f n g, quadratic f n", "All constant time algorithms belong to a set called O(1). So another way to say that an algorithm is constant time is to say that it is in O(1). Similarly, all linear algorithms belong to O(n), and all quadratic algorithms belong to O(n^2). This way of classifying algorithms is called \"big O notation\". NOTE: I am providing a casual definition of big O notation. For a more mathematical treatment, see http://thinkdast.com/bigo. This notation provides a convenient way to write general rules about how algorithms behave when we compose them. For example, if you perform a linear time algorithm followed by a constant algorithm, the total run time is linear. Using ∈ to mean \"is a member of\": If f ∈ O(n) and g ∈ O(1), f+g ∈ O(n). If you perform two linear operations, the total is still linear: If f ∈ O(n) and g ∈ O(n), f+g ∈ O(n). In fact, if you perform a linear operation any number of times, k, the total is linear, as long as k is a constant that does not depend on n. If f ∈ O(n) and k is a constant, kf ∈ O(n). But if you perform a linear operation n times, the result is quadratic: If f ∈ O(n), nf ∈ O(n^2). In general, we only care about the largest exponent of n. So if the total number of operations is 2n + 1, it belongs to O(n). The leading constant, 2, and the additive term, 1, are not important for this kind of analysis. Similarly, n^2 + 100n + 1000 is in O(n^2).  Don't be distracted by the big numbers! \"Order of growth\" is another name for the same idea.  An order of growth is a set of algorithms whose runtimes are in the same big O category; for example, all linear algorithms belong to the same order of growth because their runtimes are in O(n). In this context, an \"order\" is a group, like the Order of the Knights of the Round Table, which is a group of knights, not a way of lining them up. So you can imagine the Order of Linear Algorithms as a set of brave, chivalrous, and particularly efficient algorithms."], ["element end list loop, constant time, uses implementation, element end constant distance end, constant time next look linear methods example implementation indexof time loop indexof invokes equals", "For many methods, we can identify the order of growth by examining the code. For example, here's the implementation of \"get\" from \"MyArrayList\": Everything in \"get\" is constant time, so \"get\" is constant time. No problem. Now that we've classified \"get\", we can classify \"set\", which uses it. Here is our implementation of \"set\" from the previous exercise: One slightly clever part of this solution is that it does not check the bounds of the array explicitly; it takes advantage of \"get\", which raises an exception if the index is invalid. Everything in \"set\", including the invocation of \"get\", is constant time, so \"set\" is also constant time. Next we'll look at some linear methods. For example, here's my implementation of \"indexOf\": Each time through the loop, \"indexOf\" invokes \"equals\", so we have to classify \"equals\" first. Here it is: This method invokes \"target.equals\"; the runtime of this method might depend on the size of \"target\" or \"element\", but it probably doesn't depend on the size of the array, so we consider it constant time for purposes of analyzing \"indexOf\". Getting back to \"indexOf\", everything inside the loop is constant time, so the next question we have to consider is: how many times does the loop execute? If we get lucky, we might find the target object right away and return after testing only one element. If we are unlucky, we might have to test all of the elements. On average, we expect to test half of the elements, so this method is considered linear (except in the unlikely case that we know the target element is at the beginning of the array). The analysis of \"remove\" is similar. Here's my implementation: It uses \"get\", which is constant time, and then loops through the array, starting from \"index\". If we remove the element at the end of the list, the loop never runs and this method is constant time. If we remove the first element, we loop through all of the remaining elements, which is linear. So, again, this method is considered linear (except in the special case where we know the element is at the end or a constant distance from the end)."], ["element second time finds unused space array stores, elements next add copies, fourth time stores, array copies, element size array", "Here's a version of \"add\" that takes an index and an element as parameters: This two-parameter version, called \"add(int, E)\", uses the one-parameter version, called \"add(E)\", which puts the new element at the end. Then it shifts the other elements to the right, and puts the new element in the correct place. Before we can classify the two-parameter \"add(int, E)\", we have to classify the one-parameter \"add(E)\": The one-parameter version turns out to be hard to analyze. If there is an unused space in the array, it is constant time, but if we have to resize the array, it's linear because \"System.arraycopy\" takes time proportional to the size of the array.  So is {\\tt add} constant time or linear? We can classify this method by thinking about the average number of operations per add over a series of n adds. For simplicity, assume we start with an array that has room for 2 elements.\n- The first time we call add, it finds unused space in the array, so it stores 1 element.\n- The second time, it finds unused space in the array, so it stores 1 element.\n- The third time, we have to resize the array, copy 2 elements, and store 1 element. Now the size of the array is 4.\n- The fourth time stores 1 element.\n- The fifth time resizes the array, copies 4 elements, and stores 1 element. Now the size of the array is 8.\n- The next 3 adds store 3 elements.\n- The next add copies 8 and stores 1. Now the size is 16.\n- The next 7 adds store 7 elements."], ["constant time linear classify method, copied otherwise add, element second time finds unused space array stores, exponent n, get average number operations, finds unused space array stores, difference average case analysis, array copies, element size array, int e, http key idea", "And so on. Adding things up:\n- After 4 adds, we've stored 4 elements and copied 2.\n- After 8 adds, we've stored 8 elements and copied 6.\n- After 16 adds, we've stored 16 elements and copied 14."], ["strange algorithm, end list, exponent n, get average number operations, amount analysis work note patrick, difference average case analysis, analysis beginners, element size array, int e, indexof average sequence operations", "By now you should see the pattern: to do n adds, we have to store n elements and copy n-2. So the total number of operations is n + n - 2, which is 2n-2. To get the average number of operations per add, we divide the total by n; the result is 2 - 2/n. As n gets big, the second term, 2/n, gets small. Invoking the principle that we only care about the largest exponent of n, we can think of \"add\" as constant time. It might seem strange that an algorithm that is sometimes linear can be constant time on average. The key is that we double the length of the array each time it gets resized. That limits the number of times each element gets copied. Otherwise --- if we add a fixed amount to the length of the array, rather than multiplying by a fixed amount --- the analysis doesn't work. % NOTE: Patrick notes potential confusion in my use of average, which % was an average over a hypothetical set of inputs when we looked at % indexOf, and here is the average over a sequence of operations. % I am inclined to leave this alone on the grounds that it it more % confusing for experts (who know the difference between average % case analysis and amortized analysis) than for beginners (who will % not, I conjecture, be bothered). This way of classifying an algorithm, by computing the average time in a series of invocations, is called **amortized analysis**.  You can read more about it at http://thinkdast.com/amort. The key idea is that the extra cost of copying the array is spread, or \"amortized\", over a series of invocations. Now, if \"add(E)\" is constant time, what about \"add(int, E)\"? After calling \"add(E)\", it loops through part of the array and shifts elements. This loop is linear, except in the special case where we are adding at the end of the list. So \"add(int, E)\" is linear."], ["removeall implementation myarraylist time loop removeall invokes, many times, quadratic talk problem size careful size sizes, elements list, linear collection", "The last example we'll consider is \"removeAll\"; here's the implementation in \"MyArrayList\": Each time through the loop, \"removeAll\" invokes \"remove\", which is linear.  So it is tempting to think that \"removeAll\" is quadratic.  But that's not necessarily the case. In this method, the loop runs once for each element in \"collection\". If \"collection\" contains m elements and the list we are removing from contains n elements, this method is in O(nm). If the size of \"collection\" can be considered constant, \"removeAll\" is linear with respect to n. But if the size of the collection is proportional to n, \"removeAll\" is quadratic. For example, if \"collection\" always contains 100 or fewer elements, \"removeAll\" is linear. But if \"collection\" generally contains 1\\% of the elements in the list, \"removeAll\" is quadratic. When we talk about **problem size**, we have to be careful about which size, or sizes, we are talking about. This example demonstrates a pitfall of algorithm analysis: the tempting shortcut of counting loops.  If there is one loop, the algorithm is often\nlinear.  If there are two loops (one nested inside the other), the algorithm is often quadratic. But be careful! You have to think about how many times each loop runs. If the number of iterations is proportional to n for all loops, you can get away with just counting the loops. But if, as in this example, the number of iterations is not always proportional to n, you have to give it more thought."], ["listnode objects, several ways, list node, nodes contain references nodes, node class definition simple node listnode", "For the next exercise I provide a partial implementation of the \"List\" interface that uses a linked list to store the elements. If you are not familiar with linked lists, you can read about them at http://thinkdast.com/linkedlist, but this section provides a brief introduction. A data structure is \"linked\" if it is made up of objects, often called \"nodes\", that contain references to other nodes. In a linked\nlist, each node contains a reference to the next node in the list. Other linked structures include trees and graphs, in which nodes can contain references to more than one other node. Here's a class definition for a simple node: The \"ListNode\" object has two instance variables: \"data\" is a reference to some kind of \"Object\", and \"next\" is a reference to the next node in the list. In the last node in the list, by convention, \"next\" is \"null\". \"ListNode\" provides several constructors, allowing you to provide values for \"data\" and \"next\", or initialize them to the default value, \"null\". You can think of each \"ListNode\" as a list with a single element, but more generally, a list can contain any number of nodes. There are several ways to make a new list. A simple option is to create a set of \"ListNode\" objects, like this: And then link them up, like this: Alternatively, you can create a node and link it at the same time. For example, if you want to add a new node at the beginning of a list, you can do it like this: After this sequence of instructions, we have four nodes containing the \"Integer\"s 0, 1, 2, and 3 as data, linked up in increasing order. In the last node, the \"next\" field is \"null\"."], ["sequence instructions, partial implementation list interface, listnode integer instance variables, refer objects, object diagram variables, list store elements, next exercise", "In an object diagram, variables appear as names inside boxes, with arrows that show what they refer to.  Objects appear as boxes with their type on the outside (like \"ListNode\" and \"Integer\") and their instance variables on the inside."], ["scenes kind bug hard find, garbage collection, constant time invoke, unused nodes, classify clear method", "In \"MyArrayList\" from the previous exercise, the array grows if necessary, but it never shrinks. The array never gets garbage collected, and the elements don't get garbage collected until the list itself is destroyed. One advantage of the linked list implementation is that it shrinks when elements are removed, and the unused nodes can get garbage collected immediately. Here is my implementation of the \"clear\" method: When we set \"head\" to \"null\", we remove a reference to the first \"Node\". If there are no other references to that \"Node\" (and there shouldn't be), it will get garbage collected. At that point, the reference to the second \"Node\" is removed, so it gets garbage collected, too. This process continues until all nodes are collected. So how should we classify \"clear\"? The method itself contains two constant time operations, so it sure looks like it's constant time. But when you invoke it, you make the garbage collector do work that's proportional to the number of elements. So maybe we should consider it linear! This is an example of what is sometimes called a **performance bug**: a program that is correct in the sense that it does the right thing, but it doesn't belong to the order of growth we expected. In languages like Java that do a lot of work, like garbage collection, behind the scenes, this kind of bug can be hard to find."], ["find store element index, sure next node null case safe loop, new node, identify order growth, finally decrement size return element", "My implementation of \"indexOf\" is below. Read through it and see if you can identify its order of growth before you read the explanation. Initially \"node\" gets a copy of \"head\", so they both refer to the same \"Node\". The loop variable, \"i\", counts from 0 to \"size-1\".  Each time through the loop, we use \"equals\" to see if we've found the target. If so, we return \"i\" immediately. Otherwise we advance to the next \"Node\" in the list. Normally we would check to make sure the next \"Node\" is not \"null\", but in this case it is safe because the loop ends when we get to the end of the list (assuming \"size\" is consistent with the actual number of nodes in the list). If we get through the loop without finding the target, we return \"-1\". So what is the order of growth for this method?\n- Each time through the loop we invoke \"equals\", which is constant time (it might depend on the size of \"target\" or \"data\", but it doesn't depend on the size of the list). The other operations in the loop are also constant time.\n- The loop might run n times, because in the worse case, we might have to traverse the whole list.\nSo the runtime of this method is proportional to the length of the list. Next, here is my implementation of the two-parameter \"add\" method. Again, you should try to classify it before you read the explanation. If \"index==0\", we're adding the new \"Node\" at the beginning, so we handle that as a special case. Otherwise, we have to traverse the list to find the element at \"index-1\". We use the helper method \"getNode\": \"getNode\" checks whether \"index\" is out of bounds; if so, it throws an exception. Otherwise it traverses the list and returns the requested Node. Jumping back to \"add\", once we find the right \"Node\", we create the new \"Node\" and put it between \"node\" and \"node.next\". You might find it helpful to draw a diagram of this operation to make sure you understand it. So, what's the order of growth for \"add\"?\n- \"getNode\" is similar to \"indexOf\", and it is linear for the same reason.\n- In \"add\", everything before and after \"getNode\" is constant time.\nSo all together, \"add\" is linear. Finally, let's look at \"remove\": \"remove\" uses \"get\" to find and store the element at \"index\". Then it removes the \"Node\" that contained it. If \"index==0\", we handle that as a special case again. Otherwise we find the node at \"index-1\" and modify it to skip over \"node.next\" and link directly to \"node.next.next\". This effectively removes \"node.next\" from the list, and it can be garbage collected. Finally, we decrement \"size\" and return the element we retrieved at the beginning. So, what's the order of growth for \"remove\"? Everything in \"remove\" is constant time except \"get\" and \"getNode\", which are linear. So \"remove\" is linear. When people see two linear operations, they sometimes think the result is quadratic, but that only applies if one operation is nested inside the other. If you invoke one operation after the other, the runtimes add. If they are both in O(n), the sum is also in O(n)."], ["n linear operations myarraylist advantage, constant time, operations likely use java, table summarizes differences mylinkedlist myarraylist, implementations order growth implementation, operations mylinkedlist advantage", "The following table summarizes the differences between \"MyLinkedList\" and \"MyArrayList\", where \"1\" means O(1) or constant time and n means O(n) or linear.\nThe operations where \"MyArrayList\" has an advantage are adding at the end, removing from the end, getting and setting. The operations where \"MyLinkedList\" has an advantage are adding at the beginning and removing from the beginning. For the other operations, the two implementations are in the same order of growth. Which implementation is better? It depends on which operations you are likely to use the most. And that's why Java provides more than one implementation, because it depends."], ["timeable object title parameters profiler, measures runtimes plots results, start clock case, loop start endmillis, clear relationship problem size runtime code", "For the next exercise I provide a class called \"Profiler\" that contains code that runs a method with a range of problem sizes, measures runtimes, and plots the results. You will use \"Profiler\" to classify the performance of the \"add\" method for the Java implementations of \"ArrayList\" and \"LinkedList\". Here's an example that shows how to use the profiler: This method measures the time it takes to run \"add\" on an \"ArrayList\", which adds the new element at the end. I'll explain the code and then show the results. In order to use \"Profiler\", we need to create a \"Timeable\" object that provides two methods: \"setup\" and \"timeMe\". The \"setup\" method does whatever needs to be done before we start the clock; in this case it creates an empty list. Then \"timeMe\" does whatever operation we are trying to measure; in this case it adds n elements to the list. The code that creates \"timeable\" is an **anonymous class** that defines a new implementation of the \"Timeable\" interface and creates an instance of the new class at the same time. If you are not familiar with anonymous classes, you can read about them here: http://thinkdast.com/anonclass. But you don't need to know much for the next exercise; even if you are not comfortable with anonymous classes, you can copy and modify the example code. The next step is to create the \"Profiler\" object, passing the \"Timeable\" object and a title as parameters. The \"Profiler\" provides \"timingLoop\" which uses the \"Timeable\" object stored as an instance variable. It invokes the \"timeMe\" method on the \"Timeable\" object several times with a range of values of n. \"timingLoop\" takes two parameters:\n- \"startN\" is the value of n the timing loop should start at.\n- \"endMillis\" is a threshold in milliseconds. As \"timingLoop\" increases the problem size, the runtime increases; when the runtime exceeds this threshold, \"timingLoop\" stops.\nWhen you run the experiments, you might have to adjust these parameters. If \"startN\" is too low, the runtime might be too short to measure accurately. If \"endMillis\" is too low, you might not get enough data to see a clear relationship between problem size and runtime. This code is in \"ProfileListAdd.java\", which you'll run in the next exercise. When I ran it, I got this output: The first column is problem size, n; the second column is runtime in milliseconds. The first few measurements are pretty noisy; it might have been better to set \"startN\" around 64000. The result from \"timingLoop\" is an \"XYSeries\" that contains this data. If you pass this series to \"plotResults\", it generates a plot like the one in"], ["range problem, next section, measures runtimes plots results, timingloop uses, exceeds threshold timingloop stops, methods setup timeme setup method, enough data, method java implementations arraylist linkedlist example shows, column problem size n second column runtime milliseconds first measurements, clear relationship problem size runtime code, profiler method measures time", "The next section explains how to interpret it."], ["slope hand, straight line, total time, runtime versus n scale, log runtime log c k log n equation implies", "Based on our understanding of how \"ArrayList\" works, we expect the \"add\" method to take constant time when we add elements to the end. So the total time to add n elements should be linear. To test that theory, we could plot total runtime versus problem size, and we should see a straight line, at least for problem sizes that are big enough to measure accurately. Mathematically, we can write the function for that line: runtime = a + b n, where a is the intercept of the line and b is the slope. On the other hand, if \"add\" is linear, the total time for n adds would be quadratic. If we plot runtime versus problem size, we expect to see a parabola. Or mathematically, something like: runtime = a + b n + c n^2. With perfect data, we might be able to tell the difference between a straight line and a parabola, but if the measurements are noisy, it can be hard to tell. A better way to interpret noisy measurements is to plot runtime and problem size on a **log-log** scale. Why? Let's suppose that runtime is proportional to n^k, but we don't know what the exponent k is. We can write the relationship like this: runtime = a + b n + ... + c n^k. For large values of n, the term with the largest exponent is the most important, so: runtime ≈ c n^k, where ≈ means \"approximately equal\". Now, if we take the logarithm of both sides of this equation: log(runtime) ≈ log(c) + k log(n). This equation implies that if we plot runtime versus n on a log-log scale, we expect to see a straight line with intercept log(c) and slope k. We don't care much about the intercept, but the slope indicates the order of growth: if k=1, the algorithm is linear; if k=2, it's quadratic. Looking at the figure in the previous section, you can estimate the slope by eye. But when you call \"plotResults\" it computes a least squares fit to the data and prints the estimated slope. In this example: which is close to 1; and that suggests that the total time for n adds is linear, so each add is constant time, as expected. One important point: if you see a straight line on a graph like this, that does **not** mean that the algorithm is linear. If the run time is proportional to n^k for any exponent k, we expect to see a straight line with slope k. If the slope is close to 1, that suggests the algorithm is linear. If it is close to 2, it's probably quadratic."], ["previous exercise, new elements, additional data point timing results problem size, rather runtime proportional exponent k, tests performance", "In the previous exercise, we used \"Profiler.java\" to run various \"ArrayList\" and \"LinkedList\" operations with a range of problem sizes. We plotted runtime versus problem size on a log-log scale and estimated the slope of the resulting curve, which indicates the leading exponent of the relationship between runtime and problem size. For example, when we used the \"add\" method to add elements to the end of an \"ArrayList\", we found that the total time to perform n adds was proportional to n; that is, the estimated slope was close to 1. We concluded that performing n adds is in O(n), so on average the time for a single add is constant time, or O(1), which is what we expect based on algorithm analysis. % NOTE: Again, Patrick is concerned that my use of \"average\" might % be confusing, but I think it's reasonable to describe amortized % analysis as an average over a series of operations. The exercise asks you to fill in the body of \"profileArrayListAddBeginning\", which tests the performance of adding new elements at the beginning of an \"ArrayList\". Based on our analysis, we expect each add to be linear, because it has to shift the other elements to the right; so we expect n adds to be quadratic. Here's a solution, which you can find in the {\\tt solution} directory of the repository. This method is almost identical to \"profileArrayListAddEnd\". The only difference is in \"timeMe\", which uses the two-parameter version of \"add\" to put the new element at index 0. Also, we increased \"endMillis\" to get one additional data point. Here are the timing results (problem size on the left, runtime in milliseconds on the right):\nRemember that a straight line on this graph does **not** mean that the algorithm is linear. Rather, if the runtime is proportional to n^k for any exponent, k, we expect to see a straight line with slope k. In this case, we expect the total time for n adds to be proportional to n^2, so we expect a straight line with slope 2. In fact, the estimated slope is 1.992, which is so close I would be afraid to fake data this good."], ["squares fit results, list shift, arraylist linkedlist, good range data measurements, constant time, startn endmillis, total time, linear solution, straight line slope", "In the previous exercise you also profiled the performance of adding new elements at the beginning of a \"LinkedList\". Based on our analysis, we expect each \"add\" to take constant time, because in a linked list, we don't have to shift the existing elements; we can just add a new node at the beginning. So we expect the total time for n adds to be linear. Here's a solution: We only had a make a few changes, replacing \"ArrayList\" with \"LinkedList\" and adjusting \"startN\" and \"endMillis\" to get a good range of data. The measurements were noisier than the previous batch; here are the results:"], ["squares fit results, good range data measurements, arraylist linkedlist, list shift, constant time, startn endmillis, total time, linear solution, straight line slope", "It's not a very straight line, and the slope is not exactly 1; the slope of the least squares fit is 1.23. But these results indicate that the total time for n adds is at least approximately O(n), so each add is constant time."], ["constant time, total time, analysis fact, quadratic well code results measurements noisy line, implementation traverse entire list add element end linear", "Adding elements at the beginning is one of the operations where we expect \"LinkedList\" to be faster than \"ArrayList\". But for adding elements at the end, we expect \"LinkedList\" to be slower. In my implementation, we have to traverse the entire list to add an element to the end, which is linear. So we expect the total time for n adds to be quadratic. Well, it's not.  Here's the code: Here are the results:\nAgain, the measurements are noisy and the line is not perfectly straight, but the estimated slope is 1.19, which is close to what we got adding elements at the beginning, and not very close to 2, which is what we expected based on our analysis. In fact, it is closer to 1, which suggests that adding elements at the end is constant time. What's going on?"], ["arraylist mylinkedlist linkedlist, either end list traverse either direction result, list operations index list traverse list, next node, documentation linkedlist http", "My implementation of a linked list, \"MyLinkedList\", uses a singly-linked list; that is, each element contains a link to the next, and the \"MyArrayList\" object itself has a link to the first node. But if you read the documentation of \"LinkedList\" at http://thinkdast.com/linked, it says:\nDoubly-linked list implementation of the List and Deque interfaces. [...] All of the operations perform as could be expected for a doubly-linked list. Operations that index into the list will traverse the list from the beginning or the end, whichever is closer to the specified index.\nIf you are not familiar with doubly-linked lists, you can read more about them at http://thinkdast.com/doublelist, but the short version is:\n- Each node contains a link to the next node and a link to the previous node.\n- The \"LinkedList\" object contains links to the first and last elements of the list.\nSo we can start at either end of the list and traverse it in either direction. As a result, we can add and remove elements from the beginning and the end of the list in constant time! The following table summarizes the performance we expect from \"ArrayList\", \"MyLinkedList\" (singly-linked), and \"LinkedList\" (doubly-linked):"], ["next myarraylist object link first node, list element contains, quote quote, table summarizes performance, last elements, end list constant time, list implementation list deque interfaces operations perform, previous node", "\\begin{quote}\n\\end{quote}"], ["runtime different implementations, different amounts space arraylist elements, list element, runtime application, single chunk memory", "The doubly-linked implementation is better than \"ArrayList\" for adding and removing at the beginning, and just as good as \"ArrayList\" for adding and removing at the end. So the only advantage of \"ArrayList\" is for \"get\" and \"set\", which require linear time in a linked list, even if it is doubly-linked. If you know that the runtime of your application depends on the time it takes to \"get\" and \"set\" elements, an \"ArrayList\" might be the better choice. If the runtime depends on adding and removing elements near the beginning or the end, \"LinkedList\" might be better. But remember that these recommendations are based on the order of growth for large problems. There are other factors to consider:\n- If these operations don't take up a substantial fraction of the runtime for your application --- that is, if your applications spends most of its time doing other things --- then your choice of a \"List\" implementation won't matter very much.\n- If the lists you are working with are not very big, you might not get the performance you expect. For small problems, a quadratic algorithm might be faster than a linear algorithm, or linear might be faster than constant time. And for small problems, the difference probably doesn't matter.\n- Also, don't forget about space. So far we have focused on runtime, but different implementations require different amounts of space. In an \"ArrayList\", the elements are stored side-by-side in a single chunk of memory, so there is little wasted space, and computer hardware is often faster with contiguous chunks. In a linked list, each element requires a node with one or two links. The links take up space (sometimes more than the data!), and with nodes scattered around in memory, the hardware might be less efficient."], ["software engineer, summary analysis algorithms, choice data structure problem size large enough order growth, data structure, data structures runtime application important runtime application", "In summary, analysis of algorithms provides some guidance for choosing data structures, but only if\n- The runtime of your application is important,\n- The runtime of your application depends on your choice of data structure, and\n- The problem size is large enough that the order of growth actually predicts which data structure is better."], ["summary analysis algorithms, software engineer, choice data structure problem size large enough order growth, data structure, data structures runtime application important runtime application", "You could have a long career as a software engineer without ever finding yourself in this situation."], ["basic pieces crawler, http read history testing conjecture, crawler test, search terms returns list web pages, web pages", "A **web search engine**, like Google Search or Bing, takes a set of \"search terms\" and returns a list of web pages that are relevant to those terms (I'll discuss what \"relevant\" means later).  You can read more at http://thinkdast.com/searcheng, but I'll explain what you need as we go along. The essential components of a search engine are:\n- Crawling: We'll need a program that can download a web page, parse it, and extract the text and any links to other pages.\n- Indexing: We'll need a data structure that makes it possible to look up a search term and find the pages that contain it.\n- Retrieval: And we'll need a way to collect results from the Index and identify pages that are most relevant to the search terms.\nWe'll start with the crawler.  The goal of a crawler is to discover and download a set of web pages. For search engines like Google and Bing, the goal is to find all web pages, but often crawlers are limited to a smaller domain. In our case, we will only read pages from Wikipedia. As a first step, we'll build a crawler that reads a Wikipedia page, finds the first link, follows the link to another page, and repeats. We will use this crawler to test the \"Getting to Philosophy\" conjecture, which states:\nThis conjecture is stated at http://thinkdast.com/getphil{}, and you can read its history there. Testing the conjecture will allow us to build the basic pieces of a crawler without having to crawl the entire web, or even all of Wikipedia. And I think the exercise is kind of fun! In a few chapters, we'll work on the indexer, and then we'll get to the retriever."], ["philosophy article, process subsequent articles, quote quote, engine crawling, first lowercase link main text wikipedia article, web pages", "\\begin{quote}\n\\end{quote}"], ["philosophy article, process subsequent articles, quote quote, engine crawling, first lowercase link main text wikipedia article, web pages", "Clicking on the first lowercase link in the main text of a Wikipedia article, and then repeating the process for subsequent articles, usually eventually gets one to the Philosophy article."], ["link parent node possible navigate tree dom tree real pages, page elements, inspect element menu safari, chrome part web page select inspect menu pops firefox, jsoup java library downloads", "When you download a web page, the contents are written in HyperText Markup Language, aka HTML. For example, here is a minimal HTML document: The phrases \"This is a title\" and \"Hello world!\" are the text that actually appears on the page; the other elements are **tags** that indicate how the text should be displayed. When our crawler downloads a page, it will need to parse the HTML in order to extract the text and find the links. To do that, we'll use\n**jsoup**, which is an open-source Java library that downloads and parses HTML. The result of parsing HTML is a Document Object Model tree, or\nEach node contains links to its children; in addition, each node contains a link to its **parent**, so from any node it is possible to navigate up and down the tree. The DOM tree for real pages is usually more complicated than this example. Most web browsers provide tools for inspecting the DOM of the page you are viewing. In Chrome, you can right-click on any part of a web page and select \"Inspect\" from the menu that pops up. In Firefox, you can right-click and select \"Inspect Element\" from the menu. Safari provides a tool called Web Inspector, which you can read about at http://thinkdast.com/safari. For Internet Explorer, you can read the instructions at http://thinkdast.com/explorer.\ncom/java. The element that's highlighted is the first paragraph of the main text of the article, which is contained in a \"<div>\" element with \"id=\"mw-content-text\"\". We'll use this element id to identify the main text of each article we download."], ["text tags document elements relationships nodes, child p, nodes contains, need parse html order extract text, dom tree, node head, links children addition node, root html tag, http internet explorer, download web page contents, web inspector, elements document, child title body, body nodes children", "**DOM tree**, that contains the elements of the document, including text and tags. The tree is a linked data structure made up of nodes; the nodes represent text, tags, and other document elements. The relationships between the nodes are determined by the structure of the document. In the example above, the first node, called the\n**root**, is the \"<html>\" tag, which contains links to the two nodes it contains, \"<head>\" and \"<body>\"; these nodes are the\n**children** of the root node. The \"<head>\" node has one child, \"<title>\", and the \"<body>\" node has one child, \"<p>\" (which stands for \"paragraph\")."], ["traverses tree returns elements tags match, returns document object, method downloads html, dom document, value getelementbyid element object", "jsoup makes it easy to download and parse web pages, and to navigate the DOM tree. Here's an example: \"Jsoup.connect\" takes a URL as a \"String\" and makes a connection to the web server; the \"get\" method downloads the HTML, parses it, and returns a \"Document\" object, which represents the DOM. \"Document\" provides methods for navigating the tree and selecting nodes. In fact, it provides so many methods, it can be confusing. This example demonstrates two ways to select nodes:\n- \"getElementById\" takes a \"String\" and searches the tree for an element that has a matching \"id\" field. Here it selects the node \"<div id=\"mw-content-text\" lang=\"en\" dir=\"ltr\" class=\"mw-content-ltr\">\", which appears on every Wikipedia page to identify the \"<div>\" element that contains the main text of the page, as opposed to the navigation sidebar and other elements. The return value from \"getElementById\" is an \"Element\" object that represents this \"<div>\" and contains the elements in the \"<div>\" as children, grandchildren, etc.\n- \"select\" takes a \"String\", traverses the tree, and returns all the elements with tags that match the \"String\". In this example, it returns all paragraph tags that appear in \"content\". The return value is an \"Elements\" object.\nBefore you go on, you should skim the documentation of these classes so you know what they can do. The most important classes are \"Element\", \"Elements\", and \"Node\", which you can read about at http://thinkdast.com/jsoupelt, http://thinkdast.com/jsoupelts, and http://thinkdast.com/jsoupnode. \"Node\" represents a node in the DOM tree;  there are several subclasses that extend \"Node\", including \"Element\", \"TextNode\", \"DataNode\", and \"Comment\". \"Elements\" is a \"Collection\" of \"Element\" objects."], ["connection web server, another example diagram, uml class diagram line hollow arrow head, uml diagrams, every wikipedia page identify div element, div contains elements div children, dom document", "In a UML class diagram, a line with a hollow arrow head indicates that one class extends another.  For example, this diagram indicates that \"Elements\" extends \"ArrayList\". We'll get back to UML diagrams in Section~\\ref{uml-class-diagrams}."], ["represent tags, types node, quote quote, wikinodeiterable implements iterable node wikinodeiterable performs search, example picks, wikinodeiterable lets, text html paragraph, markup output, page example print node textnode, language concurrent, example shows, java computer, nodes order", "To make your life easier, I provide a class called \"WikiNodeIterable\" that lets you iterate through the nodes in a DOM tree. Here's an example that shows how to use it: This example picks up where the previous one leaves off. It selects the first paragraph in \"paragraphs\" and then creates a \"WikiNodeIterable\", which implements \"Iterable<Node>\". \"WikiNodeIterable\" performs a \"depth-first search\", which produces the nodes in the order they would appear on the page. In this example, we print a \"Node\" only if it is a \"TextNode\" and ignore other types of \"Node\", specifically the \"Element\" objects that represent tags. The result is the plain text of the HTML paragraph without any markup. The output is:"], ["represent tags, types node, quote quote, java computer, markup output, language concurrent, nodes order", "\\begin{quote}\n\\end{quote}"], ["selects first paragraph paragraphs, represent tags, types node, quote quote, java computer, markup output, language concurrent, nodes order", "Java is a general-purpose computer programming language that is concurrent, class-based, object-oriented,{[}13{]} and specifically designed ..."], ["common ways implement dfs, every node tree, traversals http application traversal order matter, order alternative use stack data structure, search dfs dfs", "There are several ways you might reasonably traverse a tree, each with different applications. We'll start with \"depth-first search\", or DFS. DFS starts at the root of the tree and selects the first child. If the child has children, it selects the first child again. When it gets to a node with no children, it backtracks, moving up the tree to the parent node, where it selects the next child if there is one; otherwise it backtracks again. When it has explored the last child of the root, it's done. There are two common ways to implement DFS, recursively and iteratively. The recursive implementation is simple and elegant: This method gets invoked on every \"Node\" in the tree, starting with the root. If the \"Node\" it gets is a \"TextNode\", it prints the contents. If the \"Node\" has any children, it invokes \"recursiveDFS\" on each one of them in order. In this example, we print the contents of each \"TextNode\" before traversing the children, so this is an example of a \"pre-order\" traversal. You can read about \"pre-order\", \"post-order\", and \"in-order\" traversals at http://thinkdast.com/treetrav.  For this application, the traversal order doesn't matter. By making recursive calls, \"recursiveDFS\" uses the call stack (http://thinkdast.com/callstack) to keep track of the child nodes and process them in the right order. As an alternative, we can use a stack data structure to keep track of the nodes ourselves; if we do that, we can avoid the recursion and traverse the tree iteratively."], ["element stack, remove end constant time operation careful add elements wrong place, implement stack java, stack empty pop, constant time operations", "Before I explain the iterative version of DFS, I'll explain the stack data structure.  We'll start with the general concept of a stack, which I'll call a \"stack\" with a lowercase \"s\". Then we'll talk about two Java \"interfaces\" that define stack methods: \"Stack\" and \"Deque\". % TODO: introduce the term \"Abstract Data Type\"? A stack is a data structure that is similar to a list: it is a collection that maintains the order of the elements. The primary difference between a stack and a list is that the stack provides fewer methods. In the usual convention, it provides:\n- \"push\": which adds an element to the top of the stack.\n- \"pop\": which removes and returns the top-most element from the stack.\n- \"peek\": which returns the top-most element without modifying the stack.\n- \"isEmpty\": which indicates whether the stack is empty.\nBecause \"pop\" always returns the top-most element, a stack is also called a \"LIFO\", which stands for \"last in, first out\". An alternative to a stack is a \"queue\", which returns elements in the same order they are added; that is, \"first in, first out\", or FIFO. It might not be obvious why stacks and queues are useful: they don't provide any capabilities that aren't provided by lists; in fact, they provide fewer capabilities. So why not use lists for everything? There are two reasons:\n- If you limit yourself to a small set of methods --- that is, a small API --- your code will be more readable and less error-prone. For example, if you use a list to represent a stack, you might accidentally remove an element in the wrong order. With the stack API, this kind of mistake is literally impossible. And the best way to avoid errors is to make them impossible.\n- If a data structure provides a small API, it is easier to implement efficiently. For example, a simple way to implement a stack is a singly-linked list. When we push an element onto the stack, we add it to the beginning of the list; when we pop an element, we remove it from the beginning. For a linked list, adding and removing from the beginning are constant time operations, so this implementation is efficient. Conversely, big APIs are harder to implement efficiently.\nTo implement a stack in Java, you have three options:\n- Go ahead and use \"ArrayList\" or \"LinkedList\". If you use \"ArrayList\", be sure to add and remove from the end, which is a constant time operation. And be careful not to add elements in the wrong place or remove them in the wrong order.\n- Java provides a class called \"Stack\" that provides the standard set of stack methods. But this class is an old part of Java: it is not consistent with the Java Collections Framework, which came later.\n- Probably the best choice is to use one of the implementations of the \"Deque\" interface, like \"ArrayDeque\"."], ["push pop peek isempty use deque stack, lifo stands last first alternative stack queue returns elements order, arraylist linkedlist use arraylist, returns element stack peek returns element, deek java deque interface, impossible data structure, stack methods stack deque todo, pronounced deck people", "\"Deque\" stands for \"double-ended queue\"; it's supposed to be pronounced \"deck\", but some people say \"deek\". In Java, the \"Deque\" interface provides \"push\", \"pop\", \"peek\", and \"isEmpty\", so you can use a \"Deque\" as a stack. It provides other methods, which you can read about at http://thinkdast.com/deque, but we won't use them for now."], ["use methods, methods different interfaces code, next chapter, another implementation deque old friend linkedlist linkedlist implements interfaces list deque interface get, implement java iterator", "Here is an iterative version of DFS that uses an \"ArrayDeque\" to represent a stack of \"Node\" objects: The parameter, \"root\", is the root of the tree we want to traverse, so we start by creating the stack and pushing the root onto it. The loop continues until the stack is empty. Each time through, it pops a \"Node\" off the stack. If it gets a \"TextNode\", it prints the contents. Then it pushes the children onto the stack. In order to process the children in the right order, we have to push them onto the stack in reverse order; we do that by copying the children into an \"ArrayList\", reversing the elements in place, and then iterating through the reversed \"ArrayList\". One advantage of the iterative version of DFS is that it is easier to implement as a Java \"Iterator\"; you'll see how in the next chapter. But first, one last note about the \"Deque\" interface: in addition to \"ArrayDeque\", Java provides another implementation of \"Deque\", our old friend \"LinkedList\". \"LinkedList\" implements both interfaces, \"List\" and \"Deque\". Which interface you get depends on how you use it. For example, if you assign a \"LinkedList\" object to a \"Deque\" variable, like this: you can use the methods in the \"Deque\" interface, but not all methods in the \"List\" interface. If you assign it to a \"List\" variable, like this: you can use \"List\" methods but not all \"Deque\" methods. And if you assign it like this: you can use all the methods. But if you combine methods from different interfaces, your code will be less readable and more error-prone."], ["wikipedia terms service class limits, jsoup download pages wikipedia help, repository book, code help, dom tree explain code next section", "In the repository for this book, you'll find some code to help you get started:\n- \"WikiNodeExample.java\" contains the code from the previous chapter, demonstrating recursive and iterative implementations of depth-first search (DFS) in a DOM tree.\n- \"WikiNodeIterable.java\" contains an \"Iterable\" class for traversing a DOM tree. I'll explain this code in the next section.\n- \"WikiFetcher.java\" contains a utility class that uses jsoup to download pages from Wikipedia. To help you comply with Wikipedia's terms of service, this class limits how fast you can download pages; if you request more than one page per second, it sleeps before downloading the next page.\n- \"WikiPhilosophy.java\" contains an outline of the code you will write for this exercise. We'll walk through it below."], ["dfs dom tree, simple bit starter code, download pages, previous chapter, next page, code help, file run ant wikiphilosophy, recursive iterative implementations", "You'll also find the Ant build file \"build.xml\".  If you run \"ant WikiPhilosophy\", it will run a simple bit of starter code."], ["http http, isempty checks, returns iterator, good method, children reverse order returns node", "In the previous chapter, I presented an iterative depth-first search (DFS), and suggested that an advantage of the iterative version, compared to the recursive version, is that it is easier to wrap in an \"Iterator\" object. In this section we'll see how to do that. If you are not familiar with the \"Iterator\" and \"Iterable\" interfaces, you can read about them at http://thinkdast.com/iterator and http://thinkdast.com/iterable. Take a look at the contents of \"WikiNodeIterable.java\". The outer class, \"WikiNodeIterable\" implements the \"Iterable<Node>\" interface  so we can use it in a for loop like this: where \"root\" is the root of the tree we want to traverse and \"visit\" is a method that does whatever we want when we \"visit\" a \"Node\". The implementation of \"WikiNodeIterable\" follows a conventional formula:\n- The constructor takes and stores a reference to the root \"Node\".\n- The \"iterator\" method creates a returns an \"Iterator\" object.\nHere's what it looks like: The inner class, \"WikiNodeIterator\", does all the real work: This code is almost identical to the iterative version of DFS, but now it's split into three methods:\n- The constructor initializes the stack (which is implemented using an \"ArrayDeque\") and pushes the root node onto it.\n- \"isEmpty\" checks whether the stack is empty.\n- \"next\" pops the next \"Node\" off the stack, pushes its children in reverse order, and returns the \"Node\" it popped. If someone invokes \"next\" on an empty \"Iterator\", it throws an exception.\nIt might not be obvious that it is worthwhile to rewrite a perfectly good method with two classes and five methods.  But now that we've done it, we can use \"WikiNodeIterable\" anywhere an \"Iterable\" is called for, which makes it easy and syntactically clean to separate the logic of the iteration (DFS) from whatever processing we are doing on the nodes."], ["dom element paragraph content text code look familiar new code, fetchwikipedia example, pages wikipedia, provide class, example assume urllist collection string processparagraphs method something elements", "When you write a Web crawler, it is easy to download too many pages too fast, which might violate the terms of service for the server you are downloading from. To help you avoid that, I provide a class called \"WikiFetcher\" that does two things:\n- It encapsulates the code we demonstrated in the previous chapter for downloading pages from Wikipedia, parsing the HTML, and selecting the content text.\n- It measures the time between requests and, if we don't leave enough time between requests, it sleeps until a reasonable interval has elapsed. By default, the interval is one second.\nHere's the definition of \"WikiFetcher\": The only public method is \"fetchWikipedia\", which takes a URL as a \"String\" and returns an \"Elements\" collection that contains one DOM element for each paragraph in the content text. This code should look familiar. The new code is in \"sleepIfNeeded\", which checks the time since the last request and sleeps if the elapsed time is less than \"minInterval\", which is in milliseconds. That's all there is to \"WikiFetcher\". Here's an example that demonstrates how it's used: In this example, we assume that \"urlList\" is a collection of \"String\"s, and \"processParagraphs\" is a method that does something with the \"Elements\" object returned by \"fetchWikipedia\". This example demonstrates something important: you should create one \"WikiFetcher\" object and use it to handle all requests. If you have multiple instances of \"WikiFetcher\", they won't enforce the minimum interval between requests. NOTE: My implementation of \"WikiFetcher\" is simple, but it would be easy for someone to misuse it by creating multiple instances. You could avoid this problem by making \"WikiFetcher\" a \"singleton\", which you can read about at http://thinkdast.com/singleton."], ["key value method, index maps, fundamental operation index lookup, alternative map data structure, terms values counts", "The fundamental operation of the index is a **lookup**; specifically, we need the ability to look up a term and find all pages that contain it. The simplest implementation would be a collection of pages. Given a search term, we could iterate through the contents of the pages and select the ones that contain the search term. But the runtime would be proportional to the total number of words on all the pages, which would be way too slow. A better alternative is a **map**, which is a data structure that represents a collection of **key-value pairs** and provides a fast way to look up a **key** and find the corresponding **value**. For example, the first map we'll construct is a \"TermCounter\", which maps from each search term to the number of times it appears in a page. The keys are the search terms and the values are the counts (also called \"frequencies\"). Java provides an interface called \"Map\" that specifies the methods a map should provide; the most important are:\n- \"get(key)\": This method looks up a key and returns the corresponding value.\n- \"put(key, value)\": This method adds a new key-value pair to the \"Map\", or if the key is already in the map, it replaces the value associated with \"key\".\nJava provides several implementations of \"Map\", including the two we will focus on, \"HashMap\" and \"TreeMap\". In upcoming chapters, we'll look at these implementations and analyze their performance. In addition to the \"TermCounter\", which maps from search terms to counts, we will define a class called \"Index\", which maps from a search term to a collection of pages where it appears. And that raises the next question, which is how to represent a collection of pages. Again, if we think about the operations we want to perform, that guides our decision. In this case, we'll need to combine two or more collections and find the pages that appear in all of them. You might recognize this operation as"], ["data structures top implement, element method checks, hashset treeset, set methods, intersection intersection, set intersection, set interface defines operations, several implementations set, possible implement intersection, element method", "**set intersection**: the intersection of two sets is the set of elements that appear in both. As you might expect by now, Java provides a \"Set\" interface that defines the operations a set should perform. It doesn't actually provide set intersection, but it provides methods that make it possible to implement intersection and other set operations efficiently. The core \"Set\" methods are:\n- \"add(element)\": This method adds an element to a set; if the element is already in the set, it has no effect.\n- \"contains(element)\": This method checks whether the given element is in the set.\nJava provides several implementations of \"Set\", including \"HashSet\" and \"TreeSet\". Now that we've designed our data structures from the top down, we'll implement them from the inside out, starting with \"TermCounter\"."], ["regular expressions parameters, splits text words, nodes contain text extracts text passes, wikifetcher download page wikipedia parse main text, map hand", "\"TermCounter\" is a class that represents a mapping from search terms to the number of times they appear in a page. Here is the first part of the class definition: The instance variables are \"map\", which contains the mapping from terms to counts, and \"label\", which identifies the document the terms came from; we'll use it to store URLs. To implement the mapping, I chose \"HashMap\", which is the most commonly-used \"Map\". Coming up in a few chapters, you will see how it works and why it is a common choice. \"TermCounter\" provides \"put\" and \"get\", which are defined like this: \"put\" is just a **wrapper method**; when you call \"put\" on a \"TermCounter\", it calls \"put\" on the embedded map. On the other hand, \"get\" actually does some work. When you call \"get\" on a \"TermCounter\", it calls \"get\" on the map, and then checks the result. If the term does not appear in the map, \"TermCount.get\" returns 0. Defining \"get\" this way makes it easier to write \"incrementTermCount\", which takes a term and increases by one the counter associated with that term. If the term has not been seen before, \"get\" returns 0; we add 1, then use \"put\" to add a new key-value pair to the map. If the term is already in the map, we get the old count, add 1, and then store the new count, which replaces the old value. In addition, \"TermCounter\" provides these other methods to help with indexing Web pages:\n- \"processElements\" takes an \"Elements\" object, which is a collection of jsoup \"Element\" objects. It iterates through the collection and calls \"processTree\" on each.\n- \"processTree\" takes a jsoup \"Node\" that represents the root of a DOM tree. It iterates through the tree to find the nodes that contain text; then it extracts the text and passes it to \"processText\".\n- \"processText\" takes a \"String\" that contains words, spaces, punctuation, etc. It removes punctuation characters by replacing them with spaces, converts the remaining letters to lowercase, then splits the text into words. Then it loops through the words it found and calls \"incrementTermCount\" on each.  The \"replaceAll\" and \"split\" methods take **regular expressions** as parameters; you can read more about them at http://thinkdast.com/regex.\nFinally, here's an example that demonstrates how \"TermCounter\" is used: This example uses a \"WikiFetcher\" to download a page from Wikipedia and parse the main text. Then it creates a \"TermCounter\" and uses it to count the words in the page. In the next section, you'll have a chance to run this code and test your understanding by filling in a missing method."], ["type parameters k type keys v type values mylinearmap implements map, pair definition much entry container key value definition, mylinearmap class definition class, inside mylinearlist, single instance variable entries, type parameters k v, methods map interface mylinearmap, entry objects entry", "As usual, I provide starter code and you will fill in the missing methods. Here's the beginning of the \"MyLinearMap\" class definition: This class uses two type parameters, \"K\", which is the type of the keys, and \"V\", which is the type of the values. \"MyLinearMap\" implements \"Map\", which means it has to provide the methods in the \"Map\" interface. A \"MyLinearMap\" object has a single instance variable, \"entries\", which is an \"ArrayList\" of \"Entry\" objects. Each \"Entry\" contains a key-value pair. Here is the definition: There's not much to it; an \"Entry\" is just a container for a key and a value. This definition is nested inside \"MyLinearList\", so it uses the same type parameters, \"K\" and \"V\". That's all you need to do the exercise, so let's get started."], ["element end constant time, add entry call constant time, linear time, fact implementation map core methods constant time first hear, proportional n", "In this section I present a solution to the previous exercise and analyze the performance of the core methods.  Here are \"findEntry\" and \"equals\": The runtime of \"equals\" might depend on the size of the \"target\" and the keys, but does not generally depend on the number of entries, n. So \"equals\" is constant time. In \"findEntry\", we might get lucky and find the key we're looking for at the beginning, but we can't count on it. In general, the number of entries we have to search is proportional to n, so \"findEntry\" is linear. Most of the core methods in \"MyLinearMap\" use \"findEntry\", including \"put\", \"get\", and \"remove\". Here's what they look like: After \"put\" calls \"findEntry\", everything else is constant time. Remember that \"entries\" is an \"ArrayList\", so adding an element at the end is constant time, on average. If the key is already in the map, we don't have to add an entry, but we have to call \"entry.getValue\" and \"entry.setValue\", and those are both constant time. Putting it all together, \"put\" is linear. By the same reasoning, \"get\" is also linear. \"remove\" is slightly more complicated because \"entries.remove\" might have to remove an element from the beginning or middle of the \"ArrayList\", and that takes linear time. But that's OK: two linear operations are still linear. In summary, the core methods are all linear, which is why we called this implementation \"MyLinearMap\" (ta-da!). If we know that the number of entries will be small, this implementation might be good enough, but we can do better. In fact, there is an implementation of \"Map\" where all of the core methods are constant time. When you first hear that, it might not seem possible. What we are saying, in effect, is that you can find a needle in a haystack in constant time, regardless of how big the haystack is. It's magic. I'll explain how it works in two steps:\n- Instead of storing entries in one big \"List\", we'll break them up into lots of short lists. For each key, we'll use a **hash code** (explained in the next section) to determine which list to use.\n- Using lots of short lists is faster than using just one, but as I'll explain, it doesn't change the order of growth; the core operations are still linear. But there is one more trick: if we increase the number of lists to limit the number of entries per list, the result is a constant-time map. You'll see the details in the next exercise, but first: hashing!"], ["needle haystack constant time, add entry call constant time, linear operations, next chapter present solution analyze performance core map methods, big list break lots short lists, explain change order growth core operations, linear reasoning get, number entries equals constant time findentry, efficient implementation, trick increase number", "In the next chapter, I'll present a solution, analyze the performance of the core \"Map\" methods, and introduce a more efficient implementation."], ["collection mylinearmap, maps stores arraylist key, performance mylinearmap, otherwise use hashcode get integer, hash code way", "To improve the performance of \"MyLinearMap\", we'll write a new class, called \"MyBetterMap\", that contains a collection of \"MyLinearMap\" objects. It divides the keys among the embedded maps, so the number of entries in each map is smaller, which speeds up \"findEntry\" and the methods that depend on it. Here's the beginning of the class definition: The instance variable, \"maps\", is a collection of \"MyLinearMap\" objects. The constructor takes a parameter, \"k\", that determines how many maps to use, at least initially. Then \"makeMaps\" creates the embedded maps and stores them in an \"ArrayList\". Now, the key to making this work is that we need some way to look at a key and decide which of the embedded maps it should go into. When we \"put\" a new key, we choose one of the maps; when we \"get\" the same key, we have to remember where we put it. One possibility is to choose one of the sub-maps at random and keep track of where we put each key. But how should we keep track? It might seem like we could use a \"Map\" to look up the key and find the right sub-map, but the whole point of the exercise is to write an efficient implementation of a \"Map\". We can't assume we already have one. A better approach is to use a **hash function**, which takes an \"Object\", any \"Object\", and returns an integer called a\n**hash code**.  Importantly, if it sees the same \"Object\" more than once, it always returns the same hash code. That way, if we use the hash code to store a key, we'll get the same hash code when we look it up. In Java, every \"Object\" provides a method called \"hashCode\" that computes a hash function. The implementation of this method is different for different objects; we'll see an example soon. Here's a helper method that chooses the right sub-map for a given key: If \"key\" is \"null\", we choose the sub-map with index 0, arbitrarily. Otherwise we use \"hashCode\" to get an integer, apply \"Math.abs\" to make sure it is non-negative, then use the remainder operator, \"\\%\", which guarantees that the result is between 0 and \"maps.size()-1\". So \"index\" is always a valid index into \"maps\". Then \"chooseMap\" returns a reference to the map it chose. We use \"chooseMap\" in both \"put\" and \"get\", so when we look up a key, we get the same map we chose when we added the key. At least, we should --- I'll explain a little later why this might not work. Here's my implementation of \"put\" and \"get\": Pretty simple, right? In both methods, we use \"chooseMap\" to find the right sub-map and then invoke a method on the sub-map. That's how it works; now let's think about performance. If there are n entries split up among k sub-maps, there will be n/k entries per map, on average. When we look up a key, we have to compute its hash code, which takes some time, then we search the corresponding sub-map. Because the entry lists in \"MyBetterMap\" are k times shorter than the entry list in \"MyLinearMap\", we expect the search to be k times faster. But the runtime is still proportional to n, so \"MyBetterMap\" is still linear. In the next exercise, you'll see how we can fix that."], ["objects hash code, embedded strings, easy objects mutable state think harder example immutable object define class, good hash functions, unicode code point need, code end, objects equal innerstring instance variables equal hashcode, value range, hash code, equal equals returns true hash code requirement", "The fundamental requirement for a hash function is that the same object should produce the same hash code every time. For immutable objects, that's relatively easy. For objects with mutable state, we have to think harder. As an example of an immutable object, I'll define a class called \"SillyString\" that encapsulates a \"String\": This class is not very useful, which is why it's called \"SillyString\", but I'll use it to show how a class can define its own hash function: Notice that \"SillyString\" overrides both \"equals\" and \"hashCode\". This is important. In order to work properly, \"equals\" has to be consistent with \"hashCode\", which means that if two objects are considered equal --- that is, \"equals\" returns \"true\" --- they should have the same hash code. But this requirement only works one way; if two objects have the same hash code, they don't necessarily have to be equal. \"equals\" works by invoking \"toString\", which returns \"innerString\". So two \"SillyString\" objects are equal if their \"innerString\" instance variables are equal. \"hashCode\" works by iterating through the characters in the \"String\" and adding them up. When you add a character to an \"int\", Java converts the character to an integer using its Unicode code point. You don't need to know anything about Unicode to understand this example, but if you are curious, you can read more at http://thinkdast.com/codepoint. This hash function satisfies the requirement: if two \"SillyString\" objects contain embedded strings that are equal, they will get the same hash code. This works correctly, but it might not yield good performance, because it returns the same hash code for many different strings. If two strings contain the same letters in any order, they will have the same hash code. And even if they don't contain the same letters, they might yield the same total, like \"\"ac\"\" and \"\"bb\"\". If many objects have the same hash code, they end up in the same sub-map.  If some sub-maps have more entries than others, the speedup when we have k maps might be much less than k. So one of the goals of a hash function is to be uniform; that is, it should be equally likely to produce any value in the range.  You can read more about designing good hash functions at http://thinkdast.com/hash."], ["mutable object definition sillyarray identical sillystring, map hash code array, code let, mybettermap hashmap guarantee keys, different hash code good chance", "\"String\"s are immutable, and \"SillyString\" is also immutable because \"innerString\" is declared to be \"final\". Once you create a \"SillyString\", you can't make \"innerString\" refer to a different \"String\", and you can't modify the \"String\" it refers to. Therefore, it will always have the same hash code. But let's see what happens with a mutable object. Here's a definition for \"SillyArray\", which is identical to \"SillyString\", except that it uses an array of characters instead of a \"String\": \"SillyArray\" also provides \"setChar\", which makes it possible to modify the characters in the array: Now suppose we create a \"SillyArray\" and add it to a map: The hash code for this array is 461. Now if we modify the contents of the array and then try to look it up, like this: the hash code after the mutation is 441. With a different hash code, there's a good chance we'll go looking in the wrong sub-map. In that case, we won't find the key, even though it is in the map. And that's bad. In general, it is dangerous to use mutable objects as keys in data structures that use hashing, which includes \"MyBetterMap\" and \"HashMap\". If you can guarantee that the keys won't be modified while they are in the map, or that any changes won't affect the hash code, it might be OK. But it is probably a good idea to avoid it."], ["proportion n several core mybettermap, analyze rehash constant time linear way, unit third time rehash, important double number k, unit work next time rehash", "If the number of entries in the biggest sub-map is proportional to n/k, and k grows in proportion to n, several of the core \"MyBetterMap\" methods become constant time: Each method hashes a key, which is constant time, and then invokes a method on a sub-map, which is constant time. So far, so good. But the other core method, \"put\", is a little harder to analyze. When we don't have to rehash, it is constant time, but when we do, it's linear. In that way, it's similar to \"ArrayList.add\", which we analyzed in Section~\\ref{classifying-add}. For the same reason, \"MyHashMap.put\" turns out to be constant time if we average over a series of invocations. Again, the argument is based on amortized analysis (see Section~\\ref{classifying-add}). Suppose the initial number of sub-maps, k, is 2, and the load factor is 1. Now let's see how much work it takes to \"put\" a series of keys. As the basic \"unit of work\", we'll count the number of times we have to hash a key and add it to a sub-map. The first time we call \"put\" it takes 1 unit of work. The second time also takes 1 unit. The third time we have to rehash, so it takes 2 units to rehash the existing keys and 1 unit to hash the new key. Now the size of the hash table is 4, so the next time we call \"put\", it takes 1 unit of work. But the next time we have to rehash, which takes 4 units to rehash the existing keys and 1 unit to hash the new key.\nAs the arrows suggest, if we knock down the towers, each one fills the space before the next tower. The result is a uniform height of 2 units, which shows that the average work per \"put\" is about 2 units. And that means that \"put\" is constant time on average. This diagram also shows why it is important to double the number of sub-maps, k, when we rehash. If we only add to k instead of multiplying, the towers would be too close together and they would start piling up. And that would not be constant time."], ["minute appreciate remarkable performance operations, constant time, structure fit memory another limitation implementation, structure fit cache, much matter big hash table well sort remember analysis", "We've shown that \"containsKey\", \"get\", and \"remove\" are constant time, and \"put\" is constant time on average. We should take a minute to appreciate how remarkable that is. The performance of these operations is pretty much the same no matter how big the hash table is. Well, sort of. Remember that our analysis is based on a simple model of computation where each \"unit of work\" takes the same amount of time. Real computers are more complicated than that. In particular, they are usually fastest when working with data structures small enough to fit in cache; somewhat slower if the structure doesn't fit in cache but still fits in memory; and much slower if the structure doesn't fit in memory. Another limitation of this implementation is that hashing doesn't help if we are given a value rather than a key: \"containsValue\" is linear because it has to search all of the sub-maps. And there is no particularly efficient way to look up a value and find the corresponding key (or possibly keys). And there's one more limitation: some of the methods that were constant time in \"MyLinearMap\" have become linear. For example: \"clear\" has to clear all of the sub-maps, and the number of sub-maps is proportional to n, so it's linear. Fortunately, this operation is not used very often, so for most applications this tradeoff is acceptable."], ["range problem, ran code, java hashmap run profiler see slope, runtimes milliseconds, consistent analysis", "Before we go on, we should check whether \"MyHashMap.put\" is really constant time. Run \"ant build\" to compile the source files. Then run \"ant ProfileMapPut\". It measures the runtime of \"HashMap.put\" (provided by Java) with a range of problem sizes, and plots runtime versus problem size on a log-log scale. If this operation is constant time, the total time for n operations should be linear, so the result should be a straight line with slope 1. When I ran this code, the estimated slope was close to 1, which is consistent with our analysis. You should get something similar. Modify \"ProfileMapPut.java\" so it profiles your implementation, \"MyHashMap\", instead of Java's \"HashMap\". Run the profiler again and see if the slope is near 1. You might have to adjust \"startN\" and \"endMillis\" to find a range of problem sizes where the runtimes are more than a few milliseconds, but not more than a few thousand. When I ran this code, I got a surprise: the slope was about 1.7, which suggests that this implementation is not constant time after all. It contains a \"performance bug\".  Before you read the next section, you should track down the error, fix it, and confirm that \"put\" is now constant time, as expected."], ["size final value size, constant time, little difficult invoke method superclass, size invokes, new class", "The problem with \"MyHashMap\" is in \"size\", which is inherited from \"MyBetterMap\": To add up the total size it has to iterate the sub-maps. Since we increase the number of sub-maps, k, as the number of entries, n, increases, k is proportional to n, so \"size\" is linear. And that makes \"put\" linear, too, because it uses \"size\": Everything we did to make \"put\" constant time is wasted if \"size\" is linear! Fortunately, there is a simple solution, and we have seen it before: we have to keep the number of entries in an instance variable and update it whenever we call a method that changes it. You'll find my solution in the repository for this book, in \"MyFixedHashMap.java\".  Here's the beginning of the class definition: Rather than modify \"MyHashMap\", I define a new class that extends it. It adds a new instance variable, \"size\", which is initialized to zero. Updating \"clear\" is straightforward; we invoke \"clear\" in the superclass (which clears the sub-maps), and then update \"size\". Updating \"remove\" and \"put\" is a little more difficult because when we invoke the method on the superclass, we can't tell whether the size of the sub-map changed. Here's how I worked around that: \"remove\" uses \"chooseMap\" to find the right sub-map, then subtracts away the size of the sub-map. It invokes \"remove\" on the sub-map, which may or may not change the size of the sub-map, depending on whether it finds the key. But either way, we add the new size of the sub-map back to \"size\", so the final value of \"size\" is correct. The rewritten version of \"put\" is similar: We have the same problem here: when we invoke \"put\" on the sub-map, we don't know whether it added a new entry. So we use the same solution, subtracting off the old size and then adding in the new size. Now the implementation of the \"size\" method is simple: And that's pretty clearly constant time. When I profiled this solution, I found that the total time for putting n keys is proportional to n, which means that each \"put\" is constant time, as it's supposed to be."], ["implements map myhashmap, implements map myfixedhashmap, multiple instances mylinearmap, line indicate class implements, diagram every class implements map", "One challenge of working with the code in this chapter is that we have several classes that depend on each other. Here are some of the relationships between the classes:\n- \"MyLinearMap\" contains a \"LinkedList\" and implements \"Map\".\n- \"MyBetterMap\" contains many \"MyLinearMap\" objects and implements \"Map\".\n- \"MyHashMap\" extends \"MyBetterMap\", so it also contains \"MyLinearMap\" objects, and it implements \"Map\".\n- \"MyFixedHashMap\" extends \"MyHashMap\" and implements \"Map\".\nDifferent relationships are represented by different arrows:\n- Arrows with a solid head indicate HAS-A relationships. For example, each instance of \"MyBetterMap\" contains multiple instances of \"MyLinearMap\", so they are connected by a solid arrow.\n- Arrows with a hollow head and a solid line indicate IS-A relationships. For example, \"MyHashMap\" extends \"MyBetterMap\", so they are connected by an IS-A arrow.\n- Arrows with a hollow head and a dashed line indicate that a class implements an interface; in this diagram, every class implements \"Map\"."], ["relationships classes mylinearmap, uml class diagram class, mental map project deployment document design, many mylinearmap, concise way represent lot information collection classes, code chapter several classes, alternative designs implementation, several graphical standards, arrow arrows hollow head, unified modeling language, box relationships classes, arrows uml class diagrams, http class diagram", "To help keep track of relationships like these, software engineers often use **UML class diagrams**. UML stands for Unified Modeling Language (see http://thinkdast.com/uml). A \"class diagram\" is one of several graphical standards defined by UML. In a class diagram, each class is represented by a box, and relationships between classes are represented by arrows. me/.\nUML class diagrams provide a concise way to represent a lot of information about a collection of classes. They are used during design phases to communicate about alternative designs, during implementation phases to maintain a shared mental map of the project, and during deployment to document the design."], ["binary search trees, good constant time, performance hashmap, binary search tree, keys order hard solve problems time java", "At this point you should be familiar with the \"Map\" interface and the \"HashMap\" implementation provided by Java. And by making your own \"Map\" using a hash table, you should understand how \"HashMap\" works and why we expect its core methods to be constant time. Because of this performance, \"HashMap\" is widely used, but it is not the only implementation of \"Map\". There are a few reasons you might want another implementation:\n- Hashing can be slow, so even though \"HashMap\" operations are constant time, the \"constant\" might be big.\n- Hashing works well if the hash function distributes the keys evenly among the sub-maps.  But designing good hash functions is not easy, and if too many keys end up in the same sub-map, the performance of the \"HashMap\" may be poor.\n- The keys in a hash table are not stored in any particular order; in fact, the order might change when the table grows and the keys are rehashed. For some applications, it is necessary, or at least useful, to keep the keys in order.\nIt is hard to solve all of these problems at the same time, but Java provides an implementation called \"TreeMap\" that comes close:\n- It doesn't use a hash function, so it avoids the cost of hashing and the difficulty of choosing a hash function.\n- Inside the \"TreeMap\", the keys are are stored in a\n**binary search tree**, which makes it possible to traverse the keys, in order, in linear time.\n- The runtime of the core methods is proportional to log n, which is not quite as good as constant time, but it is still very good.\nIn the next section, I'll explain how binary search trees work and then you will use one to implement a \"Map\". Along the way, we'll analyze the performance of the core map methods when implemented using a tree."], ["current key search, child keys, root use, right target, keys general number comparisons", "A binary search tree (BST) is a tree where each node contains a key, and every \"node\" has the \"BST property\":\n- If \"node\" has a left child, all keys in the left subtree must be less than the key in \"node\".\n- If \"node\" has a right child, all keys in the right subtree must be greater than the key in \"node\".\nThis figure is from the Wikipedia page on binary search trees at http://thinkdast.com/bst, which you might find useful while you work on this exercise. The key in the root is 8, and you can confirm that all keys to the left of the root are less than 8, and all keys to the right are greater. You can also check that the other nodes have this property. Looking up a key in a binary search tree is fast because we don't have to search the entire tree. Starting at the root, we can use the following algorithm:\n- Compare the key you are looking for, \"target\", to the key in the current node. If they are equal, you are done.\n- If \"target\" is less than the current key, search the left tree. If there isn't one, \"target\" is not in the tree.\n- If \"target\" is greater than the current key, search the right tree. If there isn't one, \"target\" is not in the tree.\nAt each level of the tree, you only have to search one child. For example, if you look for \"target = 4\" in the previous diagram, you start at the root, which contains the key \"8\". Because \"target\" is less than \"8\", you go left. Because \"target\" is greater than \"3\" you go right. Because \"target\" is less than \"6\", you go left. And then you find the key you are looking for. In this example, it takes four comparisons to find the target, even though the tree contains nine keys. In general, the number of comparisons is proportional to the height of the tree, not the number of keys in the tree. So what can we say about the relationship between the height of the tree, \"h\", and the number of nodes, n? Starting small and working up:\n- If \"h=1\", the tree only contains one node, so \"n=1\".\n- If \"h=2\", we can add two more nodes, for a total of \"n=3\".\n- If \"h=3\", we can add up to four more nodes, for a total of \"n=7\".\n- If \"h=4\", we can add up to eight more nodes, for a total of \"n=15\"."], ["sides n h means, full level contains maximum number nodes, tree proportional log n, time proportional log n, h level index nodes total number nodes h levels, key binary search tree time proportional log, logarithmic log time belongs order growth log n, logarithm base, pattern number levels", "By now you might see the pattern. If we number the levels of the tree from \"1\" to \"h\", the level with index \"i\" can have up to 2^{i-1} nodes. And the total number of nodes in \"h\" levels is 2^h-1. If we have n = 2^h - 1, we can take the logarithm base 2 of both sides: log_2 n ≈ h, which means that the height of the tree is proportional to log n, if the tree is full; that is, if each level contains the maximum number of nodes. So we expect that we can look up a key in a binary search tree in time proportional to log n. This is true if the tree is full, and even if the tree is only partially full. But it is not always true, as we will see. An algorithm that takes time proportional to log n is called \"logarithmic\" or \"log time\", and it belongs to the order of growth O(log n)."], ["able compare keys, exception implementations map, target null findnode, java type system point exercise job fill, throws exception", "In the repository for this book, you'll find these source files:\n- \"MyTreeMap.java\" contains the code from the previous section with outlines for the missing methods.\n- \"MyTreeMapTest.java\" contains the unit tests for \"MyTreeMap\".\nRun \"ant build\" to compile the source files. Then run \"ant MyTreeMapTest\".  Several tests should fail, because you have some work to do! I've provided outlines for \"get\" and \"containsKey\".  Both of them use \"findNode\", which is a private method I defined; it is not part of the \"Map\" interface. Here's how it starts: The parameter \"target\" is the key we're looking for. If \"target\" is \"null\", \"findNode\" throws an exception. Some implementations of \"Map\" can handle \"null\" as a key, but in a binary search tree, we need to be able to compare keys, so dealing with \"null\" is problematic. To keep things simple, this implementation does not allow \"null\" as a key. The next lines show how we can compare \"target\" to a key in the tree. From the signature of \"get\" and \"containsKey\", the compiler considers \"target\" to be an \"Object\". But we need to be able to compare keys, so we typecast \"target\" to \"Comparable<? super K>\", which means that it is comparable to an instance of type \"K\", or any superclass of \"K\".  If you are not familiar with this use of \"type wildcards\", you can read more at http://thinkdast.com/gentut. Fortunately, dealing with Java's type system is not the point of this exercise. Your job is to fill in the rest of \"findNode\". If it finds a node that contains \"target\" as a key, it should return the node. Otherwise it should return \"null\". When you get this working, the tests for \"get\" and \"containsKey\" should pass. Note that your solution should only search one path through the tree, so it should take time proportional to the height of the tree. You should not search the whole tree! Your next task is to fill in \"containsValue\". To get you started, I've provided a helper method, \"equals\", that compares \"target\" and a given key. Note that the values in the tree (as opposed to the keys) are not necessarily comparable, so we can't use \"compareTo\"; we have to invoke \"equals\" on \"target\". Unlike your previous solution for \"findNode\", your solution for \"containsValue\" does have to search the whole tree, so its runtime is proportional to the number of keys, n, not the height of the tree, \"h\". The next method you should fill in is \"put\". I've provided starter code that handles the simple cases: If you try to put \"null\" as a key, \"put\" throws an exception. If the tree is empty, \"put\" creates a new node and initializes the instance variable \"root\". Otherwise, it calls \"putHelper\", which is a private method I defined; it is not part of the \"Map\" interface. Fill in \"putHelper\" so it searches the tree and:\n- If \"key\" is already in the tree, it replaces the old value with the new, and returns the old value.\n- If \"key\" is not in the tree, it creates a new node, finds the right place to add it, and returns \"null\"."], ["performance core methods, easier search, todo help recursion, keys order, next chapter, documentation http method return set, time proportional height tree h number elements ideally, write helper method, order hint, compareto method hashset implementation set, outline keyset", "Your implementation of \"put\" should take time proportional to the height of the tree, h, not the number of elements, n. Ideally you should search the tree only once, but if you find it easier to search twice, you can do that; it will be slower, but it doesn't change the order of growth. Finally, you should fill in the body of \"keySet\".  According to the documentation at http://thinkdast.com/mapkeyset, this method should return a \"Set\" that iterates the keys in order; that is, in increasing order according to the \"compareTo\" method.  The \"HashSet\" implementation of \"Set\", which we used in Section~\\ref{exercise6}, doesn't maintain the order of the keys, but the \"LinkedHashSet\" implementation does.  You can read about it at http://thinkdast.com/linkedhashset. I've provided an outline of \"keySet\" that creates and returns a \"LinkedHashSet\": You should finish off this method so it adds the keys from the tree to \"set\" in ascending order. HINT: you might want to write a helper method; you might want to make it recursive; and you might want to read about in-order tree traversal at http://thinkdast.com/inorder."], ["performance core methods, easier search, todo help recursion, write helper method, change order growth finally, next chapter, time proportional height tree h number elements ideally, compareto method hashset implementation set", "\\index{in-order}  % TODO: more help with recursion? When you are done, all tests should pass. In the next chapter, I'll go over my solutions and test the performance of the core methods."], ["node time loop, k supertype k actual search, part map interface parameter target, permissive possible works type implements, target target", "In the previous exercise I gave you the outline of \"MyTreeMap\" and asked you to fill in the missing methods. Now I'll present a solution, starting with \"findNode\": \"findNode\" is a private method used by \"containsKey\" and \"get\"; it is not part of the \"Map\" interface. The parameter \"target\" is the key we're looking for. I explained the first part of this method in the previous exercise:\n- In this implementation, \"null\" is not a legal value for a key.\n- Before we can invoke \"compareTo\" on \"target\", we have to typecast it to some kind of \"Comparable\". The \"type wildcard\" used here is as permissive as possible; that is, it works with any type that implements \"Comparable\" and whose \"compareTo\" method accepts \"K\" or any supertype of \"K\".\nAfter all that, the actual search is relatively simple. We initialize a loop variable \"node\" so it refers to the root node. Each time through the loop, we compare the target to \"node.key\". If the target is less than the current key, we move to the left child. If it's greater, we move to the right child. And if it's equal, we return the current node. If we get to the bottom of the tree without finding the target, we conclude that it is not in the tree and return \"null\"."], ["third case, path tree, fourth case searches, previous exercise runtime, proportional height tree number, null means, first statement checks base case recursion, search whole tree containsvalue search values, time proportional number nodes, target return false note", "As I explained in the previous exercise, the runtime of \"findNode\" is proportional to the height of the tree, not the number of nodes, because we don't have to search the whole tree. But for \"containsValue\", we have to search the values, not the keys; the BST property doesn't apply to the values, so we have to search the whole tree. My solution is recursive: \"containsValue\" takes the target value as a parameter and immediately invokes \"containsValueHelper\", passing the root of the tree as an additional parameter. Here's how \"containsValueHelper\" works:\n- The first \"if\" statement checks the base case of the recursion. If \"node\" is \"null\", that means we have recursed to the bottom of the tree without finding the \"target\", so we should return \"false\". Note that this only means that the target did not appear on one path through the tree; it is still possible that it will be found on another.\n- The second case checks whether we've found what we're looking for. If so, we return \"true\". Otherwise, we have to keep going.\n- The third case makes a recursive call to search for \"target\" in the left subtree. If we find it, we can return \"true\" immediately, without searching the right subtree. Otherwise, we keep going.\n- The fourth case searches the right subtree. Again, if we find what we are looking for, we return \"true\". Otherwise, having searched the whole tree, we return \"false\".\nThis method \"visits\" every node in the tree, so it takes time proportional to the number of nodes."], ["empty return, otherwise traverse, null means, parameter node, subtree order add traverse", "The last method I asked you to write is \"keySet\", which returns a \"Set\" that contains the keys from the tree in ascending order. In other implementations of \"Map\", the keys returned by \"keySet\" are in no particular order, but one of the capabilities of the tree implementation is that it is simple and efficient to sort the keys. So we should take advantage of that. Here's my solution: In \"keySet\", we create a \"LinkedHashSet\", which is a \"Set\" implementation that keeps the elements in order (unlike most other \"Set\" implementations). Then we call \"addInOrder\" to traverse the tree. The first parameter, \"node\", is initially the root of the tree, but as you should expect by now, we use it to traverse the tree recursively. \"addInOrder\" performs a classic \"in-order traversal\" of the tree. If \"node\" is \"null\", that means the subtree is empty, so we return without adding anything to \"set\". Otherwise we:\n- Traverse the left subtree in order.\n- Add \"node.key\".\n- Traverse the right subtree in order.\nRemember that the BST property guarantees that all nodes in the left subtree are less than \"node.key\", and all nodes in the right subtree are greater. So we know that \"node.key\" has been added in the correct order. Applying the same argument recursively, we know that the elements from the left subtree are in order, as well as the elements from the right subtree.  And the base case is correct: if the subtree is empty, no keys are added.  So we can conclude that this method adds all keys in the correct order. Because this method visits every node in the tree, like \"containsValue\", it takes time proportional to n."], ["size mytreemap, previous case, particular order final height tree, tree contains right children, unique identifier uuids useful variety applications example", "In \"MyTreeMap\", the methods \"get\" and \"put\" take time proportional to the height of the tree, h. In the previous exercise, we showed that if the tree is full --- if every level of the tree contains the maximum number of nodes --- the height of the tree is proportional to log n. And I claimed that \"get\" and \"put\" are logarithmic; that is, they take time proportional to log n. But for most applications, there's no guarantee that the tree is full. In general, the shape of the tree depends on the keys and what order they are added. To see how this works out in practice, we'll test our implementation with two sample datasets: a list of random strings and a list of timestamps in increasing order. Here's the code that generates random strings: \"UUID\" is a class in the \"java.util\" package that can generate a random \"universally unique identifier\". UUIDs are useful for a variety of applications, but in this example we're taking advantage of an easy way to generate random strings. I ran this code with \"n=16384\" and measured the runtime and the height of the final tree. Here's the output: I included \"log base 2 of size of MyTreeMap\" to see how tall the tree would be if it were full. The result indicates that a full tree with height 14 would contain 16,384 nodes. The actual tree of random strings has height 33, which is substantially more than the theoretical minimum, but not too bad. To find one key in a collection of 16,384, we only have to make 33 comparisons. Compared to a linear search, that's almost 500 times faster. This performance is typical with random strings or other keys that are added in no particular order. The final height of the tree might be 2-3 times the theoretical minimum, but it is still proportional to log n, which is much less than n. In fact, log n grows so slowly as n increases, it can be difficult to distinguish logarithmic time from constant time in practice. However, binary search trees don't always behave so well. Let's see what happens when we add keys in increasing order. Here's an example that measures timestamps in nanoseconds and uses them as keys: \"System.nanoTime\" returns an integer with type \"long\" that indicates elapsed time in nanoseconds. Each time we call it, we get a somewhat bigger number. When we convert these timestamps to strings, they appear in increasing alphabetical order. And let's see what happens when we run it: The runtime is more than seven times longer than in the previous case. longer. If you wonder why, take a look at the final height of the tree: 16384!\nIf you think about how \"put\" works, you can figure out what's going on. Every time we add a new key, it's larger than all of the keys in the tree, so we always choose the right subtree, and always add the new node as the right child of the rightmost node. The result is an \"unbalanced\" tree that only contains right children. The height of this tree is proportional to n, not log n, so the performance of \"get\" and \"put\" is linear, not logarithmic. In the balanced tree, the height is 4 and the total number of nodes is 2^4-1 = 15.  In the unbalanced tree with the same number of nodes, the height is 15."], ["read trees, time hash summary binary search tree implement get, order keeps, random strings timestamps fact timestamps, avl tree avl initials inventors", "There are two possible solutions to this problem:\n- You could avoid adding keys to the \"Map\" in order. But this is not always possible.\n- You could make a tree that does a better job of handling keys if they happen to be in order.\nThe second solution is better, and there are several ways to do it. The most common is to modify \"put\" so that it detects when the tree is starting to become unbalanced and, if so, rearranges the nodes. Trees with this capability are called \"self-balancing\". Common self-balancing trees include the AVL tree (\"AVL\" are the initials of the inventors), and the red-black tree, which is what the Java \"TreeMap\" uses. In our example code, if we replace \"MyTreeMap\" with the Java \"TreeMap\", the runtimes are about the same for the random strings and the timestamps. In fact, the timestamps run faster, even though they are in order, probably because they take less time to hash. In summary, a binary search tree can implement \"get\" and \"put\" in logarithmic time, but only if the keys are added in an order that keeps the tree sufficiently balanced. Self-balancing trees avoid this problem by doing some additional work each time a new key is added. You can read more about self-balancing trees at http://thinkdast.com/balancing."], ["figure read explanation, idea trees, previous exercise implement remove, tree similar operations, node middle tree rearrange, bst property", "In the previous exercise you didn't have to implement \"remove\", but you might want to try it. If you remove a node from the middle of the tree, you have to rearrange the remaining nodes to restore the BST property.  You can probably figure out how to do that on your own, or you can read the explanation at http://thinkdast.com/bstdel. Removing a node and rebalancing a tree are similar operations: if you do this exercise, you will have a better idea of how self-balancing trees work."], ["data structures, different capabilities, data persistent store file program ends, databases simple way, file system", "The previous version of the indexer stores the index in two data structures: a \"TermCounter\" that maps from a search term to the number of times it appears on a web page, and an \"Index\" that maps from a search term to the set of pages where it appears. These data structures are stored in the memory of a running Java program, which means that when the program stops running, the index is lost. Data stored only in the memory of a running program is called \"volatile\", because it vaporizes when the program ends. Data that persists after the program that created it ends is called \"persistent\". In general, files stored in a file system are persistent, as well as data stored in databases. A simple way to make data persistent is to store it in a file. Before the program ends, it could translate its data structures into a format like JSON (http://thinkdast.com/json) and then write them into a file. When it starts again, it could read the file and rebuild the data structures. But there are several problems with this solution:\n- Reading and writing large data structures (like a Web index) would be slow.\n- The entire data structure might not fit into the memory of a single running program.\n- If a program ends unexpectedly (for example, due to a power outage), any changes made since the program last started would be lost.\nA better alternative is a database that provides persistent storage and the ability to read and write parts of the database without reading and writing the whole thing. There are many kinds of database management systems (DBMS) that provide different capabilities. You can read an overview at http://thinkdast.com/database. The database I recommend for this exercise is Redis, which provides persistent data structures that are similar to Java data structures. Specifically, it provides:\n- Lists of strings, similar to Java \"List\".\n- Hashes, similar to Java \"Map\".\n- Sets of strings, similar to Java \"Set\"."], ["redis database, data structures contains values, unique strings keys key redis, role reference java identifies, data structures", "Redis is a \"key-value database\", which means that the data structures it contains (the values) are identified by unique strings (the keys). A key in Redis plays the same role as a reference in Java: it identifies an object. We'll see some examples soon."], ["host name port number example instance, new instance, remote service fact name, top page, redis cloud", "Redis is usually run as a remote service; in fact, the name stands for \"REmote DIctionary Server\". To use Redis, you have to run the Redis server somewhere and then connect to it using a Redis client. There are many ways to set up a server and many clients you could use. For this exercise, I recommend:\n- Rather than install and run the server yourself, consider using a service like RedisToGo (http://thinkdast.com/redistogo), which runs Redis in the cloud. They offer a free plan with enough resources for the exercise.\n- For the client I recommend Jedis, which is a Java library that provides classes and methods for working with Redis.\nHere are more detailed instructions to help you get started:\n- Create an account on RedisToGo, at http://thinkdast.com/redissign, and select the plan you want (probably the free plan to get started).\n- Create an \"instance\", which is a virtual machine running the Redis server. If you click on the \"Instances\" tab, you should see your new instance, identified by a host name and a port number. For example, I have an instance named \"dory-10534\".\n- Click on the instance name to get the configuration page. Make a note of the URL near the top of the page, which looks like this:"], ["host name port number, remote service fact name, information next step, password need, letters numbers, exercise recommend rather install", "This URL contains the server's host name, \"dory.redistogo.com\", the port number, \"10534\", and the password you will need to connect to the server, which is the long string of letters and numbers in the middle. You will need this information for the next step."], ["solutions solutions, java data, previous exercises confident solutions, test code jedisindex, redis file", "In the repository for this book, you'll find the source files for this exercise:\n- \"JedisMaker.java\" contains example code for connecting to a Redis server and running a few Jedis methods.\n- \"JedisIndex.java\" contains starter code for this exercise.\n- \"JedisIndexTest.java\" contains test code for \"JedisIndex\".\n- \"WikiFetcher.java\" contains the code we saw in previous exercises to read web pages and parse them using jsoup.\nYou will also need these files, which you worked on in previous exercises:\n- \"Index.java\" implements an index using Java data structures.\n- \"TermCounter.java\" represents a map from terms to their frequencies.\n- \"WikiNodeIterable.java\" iterates through the nodes in a DOM tree produced by jsoup.\nIf you have working versions of these files, you can use them for this exercise.  If you didn't do the previous exercises, or you are not confident in your solutions, you can copy my solutions from the {\\tt solutions} folder. The first step is to use Jedis to connect to your Redis server. \"RedisMaker.java\" shows how to do this. It reads information about your Redis server from a file, connects to it and logs in using your password, then returns a \"Jedis\" object you can use to perform Redis operations. If you open \"JedisMaker.java\", you should see the \"JedisMaker\" class, which is a helper class that provides one static method, \"make\", which creates a \"Jedis\" object. Once this object is authenticated, you can use it to communicate with your Redis database. \"JedisMaker\" reads information about your Redis server from a file named \"redis_url.txt\", which you should put in the directory \"src/resources\":\n- Use a text editor to create end edit \"ThinkDataStructures/code/src/resources/redis_url.txt\".\n- Paste in the URL of your server. If you are using RedisToGo, the URL will look like this: \"redis://redistogo:1234567feedfacebeefa1e1234567@dory.redistogo.com:10534\"\nfile that makes it harder (but not impossible) to put this file in your repo. Now run \"ant build\" to compile the source files and \"ant JedisMaker\" to run the example code in \"main\": This example demonstrates the data types and methods you are most likely to use for this exercise. When you run it, the output should be: In the next section, I'll explain how the code works."], ["solutions solutions, redis server, java data, file contains, previous exercises confident solutions, test code jedisindex, creates jedis, example code main example, file public repository help, starter code exercise, information redis server file, exercise run output next section explain code works, accident repository contains, redis file", "Because this file contains the password for your Redis server, you should not put this file in a public repository. To help you avoid doing that by accident, the repository contains a {\\tt .gitignore}"], ["hash structure similar java map string string method, value look, adding elements, elements example, use create", "Redis is basically a map from keys, which are strings, to values, which can be one of several data types. The most basic Redis data type is a \\redis{string}.  I will write Redis types in italics to distinguish them from Java types.  To add a \\redis{string} to the database, use \"jedis.set\", which is similar to \"Map.put\"; the parameters are the new key and the corresponding value. To look up a key and get its value, use \"jedis.get\": In this example, the key is \"\"mykey\"\" and the value is \"\"myvalue\"\". Redis provides a \\redis{set} structure, which is similar to a Java \"Set<String>\". To add elements to a Redis \\redis{set}, you choose a key to identify the \\redis{set} and then use \"jedis.sadd\": You don't have to create the \\redis{set} as a separate step. If it doesn't exist, Redis creates it. In this case, it creates a \\redis{set} named \"myset\" that contains three elements. The method \"jedis.sismember\" checks whether an element is in a\n\\redis{set}. Adding elements and checking membership are constant time operations. Redis also provides a \\redis{list} structure, which is similar to a Java \"List<String>\". The method \"jedis.rpush\" adds elements to the end (right side) of a\n\\redis{list}: Again, you don't have to create the structure before you start adding elements. This example creates a \\redis{list} named \"mylist\" that contains three elements. The method \"jedis.lindex\" takes an integer index and returns the indicated element of a \\redis{list}. Adding and accessing elements are constant time operations. Finally, Redis provides a \\redis{hash} structure, which is similar to a Java \"Map<String, String>\". The method \"jedis.hset\" adds a new entry to the \\redis{hash}: This example creates a \\redis{hash} named \"myhash\" that contains one entry, which maps from the key \"word1\" to the value \"\"2\"\". The keys and values are \\redis{string}s, so if we want to store an \"Integer\", we have to convert it to a \"String\" before we call \"hset\". And when we look up the value using \"hget\", the result is a \"String\", so we might have to convert it back to \"Integer\". Working with Redis \\redis{hash}es can be confusing, because we use a key to identify which \\redis{hash} we want, and then another key to identify a value in the \\redis{hash}. In the context of Redis, the second key is called a \"field\", which might help keep things straight. So a \"key\" like \"myhash\" identifies a particular \\redis{hash}, and then a \"field\" like \"word1\" identifies a value in the \\redis{hash}. For many applications, the values in a Redis \\redis{hash} are integers, so Redis provides a few special methods, like \"hincrby\", that treat the values as numbers: This method accesses \"myhash\", gets the current value associated with \"word2\" (or 0 if it doesn't already exist), increments it by 1, and writes the result back to the \\redis{hash}. Setting, getting, and incrementing entries in a \\redis{hash} are constant time operations. You can read more about Redis data types at http://thinkdast.com/redistypes."], ["long time, things control, program mess something database fix start, time assemble pieces complete solution try write whole thing, clean database start fresh", "At this point you have all the information you need to do the exercise, so you can get started if you are ready. But I have a few suggestions you might want to read first:\n- For this exercise I provide less guidance than in previous exercises.  You will have to make some design decisions; in particular, you will have to figure out how to divide the problem into pieces that you can test one at a time, and then assemble the pieces into a complete solution. If you try to write the whole thing at once, without testing smaller pieces, it might take a very long time to debug.\n- One of the challenges of working with persistent data is that it is persistent. The structures stored in the database might change every time you run the program. If you mess something up in the database, you will have to fix it or start over before you can proceed. To help you keep things under control, I've provided methods called \"deleteURLSets\", \"deleteTermCounters\", and \"deleteAllKeys\", which you can use to clean out the database and start fresh. You can also use \"printIndex\" to print the contents of the index.\n- Each time you invoke a \"Jedis\" method, your client sends a message to the server, then the server performs the action you requested and sends back a message. If you perform many small operations, it will probably take a long time. You can improve performance by grouping a series of operations into a \"Transaction\"."], ["exec sends, long time, things control, communicate server saves, program mess something database fix start, invoke method transaction, time assemble pieces complete solution try write whole thing, clean database start fresh, methods jedis, speed transaction object returns transaction, example simple version deleteallkeys time, index contains pages", "For example, here's a simple version of \"deleteAllKeys\": Each time you invoke \"del\" requires a round-trip from the client to the server and back. If the index contains more than a few pages, this method would take a long time to run. We can speed it up with a \"Transaction\" object: \"jedis.multi\" returns a \"Transaction\" object, which provides all the methods of a \"Jedis\" object. But when you invoke a method on a \"Transaction\", it doesn't run the operation immediately, and it doesn't communicate with the server. It saves up a batch of operations until you invoke \"exec\". Then it sends all of the saved operations to the server at the same time, which is usually much faster."], ["good luck, tests first figure test method, basic redis commands, work methods, way divide things, methods ok", "Now you really have all the information you need; you should start working on the exercise. But if you get stuck, or if you really don't know how to get started, you can come back for a few more hints."], ["good luck, tests first figure test method, basic redis commands, work methods, way divide things, methods ok", "**Don't read the following until you have run the test code, tried out some basic Redis commands, and written a few methods in \"JedisIndex.java\"**. OK, if you are really stuck, here are some methods you might want to work on: These are the methods I used in my solution, but they are certainly not the only way to divide things up. So please take these suggestions if they help, but ignore them if they don't. For each method, consider writing the tests first. When you figure out how to test a method, you often get ideas about how to write it. Good luck!"], ["write getcounts, new pages second part exercise, field new term finds, url term, urls contain search term url termcounter redis hash maps", "In my solution, we store two kinds of structures in Redis:\n- For each search term, we have a \"URLSet\", which is a Redis \\redis{set}\nof URLs that contain the search term.\n- For each URL, we have a \"TermCounter\", which is a Redis \\redis{hash}\nthat maps each search term to the number of times it appears.\nWe discussed these data types in the previous chapter. You can also read about Redis structures at http://thinkdast.com/redistypes In \"JedisIndex\", I provide a method that takes a search term and returns the Redis key of its \"URLSet\": And a method that takes a URL and returns the Redis key of its \"TermCounter\": Here's the implementation of \"indexPage\", which takes a URL and a jsoup \"Elements\" object that contains the DOM tree of the paragraphs we want to index: To index a page, we\n- Make a Java \"TermCounter\" for the contents of the page, using code from a previous exercise.\n- Push the contents of the \"TermCounter\" to Redis.\nHere's the new code that pushes a \"TermCounter\" to Redis: This method uses a \"Transaction\" to collect the operations and send them to the server all at once, which is much faster than sending a series of small operations. It loops through the terms in the \"TermCounter\". For each one it\n- Finds or creates a \"TermCounter\" on Redis, then adds a field for the new term.\n- Finds or creates a \"URLSet\" on Redis, then adds the current URL.\nIf the page has already been indexed, we delete its old \"TermCounter\" before pushing the new contents. That's it for indexing new pages. The second part of the exercise asked you to write \"getCounts\", which takes a search term and returns a map from each URL where the term appears to the number of times it appears there. Here is my solution: This method uses two helper methods:\n- \"getURLs\" takes a search term and returns the Set of URLs where the term appears.\n- \"getCount\" takes a URL and a term and returns the number of times the term appears at the given URL."], ["index methods, number times, implementations way, current url page, write getcounts, indexed delete old termcounter, code previous exercise push contents termcounter redis new code pushes termcounter redis method, url term, urls contain search term url termcounter redis hash maps", "Here are the implementations: Because of the way we designed the index, these methods are simple and efficient."], ["large inside loop run getcount, time proportional number urls, unique search terms, entry hashmap constant time operations overall complexity getcounts n, entry hashmap geturls", "Suppose we have indexed N pages and discovered M unique search terms. How long will it take to look up a search term? Think about your answer before you continue. To look up a search term, we run \"getCounts\", which\n- Creates a map.\n- Runs \"getURLs\" to get a Set of URLs.\n- For each URL in the Set, runs \"getCount\" and adds an entry to a \"HashMap\".\n\"getURLs\" takes time proportional to the number of URLs that contain the search term. For rare terms, that might be a small number, but for common terms it might be as large as N. Inside the loop, we run \"getCount\", which finds a \"TermCounter\" on Redis, looks up a term, and adds an entry to a HashMap. Those are all constant time operations, so the overall complexity of \"getCounts\" is O(N) in the worst case. However, in practice the runtime is proportional to the number of pages that contain the term, which is normally much less than N. This algorithm is as efficient as it can be, in terms of algorithmic complexity, but it is very slow because it sends many small operations to Redis. You can make it faster using a \"Transaction\". You might want to do that as an exercise, or you can see my solution in \"RedisIndex.java\"."], ["search terms index, case performance, textnode objects, every urlset termcounter furthermore useful help, number words", "Using the data structures we designed, how long will it take to index a page? Again, think about your answer before you continue. To index a page, we traverse its DOM tree, find all the \"TextNode\" objects, and split up the strings into search terms. That all takes time proportional to the number of words on the page. For each term, we increment a counter in a HashMap, which is a constant time operation. So making the \"TermCounter\" takes time proportional to the number of words on the page. Pushing the \"TermCounter\" to Redis requires deleting a \"TermCounter\", which is linear in the number of unique terms. Then for each term we have to\n- Add an element to a \"URLSet\", and\n- Add an element to a Redis \"TermCounter\".\nBoth of these are constant time operations, so the total time to push the \"TermCounter\" is linear in the number of unique search terms. In summary, making the \"TermCounter\" is proportional to the number of words on the page. Pushing the \"TermCounter\" to Redis is proportional to the number of unique terms. Since the number of words on the page usually exceeds the number of unique search terms, the overall complexity is proportional to the number of words on the page. In theory a page might contain all search terms in the index, so the worst case performance is O(M), but we don't expect to see the worse case in practice. This analysis suggests a way to improve performance: we should probably avoid indexing very common words. First of all, they take up a lot of time and space, because they appear in almost every \"URLSet\" and \"TermCounter\". Furthermore, they are not very useful because they don't help identify relevant pages. Most search engines avoid indexing common words, which are known in this context as stop words (http://thinkdast.com/stopword)."], ["node another familiar graphs, kind crawler people, lifo stack crawler performs, urls collection works way collection, priority pages", "If you did the \"Getting to Philosophy\" exercise in Chapter~\\ref{getphilo}, you already have a program that reads a Wikipedia page, finds the first link, uses the link to load the next page, and repeats. This program is a specialized kind of crawler, but when people say \"Web crawler\" they usually mean a program that\n- Loads a starting page and indexes the contents,\n- Finds all the links on the page and adds the linked URLs to a collection, and\n- Works its way through the collection, loading pages, indexing them, and adding new URLs.\n- If it finds a URL that has already been indexed, it skips it.\nYou can think of the Web as a graph where each page is a node and each link is a directed edge from one node to another. If you are not familiar with graphs, you can read about them at http://thinkdast.com/graph. Starting from a source node, a crawler traverses this graph, visiting each reachable node once. The collection we use to store the URLs determines what kind of traversal the crawler performs:\n- If it's a first-in-first-out (FIFO) queue, the crawler performs a breadth-first traversal.\n- If it's a last-in-first-out (LIFO) stack, the crawler performs a depth-first traversal.\n- More generally, the items in the collection might be prioritized. For example, we might want to give higher priority to pages that have not been indexed for a long time.\nYou can read more about graph traversal at http://thinkdast.com/graphtrav."], ["source index initially, element source notice implementation queue linkedlist add elements, linkedlist object queue variable limit, test logic queue empty returns, offer add elements", "First, let's go over our solution to the previous exercise. I provided an outline of \"WikiCrawler\"; your job was to fill in \"crawl\". As a reminder, here are the fields in the \"WikiCrawler\" class: When we create a \"WikiCrawler\", we provide \"source\" and \"index\". Initially, \"queue\" contains only one element, \"source\". Notice that the implementation of \"queue\" is a \"LinkedList\", so we can add elements at the end --- and remove them from the beginning --- in constant time. By assigning a \"LinkedList\" object to a \"Queue\" variable, we limit ourselves to using methods in the \"Queue\" interface; specifically, we'll use \"offer\" to add elements and \"poll\" to remove them. Here's my implementation of \"WikiCrawler.crawl\": Most of the complexity in this method is there to make it easier to test. Here's the logic:\n- If the queue is empty, it returns \"null\" to indicate that it did not index a page.\n- Otherwise it removes and stores the next URL from the queue.\n- If the URL has already been indexed, \"crawl\" doesn't index it again, unless it's in testing mode.\n- Next it reads the contents of the page: if it's in testing mode, it reads from a file; otherwise it reads from the Web.\n- It indexes the page.\n- It parses the page and adds internal links to the queue.\n- Finally, it returns the URL of the page it indexed.\nI presented an implementation of \"Index.indexPage\" in Section~\\ref{redis-indexer}. So the only new method is \"WikiCrawler.queueInternalLinks\". I wrote two versions of this method with different parameters: one takes an \"Elements\" object containing one DOM tree per paragraph; the other takes an \"Element\" object that contains a single paragraph. The first version just loops through the paragraphs. The second version does the real work."], ["link internal check, url starts, chance bring pieces, links pages, exercise lot new material, simple test", "To determine whether a link is \"internal,\" we check whether the URL starts with \"/wiki/\". This might include some pages we don't want to index, like meta-pages about Wikipedia. And it might exclude some pages we want, like links to pages in non-English languages. But this simple test is good enough to get started. That's all there is to it. This exercise doesn't have a lot of new material; it is mostly a chance to bring the pieces together."], ["information retrieval, search terms, simple version, search results general term, interface users, next phase project implement search tool pieces, search results multiple search terms algorithms, http exercise focus steps, search term returns pages, results lookup mechanism, interested building web applications", "The next phase of this project is to implement a search tool. The pieces we'll need include:\n- An interface where users can provide search terms and view results.\n- A lookup mechanism that takes each search term and returns the pages that contain it.\n- Mechanisms for combining search results from multiple search terms.\n- Algorithms for ranking and sorting search results.\nThe general term for processes like this is \"information retrieval\", which you can read more about at http://thinkdast.com/infret. In this exercise, we'll focus on steps 3 and 4. We've already built a simple version of 2. If you are interested in building Web applications, you might consider working on step 1."], ["java s2, boolean searches, combine results multiple search terms, next section write method implement operations, java contain indonesia expressions", "Most search engines can perform \"boolean searches\", which means you can combine the results from multiple search terms using boolean logic. For example:\n- The search \"java AND programming\" might return only pages that contain both search terms: \"java\" and \"programming\".\n- \"java OR programming\" might return pages that contain either term but not necessarily both.\n- \"java -indonesia\" might return pages that contain \"java\" and do not contain \"indonesia\".\nExpressions like these that contain search terms and operators are called \"queries\". When applied to search results, the boolean operators \"AND\", \"OR\", and \"-\" correspond to the set operations \"intersection\", \"union\", and \"difference\". For example, suppose\n- \"s1\" is the set of pages containing \"java\",\n- \"s2\" is the set of pages containing \"programming\", and\n- \"s3\" is the set of pages containing \"indonesia\".\nIn that case:\n- The intersection of \"s1\" and \"s2\" is the set of pages containing \"java\" AND \"programming\".\n- The union of \"s1\" and \"s2\" is the set of pages containing \"java\" OR \"programming\".\n- The difference of \"s1\" and \"s2\" is the set of pages containing \"java\" and not \"indonesia\"."], ["java s2, boolean logic example search java programming, boolean searches, union s1 s2, combine results multiple search terms, next section write method implement operations, java contain indonesia expressions, java programming", "In the next section you will write a method to implement these operations."], ["make web application user interface, operators parse queries, alternatives build user interface, results sort relevance display urls consider, document frequencies total number times term", "If you get a basic version of this exercise working, you might want to work on these optional exercises:\n- Read about TF-IDF at http://thinkdast.com/tfidf and implement it. You might have to modify \"JavaIndex\" to compute document frequencies; that is, the total number of times each term appears on all pages in the index.\n- For queries with more than one search term, the total relevance for each page is currently the sum of the relevance for each term. Think about when this simple version might not work well, and try out some alternatives.\n- Build a user interface that allows users to enter queries with boolean operators. Parse the queries, generate the results, then sort them by relevance and display the highest-scoring URLs. Consider generating \"snippets\" that show where the search terms appeared on the page. If you want to make a Web application for your user interface, consider using Heroku as a simple option for developing and deploying Web applications using Java.  See http://thinkdast.com/heroku."], ["quadratic case, leading term expression, linear size list inner loop, simple describe, proportional n size array outer loop", "We'll start with insertion sort, mostly because it is simple to describe and implement. It is not very efficient, but it has some redeeming qualities, as we'll see. Rather than explain the algorithm here, I suggest you read the insertion sort Wikipedia page at http://thinkdast.com/insertsort, which includes pseudocode and animated examples. Come back when you get the general idea. Here's an implementation of insertion sort in Java: I define a class, \"ListSorter\", as a container for sort algorithms. By using the type parameter, \"T\", we can write methods that work on lists containing any object type. \"insertionSort\" takes two parameters, a \"List\" of any kind and a \"Comparator\" that knows how to compare type \"T\" objects. It sorts the list \"in place\", which means it modifies the existing list and does not have to allocate any new space. The following example shows how to call this method with a \"List\" of \"Integer\" objects: \"insertionSort\" has two nested loops, so you might guess that its runtime is quadratic. In this case, that turns out to be correct, but before you jump to that conclusion, you have to check that the number of times each loop runs is proportional to n, the size of the array. The outer loop iterates from 1 to \"list.size()\", so it is linear in the size of the list, n. The inner loop iterates from \"i\" to 0, so it is also linear in n. Therefore, the total number of times the inner loop runs is quadratic. If you are not sure about that, here's the argument:\n- The first time through, i=1 and the inner loop runs at most once.\n- The second time, i=2 and the inner loop runs at most twice.\n- The last time, i=n-1 and the inner loop runs at most n-1 times.\nSo the total number of times the inner loop runs is the sum of the series 1, 2, ... , n-1, which is n (n-1) / 2. And the leading term of that expression (the one with the highest exponent) is n^2. In the worst case, insertion sort is quadratic. However:\n- If the elements are already sorted, or nearly so, insertion sort is linear. Specifically, if each element is no more than k locations away from where it should be, the inner loop never runs more than k times, and the total runtime is O(kn).\n- Because the implementation is simple, the overhead is low; that is, although the runtime is a n^2, the coefficient of the leading term, a, is probably small.\nSo if we know that the array is nearly sorted, or is not very big, insertion sort might be a good choice. But for large arrays, we can do better. In fact, much better."], ["many times, number levels, lists elements, list contains n elements steps, terms levels recursion", "To classify the runtime of merge sort, it helps to think in terms of levels of recursion and how much work is done on each level. Suppose we start with a list that contains n elements. Here are the steps of the algorithm:\n- Make two new arrays and copy half of the elements into each.\n- Sort the two halves.\n- Merge the halves.\nAt the top level, we have 1 list with n elements. For simplicity, let's assume n is a power of 2. At the next level there are 2 lists with n/2 elements. Then 4 lists with n/4 elements, and so on until we get to n lists with 1 element. On every level we have a total of n elements. On the way down, we have to split the arrays in half, which takes time proportional to n on every level. On the way back up, we have to merge a total of n elements, which is also linear. If the number of levels is h, the total amount of work for the algorithm is O(nh). So how many levels are there? There are two ways to think about that:\n- How many times do we have to cut n in half to get to 1?\n- Or, how many times do we have to double 1 before we get to n?"], ["different picture computation, total amount work, first step copies, linear third step, many times, lists elements, figure complexity step, levels recursion, list n elements simplicity", "The first step copies each of the elements once, so it is linear. The third step also copies each element once, so it is also linear. Now we need to figure out the complexity of step 2. To do that, it helps to looks at a different picture of the computation, which shows the levels of recursion, as in"], ["n log n, next section sorts, n n taking sides yields, n log see http, comparison sort", "Another way to ask the second question is \"What power of 2 is n?\" 2^h = n Taking the log_2 of both sides yields h = log_2 n So the total time is O(n log n). I didn't bother to write the base of the logarithm because logarithms with different bases differ by a constant factor, so all logarithms are in the same order of growth. Algorithms in O(n log n) are sometimes called \"linearithmic\", but most people just say \"n log n\". It turns out that O(n log n) is the theoretical lower bound for sort algorithms that work by comparing elements to each other. That means there is no \"comparison sort\" whose order of growth is better than n log n.  See http://thinkdast.com/compsort. But as we'll see in the next section, there are non-comparison sorts that take linear time!"], ["efficient way sort, second letter words, first letter words, perform impromptu algorithm analysis, united states presidential campaign candidate barack obama", "During the 2008 United States Presidential Campaign, candidate Barack Obama was asked to perform an impromptu algorithm analysis when he visited Google. Chief executive Eric Schmidt jokingly asked him for \"the most efficient way to sort a million 32-bit integers.\" Obama had apparently been tipped off, because he quickly replied, \"I think the bubble sort would be the wrong way to go.\" You can watch the video at http://thinkdast.com/obama. Obama was right: bubble sort is conceptually simple but its runtime is quadratic; and even among quadratic sort algorithms, its performance is not very good.  See http://thinkdast.com/bubble. The answer Schmidt was probably looking for is \"radix sort\", which is a **non-comparison** sort algorithm that works if the size of the elements is bounded, like a 32-bit integer or a 20-character string. To see how this works, imagine you have a stack of index cards where each card contains a three-letter word. Here's how you could sort the cards:\n- Make one pass through the cards and divide them into buckets based on the first letter. So words starting with \"a\" should be in one bucket, followed by words starting with \"b\", and so on.\n- Divide each bucket again based on the second letter. So words starting with \"aa\" should be together, followed by words starting with \"ab\", and so on. Of course, not all buckets will be full, but that's OK.\n- Divide each bucket again based on the third letter."], ["order top row shows, words second row, letter second pass words, version radix sort, buckets long buckets", "At this point each bucket contains one element, and the buckets are sorted in ascending order.\nThe top row shows the unsorted words. The second row shows what the buckets look like after the first pass. The words in each bucket begin with the same letter. After the second pass, the words in each bucket begin with the same two letters. After the third pass, there can be only one word in each bucket, and the buckets are in order. During each pass, we iterate through the elements and add them to buckets. As long as the buckets allow addition in constant time, each pass is linear. The number of passes, which I'll call w, depends on the \"width\" of the words, but it doesn't depend on the number of words, n. So the order of growth is O(wn), which is linear in n. There are many variations on radix sort, and many ways to implement each one. You can read more about them at http://thinkdast.com/radix. As an optional exercise, consider writing a version of radix sort."], ["heap sort, time proportional n log n, elements extra work rebalance tree result, day select top, heap every node heap property takes log n time poll", "In addition to radix sort, which applies when the things you want to sort are bounded in size, there is one other special-purpose sorting algorithm you might encounter: bounded heap sort. Bounded heap sort is useful if you are working with a very large dataset and you want to report the \"Top 10\" or \"Top k\" for some value of k much smaller than n. For example, suppose you are monitoring a Web service that handles a billion transactions per day. At the end of each day, you want to report the k biggest transactions (or slowest, or any other superlative). One option is to store all transactions, sort them at the end of the day, and select the top k. That would take time proportional to n log n, and it would be very slow because we probably can't fit a billion transactions in the memory of a single program. We would have to use an \"out of core\" sort algorithm. You can read about external sorting at http://thinkdast.com/extsort. Using a bounded heap, we can do much better! Here's how we will proceed:\n- I'll explain (unbounded) heap sort.\n- You'll implement it.\n- I'll explain bounded heap sort and analyze it.\nTo understand heap sort, you have to understand a heap, which is a data structure similar to a binary search tree (BST). Here are the differences:\n- In a BST, every node, \"x\", has the \"BST property\": all nodes in the left subtree of \"x\" are less than \"x\" and all nodes in the right subtree are greater than \"x\".\n- In a heap, every node, \"x\", has the \"heap property\": all nodes in both subtrees of \"x\" are greater than \"x\".\n- Heaps are like balanced BSTs; when you add or remove elements, they do some extra work to rebalance the tree.  As a result, they can be implemented efficiently using an array of elements.\nThe smallest element in a heap is always at the root, so we can find it in constant time. Adding and removing elements from a heap takes time proportional to the height of the tree h. And because the heap is always balanced, h is proportional to log n.  You can read more about heaps at http://thinkdast.com/heap. The Java \"PriorityQueue\" is implemented using a heap. \"PriorityQueue\" provides the methods specified in the \"Queue\" interface, including \"offer\" and \"poll\":\n- \"offer\": Adds an element to the queue, updating the heap so that every node has the \"heap property\". Takes log n time.\n- \"poll\": Removes the smallest element in the queue from the root and updates the heap. Takes log n time.\nGiven a \"PriorityQueue\", you can easily sort of a collection of n elements like this:\n- Add all elements of the collection to a \"PriorityQueue\" using \"offer\".\n- Remove the elements from the queue using \"poll\" and add them to a \"List\".\nBecause \"poll\" returns the smallest element remaining in the queue, the elements are added to the \"List\" in ascending order. This way of sorting is called heap sort (see http://thinkdast.com/heapsort). Adding n elements to the queue takes n log n time. So does removing n elements. So the runtime for heap sort is O(n log n). In the repository for this book, in \"ListSorter.java\" you'll find the outline of a method called \"heapSort\". Fill it in and then run \"ant ListSorterTest\" to confirm that it works."], ["element log k adding x, k elements, order fill, track k, element heap", "A bounded heap is a heap that is limited to contain at most k elements. If you have n elements, you can keep track of the k largest elements like this: Initially, the heap is empty.  For each element, \"x\":\n- Branch 1: If the heap is not full, add \"x\" to the heap.\n- Branch 2: If the heap is full, compare \"x\" to the\nsmallest element in the heap. If \"x\" is smaller, it cannot be one of the largest k elements, so you can discard it.\n- Branch 3: If the heap is full and \"x\" is greater than the smallest element in the heap, remove the smallest element from the heap and add \"x\".\nUsing a heap with the smallest element at the top, we can keep track of the largest k elements. Let's analyze the performance of this algorithm. For each element, we perform one of:\n- Branch 1: Adding an element to the heap is O(log k).\n- Branch 2: Finding the smallest element in the heap is O(1).\n- Branch 3: Removing the smallest element is O(log k). Adding \"x\" is also O(log k).\nIn the worst case, if the elements appear in ascending order, we always run Branch 3. In that case, the total time to process n elements is O(n log k), which is linear in n. In \"ListSorter.java\" you'll find the outline of a method called \"topK\" that takes a \"List\", a \"Comparator\", and an integer k. It should return the k largest elements in the \"List\" in ascending order. Fill it in and then run \"ant ListSorterTest\" to confirm that it works."], ["pay attention, needs space proportional k number elements, space example, merge sort, elements time", "Until now we have talked a lot about runtime analysis, but for many algorithms we are also concerned about space. For example, one of the drawbacks of merge sort is that it makes copies of the data. In our implementation, the total amount of space it allocates is O(n log n). With a more clever implementation, you can get the space requirement down to O(n). In contrast, insertion sort doesn't copy the data because it sorts the elements in place. It uses temporary variables to compare two elements at a time, and it uses a few other local variables. But its space use doesn't depend on n. Our implementation of heap sort creates a new \"PriorityQueue\" to store the elements, so the space is O(n); but if you are allowed to sort the list in place, you can run heap sort with O(1) space. One of the benefits of the bounded heap algorithm you just implemented is that it only needs space proportional to k (the number of elements we want to keep), and k is often much smaller than n. Software developers tend to pay more attention to runtime than space, and for many applications, that's appropriate. But for large datasets, space can be just as important or more so. For example:\n- If a dataset doesn't fit into the memory of one program, the run time often increases dramatically, or it might not run at all. If you choose an algorithm that needs less space, and that makes it possible to fit the computation into memory, it might run much faster. In the same vein, a program that uses less space might make better use of CPU caches and run faster (see http://thinkdast.com/cache).\n- On a server that runs many programs at the same time, if you can reduce the space needed for each program, you might be able to run more programs on the same server, which reduces hardware and energy costs.\nSo those are some reasons you should know at least a little bit about the space needs of algorithms."]]}