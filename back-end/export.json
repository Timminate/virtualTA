{"conversations": [["integer : public final class integer extends number implements comparable, lang : public interface comparable, generic type , Java interface specifies, methods class implements, methods example source code Comparable interface, package public interface Comparable public int compareTo interface definition, type parameter, java interface specifies, number inherits methods instance variables number implements comparable integer, familiar read http, methods class implements, ternary operator, type parameter, package public interface comparable public int compareto interface definition, implementation compareto, object parameter returns int example source code public final class integer, comparable generic type order implement interface class specify type refers provide method, implements interface compiler checks", "A Java \"interface\" specifies a set of methods; any class that implements this \"interface\" has to provide these methods. For example, here is the source code for \"Comparable\", which is an \"interface\" defined in the package \"java.lang\":\npublic interface Comparable<T> {\n    public int compareTo(T o);\n}\nThis \"interface\" definition uses a type parameter, \"T\", which makes \"Comparable\" a **generic type**. In order to implement this \"interface\", a class has to\n- Specify the type \"T\" refers to, and\n- Provide a method named \"compareTo\" that takes an object as a parameter and returns an \"int\".\nFor example, here's the source code for \"java.lang.Integer\":\npublic final class Integer extends Number implements Comparable<Integer> {\n    public int compareTo(Integer anotherInteger) {\n        int thisVal = this.value;\n        int anotherVal = anotherInteger.value;\n        return (thisVal<anotherVal ? -1 : (thisVal==anotherVal ? 0 : 1));\n    }\n    // other methods omitted\n}\nThis class extends \"Number\", so it inherits the methods and instance variables of \"Number\"; and it implements \"Comparable<Integer>\", so it provides a method named \"compareTo\" that takes an \"Integer\" and returns an \"int\".\nWhen a class declares that it implements an \"interface\", the compiler checks that it provides all methods defined by the \"interface\"."], ["ternary operator , sometimes written ?: , , ternary, implementation compareTo, ternary operator, familiar read http, number implements comparable integer public int compareto integer anotherinteger int thisval int anotherval return thisval, familiar read http, ternary operator, type parameter, implementation compareto, comparable generic type order implement interface class specify type refers provide method, methods class implements, methods example source code comparable interface", "As an aside, this implementation of \"compareTo\" uses the \"ternary operator\", sometimes written \"?:\".  If you are not familiar with it, you can read about it at http://thinkdast.com/ternary."], ["library developers avoid changing interfaces unless absolutely necessary, see , public static void main, Java Collections Framework JCF, LinkedList interface defines, List class implements, particular set methods, ArrayList LinkedList, specific implementation, linkedlist interface defines, new listclientexample list list list listclientexample anything useful essential elements class, change library developers, arraylist way implementation changes", "The Java Collections Framework (JCF) defines an \"interface\" called \"List\" and provides two implementations, \"ArrayList\" and \"LinkedList\".\nThe \"interface\" defines what it means to be a \"List\"; any class that implements this \"interface\" has to provide a particular set of methods, including \"add\", \"get\", \"remove\", and about 20 more.\n\"ArrayList\" and \"LinkedList\" provide these methods, so they can be used interchangeably. A method written to work with a \"List\" will work with an \"ArrayList\", \"LinkedList\", or any other object that implements \"List\".\nHere's a contrived example that demonstrates the point:\npublic class ListClientExample {\n    private List list;\n    public ListClientExample() {\n        list = new LinkedList();\n    }\n    private List getList() {\n        return list;\n    }\n    public static void main(String[] args) {\n        ListClientExample lce = new ListClientExample();\n        List list = lce.getList();\n        System.out.println(list);\n    }\n}\n\"ListClientExample\" doesn't do anything useful, but it has the essential elements of a class that **encapsulates** a \"List\"; that is, it contains a \"List\" as an instance variable.  I'll use this class to make a point, and then you'll work with it in the first exercise.\nThe \"ListClientExample\" constructor initializes \"list\" by **instantiating** (that is, creating) a new \"LinkedList\"; the getter method called \"getList\" returns a reference to the internal \"List\" object; and \"main\" contains a few lines of code to test these methods.\nThe important thing about this example is that it uses \"List\" whenever possible and avoids specifying \"LinkedList\" or \"ArrayList\" unless it is necessary. For example, the instance variable is declared to be a \"List\", and \"getList\" returns a \"List\", but neither specifies which kind of list.\nIf you change your mind and decide to use an \"ArrayList\", you only have to change the constructor; you don't have to make any other changes.\nThis style is called **interface-based programming**, or more casually, \"programming to an interface\" (see http://thinkdast.com/interbaseprog). Here we are talking about the general idea of an interface, not a Java \"interface\".\nWhen you use a library, your code should only depend on the interface, like \"List\".  It should not depend on a specific implementation, like \"ArrayList\". That way, if the implementation changes in the future, the code that uses it will still work.\nOn the other hand, if the interface changes, the code that depends on it has to change, too.  That's why library developers avoid changing interfaces unless absolutely necessary."], ["selectsort public class selectionsort, public static void selectionsort, public static void swapelements, example implementation simple algorithm, selection sort, http public class SelectionSort Swaps elements indexes, public static void swapElements int array int int j int temp array array array j array j temp Finds index, end array public static int, example implementation simple algorithm, element array, selection sort, int j indexlowest array swapelements, constant time operations", "For example, here's an implementation of a simple algorithm called selection sort (see http://thinkdast.com/selectsort):\npublic class SelectionSort {\n    /** * Swaps the elements at indexes i and j. */\n    public static void swapElements(int[] array, int i, int j) {\n        int temp = array[i];\n        array[i] = array[j];\n        array[j] = temp;\n    }\n    /** * Finds the index of the lowest value * starting from the index at start (inclusive) * and going to the end of the array. */\n    public static int indexLowest(int[] array, int start) {\n        int lowIndex = start;\n        for (int i = start; i < array.length; i++) {\n            if (array[i] < array[lowIndex]) {\n                lowIndex = i;\n            }\n        }\n        return lowIndex;\n    }\n    /** * Sorts the elements (in place) using selection sort. */\n    public static void selectionSort(int[] array) {\n        for (int i = 0; i < array.length; i++) {\n            int j = indexLowest(array, i);\n            swapElements(array, i, j);\n        }\n    }\n}\nThe first method, \"swapElements\", swaps two elements of the array. Reading and writing elements are constant time operations, because if we know the size of the elements and the location of the first, we can compute the location of any other element with one multiplication and one addition, and those are constant time operations. Since everything in \"swapElements\" is constant time, the whole method is constant time.\nThe second method, \"indexLowest\", finds the index of the smallest element of the array starting at a given index, \"start\". Each time through the loop, it accesses two elements of the array and performs one comparison. Since these are all constant time operations, it doesn't really matter which ones we count. To keep it simple, let's count the number of comparisons.\n- If \"start\" is 0, \"indexLowest\" traverses the entire array, and the total number of comparisons is the length of the array, which I'll call n.\n- If \"start\" is 1, the number of comparisons is n-1.\n- In general, the number of comparisons is n - \"start\", so \"indexLowest\" is linear.\nThe third method, \"selectionSort\", sorts the array. It loops from 0 to n-1, so the loop executes n times. Each time, it calls \"indexLowest\" and then performs a constant time operation, \"swapElements\".\nThe first time \"indexLowest\" is called, it performs n comparisons. The second time, it performs n-1 comparisons, and so on. The total number of comparisons is n + n-1 + n-2 + ... + 1 + 0. The sum of this series is n(n+1)/2, which is proportional to n^2; and that means that \"selectionSort\" is quadratic."], ["nested loop, different way, total number, result different way, loop time call indexLowest number operations proportional call n times total number operations, example implementation simple algorithm, selection sort public static void selectionsort int array int, proportional means, loop time call indexlowest number operations proportional call n times total number operations, number comparisons general number comparisons, everything swapelements constant time whole method constant time second method, performs n comparisons second time performs comparisons total number comparisons, result different way", "To get to the same result a different way, we can think of \"indexLowest\" as a nested loop. Each time we call \"indexLowest\", the number of operations is proportional to n. We call it n times, so the total number of operations is proportional to n^2."], ["see , linear time algorithm followed, constant time algorithms belong, constant time algorithms belong set, another way, constant time, linear algorithms belong, quadratic algorithms belong way, another way, linear operation number times, linear algorithms belong, general care, quadratic f n", "All constant time algorithms belong to a set called O(1). So another way to say that an algorithm is constant time is to say that it is in O(1). Similarly, all linear algorithms belong to O(n), and all quadratic algorithms belong to O(n^2). This way of classifying algorithms is called \"big O notation\".\nNOTE: I am providing a casual definition of big O notation. For a more mathematical treatment, see http://thinkdast.com/bigo.\nThis notation provides a convenient way to write general rules about how algorithms behave when we compose them. For example, if you perform a linear time algorithm followed by a constant algorithm, the total run time is linear. Using ∈ to mean \"is a member of\":\nIf f ∈ O(n) and g ∈ O(1), f+g ∈ O(n).\nIf you perform two linear operations, the total is still linear:\nIf f ∈ O(n) and g ∈ O(n), f+g ∈ O(n).\nIn fact, if you perform a linear operation any number of times, k, the total is linear, as long as k is a constant that does not depend on n.\nIf f ∈ O(n) and k is a constant, kf ∈ O(n).\nBut if you perform a linear operation n times, the result is quadratic:\nIf f ∈ O(n), nf ∈ O(n^2).\nIn general, we only care about the largest exponent of n. So if the total number of operations is 2n + 1, it belongs to O(n). The leading constant, 2, and the additive term, 1, are not important for this kind of analysis. Similarly, n^2 + 100n + 1000 is in O(n^2).  Don't be distracted by the big numbers!"], ["round table },, particularly efficient algorithms, linear algorithms belong, Order growth another name idea order growth, runtimes big category example, belong order growth, n context order group, Order Knights Round Table group, imagine order linear algorithms, belong order growth, efficient algorithms, runtimes big category example, n context order group, order growth another name idea order growth, order knights round table group", "\"Order of growth\" is another name for the same idea.  An order of growth is a set of algorithms whose runtimes are in the same big O category; for example, all linear algorithms belong to the same order of growth because their runtimes are in O(n).\nIn this context, an \"order\" is a group, like the \\emph{Order of the Knights of the Round Table}, which is a group of knights, not a way of lining them up. So you can imagine the \\emph{Order of Linear Algorithms} as a set of brave, chivalrous, and particularly efficient algorithms."], ["myarraylist : public e get, indexof : public int indexof, 0 || index >= size, many methods identify order growth, code example implementation, MyArrayList public E, int index index, index size, int index index, constant time, loop constant time next question, array return return time loop indexof invokes equals, int index e element get index int array array size return element", "For many methods, we can identify the order of growth by examining the code. For example, here's the implementation of \"get\" from \"MyArrayList\":\n    public E get(int index) {\n        if (index < 0 || index >= size) {\n            throw new IndexOutOfBoundsException();\n        }\n        return array[index];\n    }\nEverything in \"get\" is constant time, so \"get\" is constant time. No problem.\nNow that we've classified \"get\", we can classify \"set\", which uses it. Here is our implementation of \"set\" from the previous exercise:\n    public E set(int index, E element) {\n        E old = get(index);\n        array[index] = element;\n        return old;\n    }\nOne slightly clever part of this solution is that it does not check the bounds of the array explicitly; it takes advantage of \"get\", which raises an exception if the index is invalid.\nEverything in \"set\", including the invocation of \"get\", is constant time, so \"set\" is also constant time.\nNext we'll look at some linear methods. For example, here's my implementation of \"indexOf\":\n    public int indexOf(Object target) {\n        for (int i = 0; i<size; i++) {\n            if (equals(target, array[i])) {\n                return i;\n            }\n        }\n        return -1;\n    }\nEach time through the loop, \"indexOf\" invokes \"equals\", so we have to classify \"equals\" first. Here it is:\n    private boolean equals(Object target, Object element) {\n        if (target == null) {\n            return element == null;\n        } else {\n            return target.equals(element);\n        }\n    }\nThis method invokes \"target.equals\"; the runtime of this method might depend on the size of \"target\" or \"element\", but it probably doesn't depend on the size of the array, so we consider it constant time for purposes of analyzing \"indexOf\".\nGetting back to \"indexOf\", everything inside the loop is constant time, so the next question we have to consider is: how many times does the loop execute?\nIf we get lucky, we might find the target object right away and return after testing only one element. If we are unlucky, we might have to test all of the elements. On average, we expect to test half of the elements, so this method is considered linear (except in the unlikely case that we know the target element is at the beginning of the array).\nThe analysis of \"remove\" is similar. Here's my implementation:\n    public E remove(int index) {\n        E element = get(index);\n        for (int i=index; i<size-1; i++) {\n            array[i] = array[i+1];\n        }\n        size--;\n        return element;\n    }\nIt uses \"get\", which is constant time, and then loops through the array, starting from \"index\". If we remove the element at the end of the list, the loop never runs and this method is constant time. If we remove the first element, we loop through all of the remaining elements, which is linear. So, again, this method is considered linear (except in the special case where we know the element is at the end or a constant distance from the end)."], ["next 7 adds store 7 elements, next 3 adds store 3 elements, called amortized analysis , index element parameters, int index E element index, index size, new IndexOutOfBoundsException add element, add element shift elements int index array array, elements next add copies, time proportional size array, element second time finds unused space array stores, exponent n, int e, big second term, copied otherwise add, element size array", "Here's a version of \"add\" that takes an index and an element as parameters:\n    public void add(int index, E element) {\n        if (index < 0 || index > size) {\n            throw new IndexOutOfBoundsException();\n        }\n        // add the element to get the resizing add(element);\n        // shift the other elements for (int i=size-1; i>index; i--) {\n            array[i] = array[i-1];\n        }\n        // put the new one in the right place array[index] = element;\n    }\nThis two-parameter version, called \"add(int, E)\", uses the one-parameter version, called \"add(E)\", which puts the new element at the end. Then it shifts the other elements to the right, and puts the new element in the correct place.\nBefore we can classify the two-parameter \"add(int, E)\", we have to classify the one-parameter \"add(E)\":\n    public boolean add(E element) {\n        if (size >= array.length) {\n            // make a bigger array and copy over the elements E[] bigger = (E[]) new Object[array.length * 2];\n            System.arraycopy(array, 0, bigger, 0, array.length);\n            array = bigger;\n        }\n        array[size] = element;\n        size++;\n        return true;\n    }\nThe one-parameter version turns out to be hard to analyze. If there is an unused space in the array, it is constant time, but if we have to resize the array, it's linear because \"System.arraycopy\" takes time proportional to the size of the array.\nSo is add constant time or linear? We can classify this method by thinking about the average number of operations per add over a series of n adds. For simplicity, assume we start with an array that has room for 2 elements.\n- The first time we call add, it finds unused space in the array, so it stores 1 element.\n- The second time, it finds unused space in the array, so it stores 1 element.\n- The third time, we have to resize the array, copy 2 elements, and store 1 element. Now the size of the array is 4.\n- The fourth time stores 1 element.\n- The fifth time resizes the array, copies 4 elements, and stores 1 element. Now the size of the array is 8.\n- The next 3 adds store 3 elements.\n- The next add copies 8 and stores 1. Now the size is 16.\n- The next 7 adds store 7 elements.\nAnd so on. Adding things up:\n- After 4 adds, we've stored 4 elements and copied 2.\n- After 8 adds, we've stored 8 elements and copied 6.\n- After 16 adds, we've stored 16 elements and copied 14.\nBy now you should see the pattern: to do n adds, we have to store n elements and copy n-2. So the total number of operations is n + n - 2, which is 2n-2.\nTo get the average number of operations per add, we divide the total by n; the result is 2 - 2/n. As n gets big, the second term, 2/n, gets small. Invoking the principle that we only care about the largest exponent of n, we can think of \"add\" as constant time.\nIt might seem strange that an algorithm that is sometimes linear can be constant time on average. The key is that we double the length of the array each time it gets resized. That limits the number of times each element gets copied. Otherwise --- if we add a fixed amount to the length of the array, rather than multiplying by a fixed amount --- the analysis doesn't work.\nThis way of classifying an algorithm, by computing the average time in a series of invocations, is called **amortized analysis**.  You can read more about it at http://thinkdast.com/amort. The key idea is that the extra cost of copying the array is spread, or \"amortized\", over a series of invocations.\nNow, if \"add(E)\" is constant time, what about \"add(int, E)\"? After calling \"add(E)\", it loops through part of the array and shifts elements. This loop is linear, except in the special case where we are adding at the end of the list. So \"add(int, E)\" is linear."], ["myarraylist : public boolean removeall, generally contains 1, always contains 100, last example, removeAll implementation MyArrayList, removeAll Collection collection boolean flag true Object obj collection flag, obj return flag time, removeAll invokes, talk problem size careful size sizes, contains n elements, proportional n, elements list, loops example number iterations, linear collection, removeall collection collection boolean flag true object obj collection flag, many times, nm size collection, case method loop, obj return flag time, removeall implementation myarraylist, last example, constant removeall linear respect size collection proportional n removeall quadratic example collection", "The last example we'll consider is \"removeAll\"; here's the implementation in \"MyArrayList\":\n    public boolean removeAll(Collection<?> collection) {\n        boolean flag = true;\n        for (Object obj: collection) {\n            flag &= remove(obj);\n        }\n        return flag;\n    }\nEach time through the loop, \"removeAll\" invokes \"remove\", which is linear.  So it is tempting to think that \"removeAll\" is quadratic.  But that's not necessarily the case.\nIn this method, the loop runs once for each element in \"collection\". If \"collection\" contains m elements and the list we are removing from contains n elements, this method is in O(nm). If the size of \"collection\" can be considered constant, \"removeAll\" is linear with respect to n. But if the size of the collection is proportional to n, \"removeAll\" is quadratic. For example, if \"collection\" always contains 100 or fewer elements, \"removeAll\" is linear. But if \"collection\" generally contains 1\\\nquadratic."], ["problem size,, one nested inside, one loop, talk problem size careful size sizes, pitfall algorithm analysis, loop algorithm, many times, number iterations proportional n loops, pitfall algorithm analysis, loop algorithm, number iterations proportional n loops, loops example number iterations, many times, proportional n, talk problem size careful size sizes, constant removeall linear respect size collection proportional n removeall quadratic example collection", "When we talk about **problem size**, we have to be careful about which size, or sizes, we are talking about. This example demonstrates a pitfall of algorithm analysis: the tempting shortcut of counting loops.  If there is one loop, the algorithm is often linear.  If there are two loops (one nested inside the other), the algorithm is often quadratic. But be careful! You have to think about how many times each loop runs. If the number of iterations is proportional to n for all loops, you can get away with just counting the loops. But if, as in this example, the number of iterations is not always proportional to n, you have to give it more thought."], ["linked structures include trees, public string tostring (), 2 ) listnode node3, next exercise, partial implementation List interface, list store elements, http section, brief introduction data structure, listnode next data, new listnode, reference next node list, http section, several constructors", "For the next exercise I provide a partial implementation of the \"List\" interface that uses a linked list to store the elements. If you are not familiar with linked lists, you can read about them at http://thinkdast.com/linkedlist, but this section provides a brief introduction.\nA data structure is \"linked\" if it is made up of objects, often called \"nodes\", that contain references to other nodes. In a linked list, each node contains a reference to the next node in the list. Other linked structures include trees and graphs, in which nodes can contain references to more than one other node.\nHere's a class definition for a simple node:\npublic class ListNode {\n    public Object data;\n    public ListNode next;\n    public ListNode() {\n        this.data = null;\n        this.next = null;\n    }\n    public ListNode(Object data) {\n        this.data = data;\n        this.next = null;\n    }\n    public ListNode(Object data, ListNode next) {\n        this.data = data;\n        this.next = next;\n    }\n    public String toString() {\n        return \"ListNode(\" + data.toString() + \")\";\n    }\n}\nThe \"ListNode\" object has two instance variables: \"data\" is a reference to some kind of \"Object\", and \"next\" is a reference to the next node in the list. In the last node in the list, by convention, \"next\" is \"null\".\n\"ListNode\" provides several constructors, allowing you to provide values for \"data\" and \"next\", or initialize them to the default value, \"null\".\nYou can think of each \"ListNode\" as a list with a single element, but more generally, a list can contain any number of nodes. There are several ways to make a new list. A simple option is to create a set of \"ListNode\" objects, like this:\n        ListNode node1 = new ListNode(1);\n        ListNode node2 = new ListNode(2);\n        ListNode node3 = new ListNode(3);\nAnd then link them up, like this:\n        node1.next = node2;\n        node2.next = node3;\n        node3.next = null;\nAlternatively, you can create a node and link it at the same time. For example, if you want to add a new node at the beginning of a list, you can do it like this:\n        ListNode node0 = new ListNode(0, node1);\nAfter this sequence of instructions, we have four nodes containing the \"Integer\"s 0, 1, 2, and 3 as data, linked up in increasing order. In the last node, the \"next\" field is \"null\"."], ["names inside boxes, variables appear, objects appear, object diagram variables, refer Objects, ListNode Integer instance variables, listnode integer instance variables, new listnode, refer objects, nodes contain references nodes, object diagram variables, listnode objects", "  In an object diagram, variables appear as names inside boxes, with arrows that show what they refer to.  Objects appear as boxes with their type on the outside (like \"ListNode\" and \"Integer\") and their instance variables on the inside."], ["array never gets garbage collected, get garbage collected immediately, get garbage collected, MyArrayList previous exercise array, list implementation shrinks elements, unused nodes, myarraylist previous exercise array, java lot work, garbage collection, list implementation shrinks elements, unused nodes, performance bug program", "In \"MyArrayList\" from the previous exercise, the array grows if necessary, but it never shrinks. The array never gets garbage collected, and the elements don't get garbage collected until the list itself is destroyed.\nOne advantage of the linked list implementation is that it shrinks when elements are removed, and the unused nodes can get garbage collected immediately."], ["contains two constant time operations, public void clear (), sure looks like, implementation clear method public void clear head null size, head null remove reference, Node references Node, point reference second Node, classify clear method, myarraylist previous exercise array, head null remove reference, constant time invoke, garbage collection, implementation clear method public void clear head null size, point reference second node, list implementation shrinks elements, unused nodes, performance bug program, garbage collector work proportional number elements", "Here is my implementation of the \"clear\" method:\n    public void clear() {\n        head = null;\n        size = 0;\n    }\nWhen we set \"head\" to \"null\", we remove a reference to the first \"Node\". If there are no other references to that \"Node\" (and there shouldn't be), it will get garbage collected. At that point, the reference to the second \"Node\" is removed, so it gets garbage collected, too. This process continues until all nodes are collected.\nSo how should we classify \"clear\"? The method itself contains two constant time operations, so it sure looks like it's constant time. But when you invoke it, you make the garbage collector do work that's proportional to the number of elements. So maybe we should consider it linear!"], ["performance bug:, like garbage collection, languages like java, performance bug program, sense right thing belong order growth, Java lot work, garbage collection, scenes kind bug hard find, myarraylist previous exercise array, java lot work, sense right thing belong order growth, garbage collection, scenes kind bug hard find, list implementation shrinks elements, unused nodes, performance bug program", "This is an example of what is sometimes called a **performance bug**:\na program that is correct in the sense that it does the right thing, but it doesn't belong to the order of growth we expected. In languages like Java that do a lot of work, like garbage collection, behind the scenes, this kind of bug can be hard to find."], ["return - 1 , loop without finding, public int indexof, implementation indexOf Read, identify order growth read explanation public int indexOf Object target Node node head int size equals, return node return Initially, copy head refer Node loop, time loop use equals, otherwise advance next node list normally, return node return initially, size consistent actual number nodes, sure next node null case safe loop, time loop use equals, found target, identify order growth read explanation public int indexof object target node node head int size equals, copy head refer node loop, target return order growth method, implementation indexof read, get end list", "My implementation of \"indexOf\" is below. Read through it and see if you can identify its order of growth before you read the explanation.\n    public int indexOf(Object target) {\n        Node node = head;\n        for (int i=0; i<size; i++) {\n            if (equals(target, node.data)) {\n                return i;\n            }\n            node = node.next;\n        }\n        return -1;\n    }\nInitially \"node\" gets a copy of \"head\", so they both refer to the same \"Node\". The loop variable, \"i\", counts from 0 to \"size-1\".  Each time through the loop, we use \"equals\" to see if we've found the target. If so, we return \"i\" immediately. Otherwise we advance to the next \"Node\" in the list.\nNormally we would check to make sure the next \"Node\" is not \"null\", but in this case it is safe because the loop ends when we get to the end of the list (assuming \"size\" is consistent with the actual number of nodes in the list).\nIf we get through the loop without finding the target, we return \"-1\".\nSo what is the order of growth for this method?"], ["loop might run n times, people see two linear operations, 0 || index >= size, invoke equals constant time, size target data, size list operations, constant time loop, whole list runtime method proportional length list Next implementation add method try, getnode similar indexof linear reason, list returns, element use helper method getnode private node getnode int index index, new node, add n sum", "- Each time through the loop we invoke \"equals\", which is constant time (it might depend on the size of \"target\" or \"data\", but it doesn't depend on the size of the list). The other operations in the loop are also constant time.\n- The loop might run n times, because in the worse case, we might have to traverse the whole list.\nSo the runtime of this method is proportional to the length of the list.\nNext, here is my implementation of the two-parameter \"add\" method. Again, you should try to classify it before you read the explanation.\n    public void add(int index, E element) {\n        if (index == 0) {\n            head = new Node(element, head);\n        } else {\n            Node node = getNode(index-1);\n            node.next = new Node(element, node.next);\n        }\n        size++;\n    }\nIf \"index==0\", we're adding the new \"Node\" at the beginning, so we handle that as a special case. Otherwise, we have to traverse the list to find the element at \"index-1\". We use the helper method \"getNode\":\n    private Node getNode(int index) {\n        if (index < 0 || index >= size) {\n            throw new IndexOutOfBoundsException();\n        }\n        Node node = head;\n        for (int i=0; i<index; i++) {\n            node = node.next;\n        }\n        return node;\n    }\n\"getNode\" checks whether \"index\" is out of bounds; if so, it throws an exception. Otherwise it traverses the list and returns the requested Node.\nJumping back to \"add\", once we find the right \"Node\", we create the new \"Node\" and put it between \"node\" and \"node.next\". You might find it helpful to draw a diagram of this operation to make sure you understand it.\nSo, what's the order of growth for \"add\"?\n- \"getNode\" is similar to \"indexOf\", and it is linear for the same reason.\n- In \"add\", everything before and after \"getNode\" is constant time.\nSo all together, \"add\" is linear.\nFinally, let's look at \"remove\":\n    public E remove(int index) {\n        E element = get(index);\n        if (index == 0) {\n            head = head.next;\n        } else {\n            Node node = getNode(index-1);\n            node.next = node.next.next;\n        }\n        size--;\n        return element;\n    }\n\"remove\" uses \"get\" to find and store the element at \"index\". Then it removes the \"Node\" that contained it.\nIf \"index==0\", we handle that as a special case again. Otherwise we find the node at \"index-1\" and modify it to skip over \"node.next\" and link directly to \"node.next.next\". This effectively removes \"node.next\" from the list, and it can be garbage collected.\nFinally, we decrement \"size\" and return the element we retrieved at the beginning.\nSo, what's the order of growth for \"remove\"? Everything in \"remove\" is constant time except \"get\" and \"getNode\", which are linear. So \"remove\" is linear.\nWhen people see two linear operations, they sometimes think the result is quadratic, but that only applies if one operation is nested inside the other. If you invoke one operation after the other, the runtimes add. If they are both in O(n), the sum is also in O(n)."], ["following table summarizes, constant time, myarraylist ,, table summarizes differences MyLinkedList MyArrayList, constant time, n linear operations MyArrayList advantage, operations MyLinkedList advantage, constant time, n linear operations myarraylist advantage, operations likely use java, implementations order growth implementation, table summarizes differences mylinkedlist myarraylist, operations mylinkedlist advantage", "The following table summarizes the differences between \"MyLinkedList\" and \"MyArrayList\", where \"1\" means O(1) or constant time and n means O(n) or linear.\nThe operations where \"MyArrayList\" has an advantage are adding at the end, removing from the end, getting and setting.\nThe operations where \"MyLinkedList\" has an advantage are adding at the beginning and removing from the beginning."], ["two implementations, java provides, one implementation, implementations order growth implementation, operations likely use Java, constant time, n linear operations myarraylist advantage, operations likely use java, implementations order growth implementation, operations mylinkedlist advantage", "For the other operations, the two implementations are in the same order of growth.\nWhich implementation is better? It depends on which operations you are likely to use the most. And that's why Java provides more than one implementation, because it depends."], ["public static void profilearraylistaddend (), arraylist add end profiler profiler, anonymous class, next exercise provide class, code runs, range problem, measures runtimes plots results, Profiler classify performance, timeable anonymous class, comfortable anonymous classes, exceeds threshold timingloop stops, next exercise, start clock case", "For the next exercise I provide a class called \"Profiler\" that contains code that runs a method with a range of problem sizes, measures runtimes, and plots the results.\nYou will use \"Profiler\" to classify the performance of the \"add\" method for the Java implementations of \"ArrayList\" and \"LinkedList\".\nHere's an example that shows how to use the profiler:\n    public static void profileArrayListAddEnd() {\n        Timeable timeable = new Timeable() {\n            List<String> list;\n            public void setup(int n) {\n                list = new ArrayList<String>();\n            }\n            public void timeMe(int n) {\n                for (int i=0; i<n; i++) {\n                    list.add(\"a string\");\n                }\n            }\n        };\n        String title = \"ArrayList add end\";\n        Profiler profiler = new Profiler(title, timeable);\n        int startN = 4000;\n        int endMillis = 1000;\n        XYSeries series = profiler.timingLoop(startN, endMillis);\n        profiler.plotResults(series);\n    }\nThis method measures the time it takes to run \"add\" on an \"ArrayList\", which adds the new element at the end. I'll explain the code and then show the results.\nIn order to use \"Profiler\", we need to create a \"Timeable\" object that provides two methods: \"setup\" and \"timeMe\". The \"setup\" method does whatever needs to be done before we start the clock; in this case it creates an empty list. Then \"timeMe\" does whatever operation we are trying to measure; in this case it adds n elements to the list.\nThe code that creates \"timeable\" is an **anonymous class** that defines a new implementation of the \"Timeable\" interface and creates an instance of the new class at the same time. If you are not familiar with anonymous classes, you can read about them here:\nhttp://thinkdast.com/anonclass.\nBut you don't need to know much for the next exercise; even if you are not comfortable with anonymous classes, you can copy and modify the example code.\nThe next step is to create the \"Profiler\" object, passing the \"Timeable\" object and a title as parameters.\nThe \"Profiler\" provides \"timingLoop\" which uses the \"Timeable\" object stored as an instance variable. It invokes the \"timeMe\" method on the \"Timeable\" object several times with a range of values of n. \"timingLoop\" takes two parameters:\n- \"startN\" is the value of n the timing loop should start at.\n- \"endMillis\" is a threshold in milliseconds. As \"timingLoop\" increases the problem size, the runtime increases;\n  when the runtime exceeds this threshold, \"timingLoop\" stops.\nWhen you run the experiments, you might have to adjust these parameters. If \"startN\" is too low, the runtime might be too short to measure accurately. If \"endMillis\" is too low, you might not get enough data to see a clear relationship between problem size and runtime.\nThis code is in \"ProfileListAdd.java\", which you'll run in the next exercise. When I ran it, I got this output:\n4000, 3 8000, 0 16000, 1 32000, 2 64000, 3 128000, 6 256000, 18 512000, 30 1024000, 88 2048000, 185 4096000, 242 8192000, 544 16384000, 1325\nThe first column is problem size, n; the second column is runtime in milliseconds. The first few measurements are pretty noisy; it might have been better to set \"startN\" around 64000."], ["next section explains, plotresults ,, plot like, result timingLoop XYSeries, data pass series plotResults, next section, data pass series plotresults, xyseries series startn endmillis series method measures time, timeable anonymous class, result timingloop xyseries, next section, first column problem size n second column runtime milliseconds first measurements, next exercise, new implementation timeable interface, next exercise provide class", "The result from \"timingLoop\" is an \"XYSeries\" that contains this data. If you pass this series to \"plotResults\", it generates a plot like the one in\nThe next section explains how to interpret it."], ["take constant time, add n elements, total time, understanding ArrayList works, add method, constant time add elements, total time, n elements, mean algorithm, run time proportional exponent k, total time, straight line slope, constant time add elements, straight line, runtime problem size scale let, understanding arraylist works, n elements, mathematically write function line runtime b n intercept line, exponent k, add method, plot runtime versus problem size, runtime b n c perfect data", "Based on our understanding of how \"ArrayList\" works, we expect the \"add\" method to take constant time when we add elements to the end. So the total time to add n elements should be linear."], ["could plot total runtime versus problem size, plot runtime versus problem size, plot runtime versus n, test theory, total runtime versus problem size, straight line, Mathematically write function line runtime b n intercept line, slope hand, straight line slope, straight line, runtime problem size scale let, exponent k, plot runtime versus problem size", "To test that theory, we could plot total runtime versus problem size, and we should see a straight line, at least for problem sizes that are big enough to measure accurately. Mathematically, we can write the function for that line: runtime = a + b n, where a is the intercept of the line and b is the slope.\nOn the other hand, if \"add\" is linear, the total time for n adds would be quadratic. If we plot runtime versus problem size, we expect to see a parabola. Or mathematically, something like: runtime = a + b n + c n^2. With perfect data, we might be able to tell the difference between a straight line and a parabola, but if the measurements are noisy, it can be hard to tell. A better way to interpret noisy measurements is to plot runtime and problem size on a **log-log** scale.\nWhy? Let's suppose that runtime is proportional to n^k, but we don't know what the exponent k is. We can write the relationship like this: runtime = a + b n + ... + c n^k. For large values of n, the term with the largest exponent is the most important, so: runtime ≈ c n^k, where ≈ means \"approximately equal\". Now, if we take the logarithm of both sides of this equation: log(runtime) ≈ log(c) + k log(n). This equation implies that if we plot runtime versus n on a log-log scale, we expect to see a straight line with intercept log(c) and slope k. We don't care much about the intercept, but the slope indicates the order of growth: if k=1, the algorithm is linear; if k=2, it's quadratic.\nLooking at the figure in the previous section, you can estimate the slope by eye. But when you call \"plotResults\" it computes a least squares fit to the data and prints the estimated slope. In this example:\nEstimated slope = 1.06194352346708\nwhich is close to 1; and that suggests that the total time for n adds is linear, so each add is constant time, as expected.\nOne important point: if you see a straight line on a graph like this, that does **not** mean that the algorithm is linear. If the run time is proportional to n^k for any exponent k, we expect to see a straight line with slope k. If the slope is close to 1, that suggests the algorithm is linear. If it is close to 2, it's probably quadratic."], ["plotted runtime versus problem size, performing n adds, perform n adds, previous exercise, various ArrayList LinkedList operations, problem sizes, runtime versus problem size scale, exponent relationship runtime problem size example, total time perform, algorithm analysis exercise, previous exercise, performing n, n average time", "In the previous exercise, we used \"Profiler.java\" to run various \"ArrayList\" and \"LinkedList\" operations with a range of problem sizes. We plotted runtime versus problem size on a log-log scale and estimated the slope of the resulting curve, which indicates the leading exponent of the relationship between runtime and problem size.\nFor example, when we used the \"add\" method to add elements to the end of an \"ArrayList\", we found that the total time to perform n adds was proportional to n; that is, the estimated slope was close to 1. We concluded that performing n adds is in O(n), so on average the time for a single add is constant time, or O(1), which is what we expect based on algorithm analysis.\nThe exercise asks you to fill in the body of \"profileArrayListAddBeginning\", which tests the performance of adding new elements at the beginning of an \"ArrayList\". Based on our analysis, we expect each add to be linear, because it has to shift the other elements to the right; so we expect n adds to be quadratic."], ["solution directory, solution, repository, solution directory repository, total time, solution directory repository, results problem size, rather runtime proportional exponent k, identical profilearraylistaddend difference timeme, runtime milliseconds, public static void", "Here's a solution, which you can find in the solution directory of the repository."], ["runprofiler ( arraylist add beginning , timeable, get one additional data point, public static void profilearraylistaddbeginning (), public static void, Timeable timeable new Timeable List String list public void setup int n list new ArrayList String public void timeMe int n int, int startN, runProfiler ArrayList, timeable startN endMillis method, solution directory repository, timeable startn endmillis method, version add, new element index, additional data point, timeable timeable new timeable list string list public void setup int n list new arraylist string public void timeme int n int, results problem size, runtime milliseconds, rather runtime proportional exponent k, int startn, identical profilearraylistaddend difference timeme, runprofiler arraylist, public static void", "    public static void profileArrayListAddBeginning() {\n        Timeable timeable = new Timeable() {\n            List<String> list;\n            public void setup(int n) {\n                list = new ArrayList<String>();\n            }\n            public void timeMe(int n) {\n                for (int i=0; i<n; i++) {\n                    list.add(0, \"a string\");\n                }\n            }\n        };\n        int startN = 4000;\n        int endMillis = 10000;\n        runProfiler(\"ArrayList add beginning\", timeable, startN, endMillis);\n    }\nThis method is almost identical to \"profileArrayListAddEnd\". The only difference is in \"timeMe\", which uses the two-parameter version of \"add\" to put the new element at index 0. Also, we increased \"endMillis\" to get one additional data point."], ["right 4000, total time, timing results, results problem size, runtime milliseconds, Remember straight line graph mean, Rather runtime proportional exponent k, straight line slope case, total time, solution directory repository, public static void, results problem size, runtime milliseconds, rather runtime proportional exponent k, remember straight line graph mean, identical profilearraylistaddend difference timeme, straight line slope case, proportional expect straight line slope", "Here are the timing results (problem size on the left, runtime in milliseconds on the right):\n4000, 14 8000, 35 16000, 150 32000, 604 64000, 2518 128000, 11555\nRemember that a straight line on this graph does **not** mean that the algorithm is linear. Rather, if the runtime is proportional to n^k for any exponent, k, we expect to see a straight line with slope k. In this case, we expect the total time for n adds to be proportional to n^2, so we expect a straight line with slope 2. In fact, the estimated slope is 1.992, which is so close I would be afraid to fake data this good."], ["runprofiler ( linkedlist add beginning , timeable, public static void profilelinkedlistaddbeginning (), public void timeme, previous exercise, new elements, constant time, list shift, expect total time, startn endmillis, total time, constant time, new elements, squares fit results", "In the previous exercise you also profiled the performance of adding new elements at the beginning of a \"LinkedList\". Based on our analysis, we expect each \"add\" to take constant time, because in a linked list, we don't have to shift the existing elements; we can just add a new node at the beginning. So we expect the total time for n adds to be linear.\nHere's a solution:\n    public static void profileLinkedListAddBeginning() {\n        Timeable timeable = new Timeable() {\n            List<String> list;\n            public void setup(int n) {\n                list = new LinkedList<String>();\n            }\n            public void timeMe(int n) {\n                for (int i=0; i<n; i++) {\n                    list.add(0, \"a string\");\n                }\n            }\n        };\n        int startN = 128000;\n        int endMillis = 2000;\n        runProfiler(\"LinkedList add beginning\", timeable, startN, endMillis);\n    }\nWe only had a make a few changes, replacing \"ArrayList\" with \"LinkedList\" and adjusting \"startN\" and \"endMillis\" to get a good range of data. The measurements were noisier than the previous batch; here are the results:\n128000, 16 256000, 19 512000, 28 1024000, 77 2048000, 330 4096000, 892 8192000, 1047 16384000, 4755\nIt's not a very straight line, and the slope is not exactly 1; the slope of the least squares fit is 1.23. But these results indicate that the total time for n adds is at least approximately O(n), so each add is constant time."], ["runprofiler ( linkedlist add end , timeable, public static void profilelinkedlistaddend (), public void timeme, implementation traverse entire list add element end linear, total time, quadratic Well code public static void profileLinkedListAddEnd Timeable timeable new Timeable List String list public void setup int n list new LinkedList String public void timeMe int n int n, int startN, int endMillis, total time, constant time, int endmillis, quadratic well code public static void profilelinkedlistaddend timeable timeable new timeable list string list public void setup int n list new linkedlist string public void timeme int n int n, measurements noisy line, implementation traverse entire list add element end linear, int startn, runprofiler linkedlist add end timeable startn endmillis results, analysis fact", "Adding elements at the beginning is one of the operations where we expect \"LinkedList\" to be faster than \"ArrayList\". But for adding elements at the end, we expect \"LinkedList\" to be slower. In my implementation, we have to traverse the entire list to add an element to the end, which is linear. So we expect the total time for n adds to be quadratic.\nWell, it's not.  Here's the code:\n    public static void profileLinkedListAddEnd() {\n        Timeable timeable = new Timeable() {\n            List<String> list;\n            public void setup(int n) {\n                list = new LinkedList<String>();\n            }\n            public void timeMe(int n) {\n                for (int i=0; i<n; i++) {\n                    list.add(\"a string\");\n                }\n            }\n        };\n        int startN = 64000;\n        int endMillis = 1000;\n        runProfiler(\"LinkedList add end\", timeable, startN, endMillis);\n    }\nHere are the results:\n64000, 9 128000, 9 256000, 21 512000, 24 1024000, 78 2048000, 235 4096000, 851 8192000, 950 16384000, 6160\nAgain, the measurements are noisy and the line is not perfectly straight, but the estimated slope is 1.19, which is close to what we got adding elements at the beginning, and not very close to 2, which is what we expected based on our analysis. In fact, it is closer to 1, which suggests that adding elements at the end is constant time. What's going on?"], [" , , following table summarizes, list MyLinkedList, list element contains, next MyArrayList object link first node, documentation LinkedList http, list implementation List Deque interfaces operations perform, object contains links, list operations index list traverse list, list implementation list deque interfaces operations perform, table summarizes performance, list element contains", "My implementation of a linked list, \"MyLinkedList\", uses a singly-linked list; that is, each element contains a link to the next, and the \"MyArrayList\" object itself has a link to the first node.\nBut if you read the documentation of \"LinkedList\" at http://thinkdast.com/linked, it says:\n\"Doubly-linked list implementation of the List and Deque interfaces. [...] All of the operations perform as could be expected for a doubly-linked list. Operations that index into the list will traverse the list from the beginning or the end, whichever is closer to the specified index.\"\nIf you are not familiar with doubly-linked lists, you can read more about them at http://thinkdast.com/doublelist, but the short version is:\n- Each node contains a link to the next node and a link to the previous node.\n- The \"LinkedList\" object contains links to the first and last elements of the list.\nSo we can start at either end of the list and traverse it in either direction. As a result, we can add and remove elements from the beginning and the end of the list in constant time!\nThe following table summarizes the performance we expect from \"ArrayList\", \"MyLinkedList\" (singly-linked), and \"LinkedList\" (doubly-linked):"], ["require linear time, removing elements near, application depends, good ArrayList, end advantage ArrayList get, linear time, runtime application, choice runtime, good arraylist, choice runtime, end linkedlist, runtime application, end advantage arraylist get, linear time", "The doubly-linked implementation is better than \"ArrayList\" for adding and removing at the beginning, and just as good as \"ArrayList\" for adding and removing at the end. So the only advantage of \"ArrayList\" is for \"get\" and \"set\", which require linear time in a linked list, even if it is doubly-linked.\nIf you know that the runtime of your application depends on the time it takes to \"get\" and \"set\" elements, an \"ArrayList\" might be the better choice. If the runtime depends on adding and removing elements near the beginning or the end, \"LinkedList\" might be better."], ["software engineer without ever finding, different implementations require different amounts, nodes scattered around, order growth large problems factors, substantial fraction runtime application applications, time things choice List implementation, much lists, small problems quadratic algorithm, space computer hardware, list element, constant time small problems difference, memory hardware, runtime different implementations", "But remember that these recommendations are based on the order of growth for large problems. There are other factors to consider:\n- If these operations don't take up a substantial fraction of the runtime for your application --- that is, if your applications spends most of its time doing other things --- then your choice of a \"List\" implementation won't matter very much.\n- If the lists you are working with are not very big, you might not get the performance you expect. For small problems, a quadratic algorithm might be faster than a linear algorithm, or linear might be faster than constant time. And for small problems, the difference probably doesn't matter.\n- Also, don't forget about space. So far we have focused on runtime, but different implementations require different amounts of space. In an \"ArrayList\", the elements are stored side-by-side in a single chunk of memory, so there is little wasted space, and computer hardware is often faster with contiguous chunks. In a linked list, each element requires a node with one or two links. The links take up space (sometimes more than the data!), and with nodes scattered around in memory, the hardware might be less efficient.\nIn summary, analysis of algorithms provides some guidance for choosing data structures, but only if\n- The runtime of your application is important,\n- The runtime of your application depends on your choice of data structure, and\n- The problem size is large enough that the order of growth actually predicts which data structure is better.\nYou could have a long career as a software engineer without ever finding yourself in this situation."], ["web search engine, like google search, search engines like google, search engine, web search engine, Google Search Bing, search terms returns list web pages, relevant means, essential components, need data structure, way collect results index, crawler goal crawler discover download, web pages, search terms returns list web pages", "A **web search engine**, like Google Search or Bing, takes a set of \"search terms\" and returns a list of web pages that are relevant to those terms (I'll discuss what \"relevant\" means later).  You can read more at http://thinkdast.com/searcheng, but I'll explain what you need as we go along.\nThe essential components of a search engine are:\n- Crawling: We'll need a program that can download a web page, parse it, and extract the text and any links to other pages.\n- Indexing: We'll need a data structure that makes it possible to look up a search term and find the pages that contain it.\n- Retrieval: And we'll need a way to collect results from the Index and identify pages that are most relevant to the search terms.\nWe'll start with the crawler.  The goal of a crawler is to discover and download a set of web pages. For search engines like Google and Bing, the goal is to find all web pages, but often crawlers are limited to a smaller domain. In our case, we will only read pages from Wikipedia."], ["usually eventually gets one, , philosophy article , first step, crawler reads Wikipedia page, first link, another page, crawler test, process subsequent articles, first link, relevant means, philosophy conjecture states, philosophy article conjecture, first lowercase link main text wikipedia article, exercise kind fun, web pages, first step, basic pieces crawler, http read history testing conjecture, crawler test, crawler reads wikipedia page", "As a first step, we'll build a crawler that reads a Wikipedia page, finds the first link, follows the link to another page, and repeats. We will use this crawler to test the \"Getting to Philosophy\" conjecture, which states:\n\"Clicking on the first lowercase link in the main text of a Wikipedia article, and then repeating the process for subsequent articles, usually eventually gets one to the Philosophy article.\"\nThis conjecture is stated at http://thinkdast.com/getphil{}, and you can read its history there.\nTesting the conjecture will allow us to build the basic pieces of a crawler without having to crawl the entire web, or even all of Wikipedia. And I think the exercise is kind of fun!"], ["work, retriever, indexer, process subsequent articles, first link, relevant means, domain case, pages wikipedia, philosophy article conjecture, first lowercase link main text wikipedia article, program download web page parse extract text links pages, exercise kind fun, web pages, basic pieces crawler", "In a few chapters, we'll work on the indexer, and then we'll get to the retriever."], ["web browsers provide tools, linked data structure made, hello world </ p, download web page contents, HyperText Markup Language, HTML example minimal HTML document DOCTYPE html html head title title body p Hello world, title Hello world text, page elements, hypertext markup language, text tags, first paragraph main text article, web inspector, div element use element id identify main text article download", "When you download a web page, the contents are written in HyperText Markup Language, aka HTML. For example, here is a minimal HTML document:\n<!DOCTYPE html>\n<html>\n  <head>\n    <title>This is a title</title>\n  </head>\n  <body>\n    <p>Hello world!</p>\n  </body>\n</html>\nThe phrases \"This is a title\" and \"Hello world!\" are the text that actually appears on the page; the other elements are **tags** that indicate how the text should be displayed.\nWhen our crawler downloads a page, it will need to parse the HTML in order to extract the text and find the links. To do that, we'll use **jsoup**, which is an open-source Java library that downloads and parses HTML.\nThe result of parsing HTML is a Document Object Model tree, or **DOM tree**, that contains the elements of the document, including text and tags. The tree is a linked data structure made up of nodes; the nodes represent text, tags, and other document elements.\nThe relationships between the nodes are determined by the structure of the document. In the example above, the first node, called the **root**, is the \"<html>\" tag, which contains links to the two nodes it contains, \"<head>\" and \"<body>\"; these nodes are the **children** of the root node.\nThe \"<head>\" node has one child, \"<title>\", and the \"<body>\" node has one child, \"<p>\" (which stands for \"paragraph\").\nEach node contains links to its children; in addition, each node contains a link to its **parent**, so from any node it is possible to navigate up and down the tree. The DOM tree for real pages is usually more complicated than this example.\nMost web browsers provide tools for inspecting the DOM of the page you are viewing. In Chrome, you can right-click on any part of a web page and select \"Inspect\" from the menu that pops up. In Firefox, you can right-click and select \"Inspect Element\" from the menu. Safari provides a tool called Web Inspector, which you can read about at http://thinkdast.com/safari. For Internet Explorer, you can read the instructions at http://thinkdast.com/explorer.\ncom/java. The element that's highlighted is the first paragraph of the main text of the article, which is contained in a \"<div>\" element with \"id=\"mw-content-text\"\". We'll use this element id to identify the main text of each article we download."], ["node < div id = mw, programming_language ) // download, select ( p ), easy download parse web pages, DOM tree example String url http download parse document Connection conn url Document doc select content text pull paragraphs Element content Elements paragraphs p, URL String, connection web server, method downloads HTML, connection web server, returns document object, skim documentation classes, dom tree example string url http download parse document connection conn url document doc select content text pull paragraphs element content elements paragraphs p, id field selects node div", "jsoup makes it easy to download and parse web pages, and to navigate the DOM tree. Here's an example:\n    String url = \"http://en.wikipedia.org/wiki/Java_(programming_language)\";\n    // download and parse the document Connection conn = Jsoup.connect(url);\n    Document doc = conn.get();\n    // select the content text and pull out the paragraphs. Element content = doc.getElementById(\"mw-content-text\");\n    Elements paragraphs = content.select(\"p\");\n\"Jsoup.connect\" takes a URL as a \"String\" and makes a connection to the web server; the \"get\" method downloads the HTML, parses it, and returns a \"Document\" object, which represents the DOM.\n\"Document\" provides methods for navigating the tree and selecting nodes. In fact, it provides so many methods, it can be confusing. This example demonstrates two ways to select nodes:\n- \"getElementById\" takes a \"String\" and searches the tree for an element that has a matching \"id\" field. Here it selects the node \"<div id=\"mw-content-text\" lang=\"en\" dir=\"ltr\" class=\"mw-content-ltr\">\", which appears on every Wikipedia page to identify the \"<div>\" element that contains the main text of the page, as opposed to the navigation sidebar and other elements.\n  The return value from \"getElementById\" is an \"Element\" object that represents this \"<div>\" and contains the elements in the \"<div>\" as children, grandchildren, etc.\n- \"select\" takes a \"String\", traverses the tree, and returns all the elements with tags that match the \"String\". In this example, it returns all paragraph tags that appear in \"content\". The return value is an \"Elements\" object.\nBefore you go on, you should skim the documentation of these classes so you know what they can do. The most important classes are \"Element\", \"Elements\", and \"Node\", which you can read about at http://thinkdast.com/jsoupelt, http://thinkdast.com/jsoupelts, and http://thinkdast.com/jsoupnode."], ["node , including, textnode ,, datanode ,, node DOM tree several subclasses, Element TextNode DataNode Comment Elements Collection Element objects, uml class diagram line hollow arrow head, another example diagram, element textnode datanode comment elements collection element objects, node dom tree several subclasses, uml diagrams", "\"Node\" represents a node in the DOM tree;  there are several subclasses that extend \"Node\", including \"Element\", \"TextNode\", \"DataNode\", and \"Comment\". \"Elements\" is a \"Collection\" of \"Element\" objects."], ["hollow arrow head indicates, one class extends another, section ~\\ ref, UML class diagram line hollow arrow head, another example diagram, UML diagrams, uml class diagram line hollow arrow head, another example diagram, element textnode datanode comment elements collection element objects, node dom tree several subclasses, uml diagrams", "  In a UML class diagram, a line with a hollow arrow head indicates that one class extends another.  For example, this diagram indicates that \"Elements\" extends \"ArrayList\". We'll get back to UML diagrams in Section~\\ref{uml-class-diagrams}."], ["select ( p ) element firstpara, purpose computer programming language, oriented ,{[} 13 {]}, provide class, WikiNodeIterable lets, nodes DOM, example shows, Elements paragraphs p Element firstPara, represent tags, provide class, nodes dom, selects first paragraph paragraphs, markup output java computer", "To make your life easier, I provide a class called \"WikiNodeIterable\" that lets you iterate through the nodes in a DOM tree. Here's an example that shows how to use it:\n    Elements paragraphs = content.select(\"p\");\n    Element firstPara = paragraphs.get(0);\n    Iterable<Node> iter = new WikiNodeIterable(firstPara);\n    for (Node node: iter) {\n        if (node instanceof TextNode) {\n            System.out.print(node);\n        }\n    }\nThis example picks up where the previous one leaves off. It selects the first paragraph in \"paragraphs\" and then creates a \"WikiNodeIterable\", which implements \"Iterable<Node>\". \"WikiNodeIterable\" performs a \"depth-first search\", which produces the nodes in the order they would appear on the page.\nIn this example, we print a \"Node\" only if it is a \"TextNode\" and ignore other types of \"Node\", specifically the \"Element\" objects that represent tags. The result is the plain text of the HTML paragraph without any markup. The output is:\n\"Java is a general-purpose computer programming language that is concurrent, class-based, object-oriented,{[}13{]} and specifically designed ...\""], ["private static void recursivedfs, , , several ways, tree different applications, search DFS DFS, root tree selects, child children selects, textnode prints contents node children, recursive calls, root node, order alternative use stack data structure, avoid recursion traverse tree", "There are several ways you might reasonably traverse a tree, each with different applications. We'll start with \"depth-first search\", or DFS. DFS starts at the root of the tree and selects the first child. If the child has children, it selects the first child again. When it gets to a node with no children, it backtracks, moving up the tree to the parent node, where it selects the next child if there is one; otherwise it backtracks again. When it has explored the last child of the root, it's done.\nThere are two common ways to implement DFS, recursively and iteratively. The recursive implementation is simple and elegant:\nprivate static void recursiveDFS(Node node) {\n    if (node instanceof TextNode) {\n        System.out.print(node);\n    }\n    for (Node child: node.childNodes()) {\n        recursiveDFS(child);\n    }\n}\nThis method gets invoked on every \"Node\" in the tree, starting with the root. If the \"Node\" it gets is a \"TextNode\", it prints the contents. If the \"Node\" has any children, it invokes \"recursiveDFS\" on each one of them in order.\nIn this example, we print the contents of each \"TextNode\" before traversing the children, so this is an example of a \"pre-order\" traversal. You can read about \"pre-order\", \"post-order\", and \"in-order\" traversals at http://thinkdast.com/treetrav.  For this application, the traversal order doesn't matter.\nBy making recursive calls, \"recursiveDFS\" uses the call stack (http://thinkdast.com/callstack) to keep track of the child nodes and process them in the right order. As an alternative, we can use a stack data structure to keep track of the nodes ourselves; if we do that, we can avoid the recursion and traverse the tree iteratively."], ["stack provides fewer methods, , constant time operations, iterative version DFS, stack data structure, general concept stack call stack lowercase talk, Java interfaces, stack methods Stack Deque stack data structure similar list collection, order elements primary difference stack list stack, remove end constant time operation careful add elements wrong place, lifo stands last first alternative stack queue returns elements order, example simple way implement stack list push element, big apis harder implement", "Before I explain the iterative version of DFS, I'll explain the stack data structure.  We'll start with the general concept of a stack, which I'll call a \"stack\" with a lowercase \"s\". Then we'll talk about two Java \"interfaces\" that define stack methods:\n\"Stack\" and \"Deque\".\nA stack is a data structure that is similar to a list: it is a collection that maintains the order of the elements. The primary difference between a stack and a list is that the stack provides fewer methods. In the usual convention, it provides:\n- \"push\": which adds an element to the top of the stack.\n- \"pop\": which removes and returns the top-most element from the stack.\n- \"peek\": which returns the top-most element without modifying the stack.\n- \"isEmpty\": which indicates whether the stack is empty.\nBecause \"pop\" always returns the top-most element, a stack is also called a \"LIFO\", which stands for \"last in, first out\". An alternative to a stack is a \"queue\", which returns elements in the same order they are added; that is, \"first in, first out\", or FIFO.\nIt might not be obvious why stacks and queues are useful: they don't provide any capabilities that aren't provided by lists; in fact, they provide fewer capabilities. So why not use lists for everything? There are two reasons:\n- If you limit yourself to a small set of methods --- that is, a small API --- your code will be more readable and less error-prone. For example, if you use a list to represent a stack, you might accidentally remove an element in the wrong order. With the stack API, this kind of mistake is literally impossible. And the best way to avoid errors is to make them impossible.\n- If a data structure provides a small API, it is easier to implement efficiently. For example, a simple way to implement a stack is a singly-linked list. When we push an element onto the stack, we add it to the beginning of the list; when we pop an element, we remove it from the beginning. For a linked list, adding and removing from the beginning are constant time operations, so this implementation is efficient. Conversely, big APIs are harder to implement efficiently.\nTo implement a stack in Java, you have three options:\n- Go ahead and use \"ArrayList\" or \"LinkedList\". If you use \"ArrayList\", be sure to add and remove from the end, which is a constant time operation. And be careful not to add elements in the wrong place or remove them in the wrong order.\n- Java provides a class called \"Stack\" that provides the standard set of stack methods. But this class is an old part of Java: it is not consistent with the Java Collections Framework, which came later.\n- Probably the best choice is to use one of the implementations of the \"Deque\" interface, like \"ArrayDeque\".\n\"Deque\" stands for \"double-ended queue\"; it's supposed to be pronounced \"deck\", but some people say \"deek\". In Java, the \"Deque\" interface provides \"push\", \"pop\", \"peek\", and \"isEmpty\", so you can use a \"Deque\" as a stack. It provides other methods, which you can read about at http://thinkdast.com/deque, but we won't use them for now."], ["arraydeque , java provides another implementation, private static void iterativedfs, childnodes ()) collections, iterative version DFS, ArrayDeque represent stack Node, private static void iterativeDFS Node root Deque Node, new ArrayDeque Node root Node node node instanceof TextNode node List Node, new ArrayList Node, another implementation deque old friend linkedlist linkedlist implements interfaces list deque interface get, methods different interfaces code, new linkedlist node use methods, linkedlist node, new linkedlist node use methods deque interface methods list interface assign list variable", "Here is an iterative version of DFS that uses an \"ArrayDeque\" to represent a stack of \"Node\" objects:\n    private static void iterativeDFS(Node root) {\n        Deque<Node> stack = new ArrayDeque<Node>();\n        stack.push(root);\n        while (!stack.isEmpty()) {\n            Node node = stack.pop();\n            if (node instanceof TextNode) {\n                System.out.print(node);\n            }\n            List<Node> nodes = new ArrayList<Node>(node.childNodes());\n            Collections.reverse(nodes);\n            for (Node child: nodes) {\n                stack.push(child);\n            }\n        }\n    }\nThe parameter, \"root\", is the root of the tree we want to traverse, so we start by creating the stack and pushing the root onto it.\nThe loop continues until the stack is empty. Each time through, it pops a \"Node\" off the stack. If it gets a \"TextNode\", it prints the contents. Then it pushes the children onto the stack. In order to process the children in the right order, we have to push them onto the stack in reverse order; we do that by copying the children into an \"ArrayList\", reversing the elements in place, and then iterating through the reversed \"ArrayList\".\nOne advantage of the iterative version of DFS is that it is easier to implement as a Java \"Iterator\"; you'll see how in the next chapter.\nBut first, one last note about the \"Deque\" interface: in addition to \"ArrayDeque\", Java provides another implementation of \"Deque\", our old friend \"LinkedList\". \"LinkedList\" implements both interfaces, \"List\" and \"Deque\". Which interface you get depends on how you use it. For example, if you assign a \"LinkedList\" object to a \"Deque\" variable, like this:\nDeqeue<Node> deque = new LinkedList<Node>();\nyou can use the methods in the \"Deque\" interface, but not all methods in the \"List\" interface. If you assign it to a \"List\" variable, like this:\nList<Node> deque = new LinkedList<Node>();\nyou can use \"List\" methods but not all \"Deque\" methods. And if you assign it like this:\nLinkedList<Node> deque = new LinkedList<Node>();\nyou can use all the methods. But if you combine methods from different interfaces, your code will be less readable and more error-prone."], ["one page per second, ant wikiphilosophy ,, ant build file, repository book, code help, previous chapter, recursive iterative implementations, DFS DOM tree, file run ant wikiphilosophy, iterable class, dfs dom tree, utility class, simple bit starter code", "In the repository for this book, you'll find some code to help you get started:\n- \"WikiNodeExample.java\" contains the code from the previous chapter, demonstrating recursive and iterative implementations of depth-first search (DFS) in a DOM tree.\n- \"WikiNodeIterable.java\" contains an \"Iterable\" class for traversing a DOM tree. I'll explain this code in the next section.\n- \"WikiFetcher.java\" contains a utility class that uses jsoup to download pages from Wikipedia. To help you comply with Wikipedia's terms of service, this class limits how fast you can download pages;\n  if you request more than one page per second, it sleeps before downloading the next page.\n- \"WikiPhilosophy.java\" contains an outline of the code you will write for this exercise. We'll walk through it below.\nYou'll also find the Ant build file \"build.xml\".  If you run \"ant WikiPhilosophy\", it will run a simple bit of starter code."], [" , , recursive version, previous chapter, iterative search DFS, advantage iterative version, recursive version, wrap Iterator object section see familiar Iterator Iterable interfaces, http http, iterative search dfs, recursive version, previous chapter, wrap iterator object section see familiar iterator iterable interfaces, advantage iterative version", "In the previous chapter, I presented an iterative depth-first search (DFS), and suggested that an advantage of the iterative version, compared to the recursive version, is that it is easier to wrap in an \"Iterator\" object. In this section we'll see how to do that.\nIf you are not familiar with the \"Iterator\" and \"Iterable\" interfaces, you can read about them at http://thinkdast.com/iterator and\nhttp://thinkdast.com/iterable."], ["override public boolean hasnext (), private class wikinodeiterator implements iterator, override public node next (), look contents outer class WikiNodeIterable implements Iterable Node interface use loop, Node root Iterable Node, new WikiNodeIterable root Node node iter visit node root root tree, traverse visit method, visit Node implementation WikiNodeIterable, conventional formula constructor, look contents outer class wikinodeiterable implements iterable node interface use loop, public class wikinodeiterable implements iterable node private node root public wikinodeiterable node root root override public iterator node iterator, stack empty next pops, node child nodes", "Take a look at the contents of \"WikiNodeIterable.java\". The outer class, \"WikiNodeIterable\" implements the \"Iterable<Node>\" interface  so we can use it in a for loop like this:\n    Node root = ... Iterable<Node> iter = new WikiNodeIterable(root);\n    for (Node node: iter) {\n        visit(node);\n    }\nwhere \"root\" is the root of the tree we want to traverse and \"visit\" is a method that does whatever we want when we \"visit\" a \"Node\".\nThe implementation of \"WikiNodeIterable\" follows a conventional formula:\n- The constructor takes and stores a reference to the root \"Node\".\n- The \"iterator\" method creates a returns an \"Iterator\" object.\nHere's what it looks like:\npublic class WikiNodeIterable implements Iterable<Node> {\n    private Node root;\n    public WikiNodeIterable(Node root) {\n        this.root = root;\n    }\n    @Override public Iterator<Node> iterator() {\n        return new WikiNodeIterator(root);\n    }\n}\nThe inner class, \"WikiNodeIterator\", does all the real work:\n    private class WikiNodeIterator implements Iterator<Node> {\n        Deque<Node> stack;\n        public WikiNodeIterator(Node node) {\n            stack = new ArrayDeque<Node>();\n            stack.push(root);\n        }\n        @Override public boolean hasNext() {\n            return !stack.isEmpty();\n        }\n        @Override public Node next() {\n            if (stack.isEmpty()) {\n                throw new NoSuchElementException();\n            }\n            Node node = stack.pop();\n            List<Node> nodes = new ArrayList<Node>(node.childNodes());\n            Collections.reverse(nodes);\n            for (Node child: nodes) {\n                stack.push(child);\n            }\n            return node;\n        }\n    }\nThis code is almost identical to the iterative version of DFS, but now it's split into three methods:\n- The constructor initializes the stack (which is implemented using an \"ArrayDeque\") and pushes the root node onto it.\n- \"isEmpty\" checks whether the stack is empty.\n- \"next\" pops the next \"Node\" off the stack, pushes its children in reverse order, and returns the \"Node\" it popped. If someone invokes \"next\" on an empty \"Iterator\", it throws an exception.\nIt might not be obvious that it is worthwhile to rewrite a perfectly good method with two classes and five methods.  But now that we've done it, we can use \"WikiNodeIterable\" anywhere an \"Iterable\" is called for, which makes it easy and syntactically clean to separate the logic of the iteration (DFS) from whatever processing we are doing on the nodes."], ["select ( p ) return paragraphs, throws ioexception public elements fetchwikipedia, wikifetcher : public class wikifetcher, write Web crawler, many pages, terms service, provide class, previous chapter, reasonable interval, wikifetcher singleton, dom element paragraph content text code look familiar new code, new wikifetcher string url urllist elements, wikifetcher object use handle requests multiple instances wikifetcher", "When you write a Web crawler, it is easy to download too many pages too fast, which might violate the terms of service for the server you are downloading from. To help you avoid that, I provide a class called \"WikiFetcher\" that does two things:\n- It encapsulates the code we demonstrated in the previous chapter for downloading pages from Wikipedia, parsing the HTML, and selecting the content text.\n- It measures the time between requests and, if we don't leave enough time between requests, it sleeps until a reasonable interval has elapsed. By default, the interval is one second.\nHere's the definition of \"WikiFetcher\":\npublic class WikiFetcher {\n    private long lastRequestTime = -1;\n    private long minInterval = 1000;\n    /** * Fetches and parses a URL string, * returning a list of paragraph elements. *\n     * @param url * @return * @throws IOException */\n    public Elements fetchWikipedia(String url) throws IOException {\n        sleepIfNeeded();\n        Connection conn = Jsoup.connect(url);\n        Document doc = conn.get();\n        Element content = doc.getElementById(\"mw-content-text\");\n        Elements paragraphs = content.select(\"p\");\n        return paragraphs;\n    }\n    private void sleepIfNeeded() {\n        if (lastRequestTime != -1) {\n            long currentTime = System.currentTimeMillis();\n            long nextRequestTime = lastRequestTime + minInterval;\n            if (currentTime < nextRequestTime) {\n                try {\n                    Thread.sleep(nextRequestTime - currentTime);\n                } catch (InterruptedException e) {\n                    System.err.println( \"Warning: sleep interrupted in fetchWikipedia.\");\n                }\n            }\n        }\n        lastRequestTime = System.currentTimeMillis();\n    }\n}\nThe only public method is \"fetchWikipedia\", which takes a URL as a \"String\" and returns an \"Elements\" collection that contains one DOM element for each paragraph in the content text. This code should look familiar.\nThe new code is in \"sleepIfNeeded\", which checks the time since the last request and sleeps if the elapsed time is less than \"minInterval\", which is in milliseconds.\nThat's all there is to \"WikiFetcher\". Here's an example that demonstrates how it's used:\n    WikiFetcher wf = new WikiFetcher();\n    for (String url: urlList) {\n        Elements paragraphs = wf.fetchWikipedia(url);\n        processParagraphs(paragraphs);\n    }\nIn this example, we assume that \"urlList\" is a collection of \"String\"s, and \"processParagraphs\" is a method that does something with the \"Elements\" object returned by \"fetchWikipedia\".\nThis example demonstrates something important: you should create one \"WikiFetcher\" object and use it to handle all requests. If you have multiple instances of \"WikiFetcher\", they won't enforce the minimum interval between requests.\nNOTE: My implementation of \"WikiFetcher\" is simple, but it would be easy for someone to misuse it by creating multiple instances. You could avoid this problem by making \"WikiFetcher\" a \"singleton\", which you can read about at http://thinkdast.com/singleton."], ["corresponding value , java provides several implementations, java provides several implementations, alternative map data structure, collection pairs, fast way look, value example, construct TermCounter, define class, possible implement intersection, collection pairs, key java, focus hashmap treemap", "A better alternative is a **map**, which is a data structure that represents a collection of **key-value pairs** and provides a fast way to look up a **key** and find the corresponding **value**. For example, the first map we'll construct is a \"TermCounter\", which maps from each search term to the number of times it appears in a page. The keys are the search terms and the values are the counts (also called \"frequencies\").\nJava provides an interface called \"Map\" that specifies the methods a map should provide; the most important are:\n- \"get(key)\": This method looks up a key and returns the corresponding value.\n- \"put(key, value)\": This method adds a new key-value pair to the \"Map\", or if the key is already in the map, it replaces the value associated with \"key\".\nJava provides several implementations of \"Map\", including the two we will focus on, \"HashMap\" and \"TreeMap\". In upcoming chapters, we'll look at these implementations and analyze their performance.\nIn addition to the \"TermCounter\", which maps from search terms to counts, we will define a class called \"Index\", which maps from a search term to a collection of pages where it appears. And that raises the next question, which is how to represent a collection of pages. Again, if we think about the operations we want to perform, that guides our decision.\nIn this case, we'll need to combine two or more collections and find the pages that appear in all of them. You might recognize this operation as **set intersection**: the intersection of two sets is the set of elements that appear in both.\nAs you might expect by now, Java provides a \"Set\" interface that defines the operations a set should perform. It doesn't actually provide set intersection, but it provides methods that make it possible to implement intersection and other set operations efficiently. The core \"Set\" methods are:\n- \"add(element)\": This method adds an element to a set; if the element is already in the set, it has no effect.\n- \"contains(element)\": This method checks whether the given element is in the set.\nJava provides several implementations of \"Set\", including \"HashSet\" and \"TreeSet\".\nNow that we've designed our data structures from the top down, we'll implement them from the inside out, starting with \"TermCounter\"."], ["methods take regular expressions, term ) return count == null, wrapper method, TermCounter class, search terms number times, page first part class definition public class TermCounter private Map String Integer, private String label public TermCounter String label, new HashMap String Integer instance variables, new pair map term, splits text words, term public void incrementtermcount string term, wikifetcher download page wikipedia parse main text, search terms number times", "\"TermCounter\" is a class that represents a mapping from search terms to the number of times they appear in a page. Here is the first part of the class definition:\npublic class TermCounter {\n    private Map<String, Integer> map;\n    private String label;\n    public TermCounter(String label) {\n        this.label = label;\n        this.map = new HashMap<String, Integer>();\n    }\n}\nThe instance variables are \"map\", which contains the mapping from terms to counts, and \"label\", which identifies the document the terms came from; we'll use it to store URLs.\nTo implement the mapping, I chose \"HashMap\", which is the most commonly-used \"Map\". Coming up in a few chapters, you will see how it works and why it is a common choice.\n\"TermCounter\" provides \"put\" and \"get\", which are defined like this:\n    public void put(String term, int count) {\n        map.put(term, count);\n    }\n    public Integer get(String term) {\n        Integer count = map.get(term);\n        return count == null ? 0 : count;\n    }\n\"put\" is just a **wrapper method**; when you call \"put\" on a \"TermCounter\", it calls \"put\" on the embedded map.\nOn the other hand, \"get\" actually does some work. When you call \"get\" on a \"TermCounter\", it calls \"get\" on the map, and then checks the result. If the term does not appear in the map, \"TermCount.get\" returns 0. Defining \"get\" this way makes it easier to write \"incrementTermCount\", which takes a term and increases by one the counter associated with that term.\n    public void incrementTermCount(String term) {\n        put(term, get(term) + 1);\n    }\nIf the term has not been seen before, \"get\" returns 0; we add 1, then use \"put\" to add a new key-value pair to the map. If the term is already in the map, we get the old count, add 1, and then store the new count, which replaces the old value.\nIn addition, \"TermCounter\" provides these other methods to help with indexing Web pages:\n    public void processElements(Elements paragraphs) {\n        for (Node node: paragraphs) {\n            processTree(node);\n        }\n    }\n    public void processTree(Node root) {\n        for (Node node: new WikiNodeIterable(root)) {\n            if (node instanceof TextNode) {\n                processText(((TextNode) node).text());\n            }\n        }\n    }\n    public void processText(String text) {\n        String[] array = text.replaceAll(\"\\\\pP\", \" \"). toLowerCase().\n                              split(\"\\\\s+\");\n        for (int i=0; i<array.length; i++) {\n            String term = array[i];\n            incrementTermCount(term);\n        }\n    }\n- \"processElements\" takes an \"Elements\" object, which is a collection of jsoup \"Element\" objects. It iterates through the collection and calls \"processTree\" on each.\n- \"processTree\" takes a jsoup \"Node\" that represents the root of a DOM tree. It iterates through the tree to find the nodes that contain text; then it extracts the text and passes it to \"processText\".\n- \"processText\" takes a \"String\" that contains words, spaces, punctuation, etc. It removes punctuation characters by replacing them with spaces, converts the remaining letters to lowercase, then splits the text into words. Then it loops through the words it found and calls \"incrementTermCount\" on each.  The \"replaceAll\" and \"split\" methods take **regular expressions** as parameters;\n  you can read more about them at http://thinkdast.com/regex.\nFinally, here's an example that demonstrates how \"TermCounter\" is used:\n    String url = \"http://en.wikipedia.org/wiki/Java_(programming_language)\";\n    WikiFetcher wf = new WikiFetcher();\n    Elements paragraphs = wf.fetchWikipedia(url);\n    TermCounter counter = new TermCounter(url);\n    counter.processElements(paragraphs);\n    counter.printCounts();\nThis example uses a \"WikiFetcher\" to download a page from Wikipedia and parse the main text. Then it creates a \"TermCounter\" and uses it to count the words in the page."], ["next section, missing method, understanding, next section chance run, works common choice termcounter, count words page, incrementtermcount replaceall split methods, termcounter class, jsoup node, elements object collection jsoup element, page first part class definition public class termcounter private map string integer, next section chance run, method call, public void", "In the next section, you'll have a chance to run this code and test your understanding by filling in a missing method."], ["override public v getvalue (), override public k getkey (), class uses two type parameters, usual provide starter code fill, MyLinearMap class definition public class MyLinearMap K V implements Map K V private List Entry, new ArrayList Entry class, type parameters K type keys V type values MyLinearMap implements Map, methods Map interface MyLinearMap, value override public k getkey, mylinearmap class definition public class mylinearmap k v implements map k v private list entry, inside mylinearlist, usual provide starter code fill, value much entry container key value definition, need exercise, new arraylist entry class, pair definition public class entry implements k v private k key private v value public entry k key v value, single instance variable entries, methods map interface mylinearmap, type parameters k v, key override public v getvalue", "As usual, I provide starter code and you will fill in the missing methods. Here's the beginning of the \"MyLinearMap\" class definition:\npublic class MyLinearMap<K, V> implements Map<K, V> {\n    private List<Entry> entries = new ArrayList<Entry>();\nThis class uses two type parameters, \"K\", which is the type of the keys, and \"V\", which is the type of the values. \"MyLinearMap\" implements \"Map\", which means it has to provide the methods in the \"Map\" interface.\nA \"MyLinearMap\" object has a single instance variable, \"entries\", which is an \"ArrayList\" of \"Entry\" objects. Each \"Entry\" contains a key-value pair. Here is the definition:\n    public class Entry implements Map.Entry<K, V> {\n        private K key;\n        private V value;\n        public Entry(K key, V value) {\n            this.key = key;\n            this.value = value;\n        }\n        @Override public K getKey() {\n            return key;\n        }\n        @Override public V getValue() {\n            return value;\n        }\n    }\nThere's not much to it; an \"Entry\" is just a container for a key and a value. This definition is nested inside \"MyLinearList\", so it uses the same type parameters, \"K\" and \"V\"."], ["get started, need, let, need exercise, mylinearmap class definition public class mylinearmap k v implements map k v private list entry, entry objects entry, inside mylinearlist, new arraylist entry class, need exercise, single instance variable entries, methods map interface mylinearmap, type parameters k v, type parameters k type keys v type values mylinearmap implements map, key override public v getvalue", "That's all you need to do the exercise, so let's get started."], ["equals : private entry findentry, value )) return null, value ) return oldvalue, section present solution previous exercise analyze performance core methods, equals private Entry findEntry Object target Entry entry entries, return entry, null private boolean equals Object target Object, target null return obj null return obj runtime equals, constant time remember entries arraylist, add entry call constant time, findentry everything, v value entry, number entries equals constant time findentry", "In this section I present a solution to the previous exercise and analyze the performance of the core methods.  Here are \"findEntry\" and \"equals\":\nprivate Entry findEntry(Object target) {\n    for (Entry entry: entries) {\n        if (equals(target, entry.getKey())) {\n            return entry;\n        }\n    }\n    return null;\n}\nprivate boolean equals(Object target, Object obj) {\n    if (target == null) {\n        return obj == null;\n    }\n    return target.equals(obj);\n}\nThe runtime of \"equals\" might depend on the size of the \"target\" and the keys, but does not generally depend on the number of entries, n. So \"equals\" is constant time.\nIn \"findEntry\", we might get lucky and find the key we're looking for at the beginning, but we can't count on it. In general, the number of entries we have to search is proportional to n, so \"findEntry\" is linear.\nMost of the core methods in \"MyLinearMap\" use \"findEntry\", including \"put\", \"get\", and \"remove\". Here's what they look like:\npublic V put(K key, V value) {\n    Entry entry = findEntry(key);\n    if (entry == null) {\n        entries.add(new Entry(key, value));\n        return null;\n    } else {\n        V oldValue = entry.getValue();\n        entry.setValue(value);\n        return oldValue;\n    }\n}\npublic V get(Object key) {\n    Entry entry = findEntry(key);\n    if (entry == null) {\n        return null;\n    }\n    return entry.getValue();\n}\npublic V remove(Object key) {\n    Entry entry = findEntry(key);\n    if (entry == null) {\n        return null;\n    } else {\n        V value = entry.getValue();\n        entries.remove(entry);\n        return value;\n    }\n}\nAfter \"put\" calls \"findEntry\", everything else is constant time. Remember that \"entries\" is an \"ArrayList\", so adding an element at the end is constant time, on average. If the key is already in the map, we don't have to add an entry, but we have to call \"entry.getValue\" and \"entry.setValue\", and those are both constant time. Putting it all together, \"put\" is linear."], ["two linear operations, takes linear time, still linear, middle ArrayList, linear time, linear operations, needle haystack constant time, linear operations, summary core methods, number entries small implementation, implementation mylinearmap, big haystack magic, middle arraylist, linear time", "By the same reasoning, \"get\" is also linear.\n\"remove\" is slightly more complicated because \"entries.remove\" might have to remove an element from the beginning or middle of the \"ArrayList\", and that takes linear time. But that's OK: two linear operations are still linear."], ["seem possible, good enough, first hear, summary core methods, implementation MyLinearMap, number entries small implementation, fact implementation Map core methods constant time first hear, needle haystack constant time, needle haystack constant time, linear operations, fact implementation map core methods constant time first hear, number entries small implementation, summary core methods, implementation mylinearmap, big haystack magic, middle arraylist, linear time", "In summary, the core methods are all linear, which is why we called this implementation \"MyLinearMap\" (ta-da!).\nIf we know that the number of entries will be small, this implementation might be good enough, but we can do better. In fact, there is an implementation of \"Map\" where all of the core methods are constant time. When you first hear that, it might not seem possible. What we are saying, in effect, is that you can find a needle in a haystack in constant time, regardless of how big the haystack is. It's magic."], ["entries per list, storing entries, two steps, big List break lots short lists, hash code, next section determine list, lots short lists, explain change order growth core operations, efficient implementation, needle haystack constant time, explain change order growth core operations, linear operations, trick increase number, summary core methods, number entries small implementation, list result map, big haystack magic, middle arraylist, next section determine list, linear time", "I'll explain how it works in two steps:\n- Instead of storing entries in one big \"List\", we'll break them up into lots of short lists. For each key, we'll use a \\textbf{hash code} (explained in the next section) to determine which list to use.\n- Using lots of short lists is faster than using just one, but as I'll explain, it doesn't change the order of growth; the core operations are still linear. But there is one more trick: if we increase the number of lists to limit the number of entries per list, the result is a constant-time map. You'll see the details in the next exercise, but first: hashing!\nIn the next chapter, I'll present a solution, analyze the performance of the core \"Map\" methods, and introduce a more efficient implementation."], ["size ()- 1 , get : public v put, hash function,, performance MyLinearMap, new class, collection MyLinearMap, divides keys, embedded maps number entries, added key, approach use hash function, key null index, divides keys, public mybettermap int k makemaps", "To improve the performance of \"MyLinearMap\", we'll write a new class, called \"MyBetterMap\", that contains a collection of \"MyLinearMap\" objects. It divides the keys among the embedded maps, so the number of entries in each map is smaller, which speeds up \"findEntry\" and the methods that depend on it.\nHere's the beginning of the class definition:\npublic class MyBetterMap<K, V> implements Map<K, V> {\n    protected List<MyLinearMap<K, V>> maps;\n    public MyBetterMap(int k) {\n        makeMaps(k);\n    }\n    protected void makeMaps(int k) {\n        maps = new ArrayList<MyLinearMap<K, V>>(k);\n        for (int i=0; i<k; i++) {\n            maps.add(new MyLinearMap<K, V>());\n        }\n    }\n}\nThe instance variable, \"maps\", is a collection of \"MyLinearMap\" objects. The constructor takes a parameter, \"k\", that determines how many maps to use, at least initially. Then \"makeMaps\" creates the embedded maps and stores them in an \"ArrayList\".\nNow, the key to making this work is that we need some way to look at a key and decide which of the embedded maps it should go into. When we \"put\" a new key, we choose one of the maps; when we \"get\" the same key, we have to remember where we put it.\nOne possibility is to choose one of the sub-maps at random and keep track of where we put each key. But how should we keep track? It might seem like we could use a \"Map\" to look up the key and find the right sub-map, but the whole point of the exercise is to write an efficient implementation of a \"Map\". We can't assume we already have one.\nA better approach is to use a **hash function**, which takes an \"Object\", any \"Object\", and returns an integer called a **hash code**.  Importantly, if it sees the same \"Object\" more than once, it always returns the same hash code. That way, if we use the hash code to store a key, we'll get the same hash code when we look it up.\nIn Java, every \"Object\" provides a method called \"hashCode\" that computes a hash function. The implementation of this method is different for different objects; we'll see an example soon.\nHere's a helper method that chooses the right sub-map for a given key:\nprotected MyLinearMap<K, V> chooseMap(Object key) {\n    int index = 0;\n    if (key != null) {\n        index = Math.abs(key.hashCode())\n    }\n    return maps.get(index);\n}\nIf \"key\" is \"null\", we choose the sub-map with index 0, arbitrarily. Otherwise we use \"hashCode\" to get an integer, apply \"Math.abs\" to make sure it is non-negative, then use the remainder operator, \"\\%\", which guarantees that the result is between 0 and \"maps.size()-1\". So \"index\" is always a valid index into \"maps\". Then \"chooseMap\" returns a reference to the map it chose.\nWe use \"chooseMap\" in both \"put\" and \"get\", so when we look up a key, we get the same map we chose when we added the key. At least, we should --- I'll explain a little later why this might not work.\nHere's my implementation of \"put\" and \"get\":\npublic V put(K key, V value) {\n  MyLinearMap<K, V> map = chooseMap(key);\n    return map.put(key, value);\n}\npublic V get(Object key) {\n    MyLinearMap<K, V> map = chooseMap(key);\n    return map.get(key);\n}\nPretty simple, right? In both methods, we use \"chooseMap\" to find the right sub-map and then invoke a method on the sub-map. That's how it works; now let's think about performance.\nIf there are n entries split up among k sub-maps, there will be n/k entries per map, on average. When we look up a key, we have to compute its hash code, which takes some time, then we search the corresponding sub-map.\nBecause the entry lists in \"MyBetterMap\" are k times shorter than the entry list in \"MyLinearMap\", we expect the search to be k times faster. But the runtime is still proportional to n, so \"MyBetterMap\" is still linear. In the next exercise, you'll see how we can fix that."], ["override public int hashcode (), string : public class sillystring, like ac , fundamental requirement hash function, hash code every time immutable objects, easy objects mutable state think harder example immutable object define class, public class, public SillyString String, hash code, add character int java, many different strings, public string, int total return total notice sillystring", "The fundamental requirement for a hash function is that the same object should produce the same hash code every time. For immutable objects, that's relatively easy. For objects with mutable state, we have to think harder.\nAs an example of an immutable object, I'll define a class called \"SillyString\" that encapsulates a \"String\":\npublic class SillyString {\n    private final String innerString;\n    public SillyString(String innerString) {\n        this.innerString = innerString;\n    }\n    public String toString() {\n        return innerString;\n    }\nThis class is not very useful, which is why it's called \"SillyString\", but I'll use it to show how a class can define its own hash function:\n    @Override public boolean equals(Object other) {\n        return this.toString().equals(other.toString());\n    }\n    @Override public int hashCode() {\n        int total = 0;\n        for (int i=0; i<innerString.length(); i++) {\n            total += innerString.charAt(i);\n        }\n        return total;\n    }\nNotice that \"SillyString\" overrides both \"equals\" and \"hashCode\". This is important. In order to work properly, \"equals\" has to be consistent with \"hashCode\", which means that if two objects are considered equal --- that is, \"equals\" returns \"true\" --- they should have the same hash code. But this requirement only works one way; if two objects have the same hash code, they don't necessarily have to be equal.\n\"equals\" works by invoking \"toString\", which returns \"innerString\". So two \"SillyString\" objects are equal if their \"innerString\" instance variables are equal.\n\"hashCode\" works by iterating through the characters in the \"String\" and adding them up. When you add a character to an \"int\", Java converts the character to an integer using its Unicode code point. You don't need to know anything about Unicode to understand this example, but if you are curious, you can read more at http://thinkdast.com/codepoint.\nThis hash function satisfies the requirement: if two \"SillyString\" objects contain embedded strings that are equal, they will get the same hash code.\nThis works correctly, but it might not yield good performance, because it returns the same hash code for many different strings. If two strings contain the same letters in any order, they will have the same hash code. And even if they don't contain the same letters, they might yield the same total, like \"\"ac\"\" and \"\"bb\"\".\nIf many objects have the same hash code, they end up in the same sub-map.  If some sub-maps have more entries than others, the speedup when we have k maps might be much less than k. So one of the goals of a hash function is to be uniform; that is, it should be equally likely to produce any value in the range.  You can read more about designing good hash functions at http://thinkdast.com/hash."], ["new sillyarray ( word1 tochararray ()) map, override public int hashcode (), string : public class sillyarray, immutable SillyString, immutable innerString, final create SillyString, different String, refers Therefore, new sillyarray word1, hash code, data structures, map bad general dangerous use mutable objects, c suppose create sillyarray add map sillyarray", "\"String\"s are immutable, and \"SillyString\" is also immutable because \"innerString\" is declared to be \"final\". Once you create a \"SillyString\", you can't make \"innerString\" refer to a different \"String\", and you can't modify the \"String\" it refers to. Therefore, it will always have the same hash code.\nBut let's see what happens with a mutable object. Here's a definition for \"SillyArray\", which is identical to \"SillyString\", except that it uses an array of characters instead of a \"String\":\npublic class SillyArray {\n    private final char[] array;\n    public SillyArray(char[] array) {\n        this.array = array;\n    }\n    public String toString() {\n        return Arrays.toString(array);\n    }\n    @Override public boolean equals(Object other) {\n        return this.toString().equals(other.toString());\n    }\n    @Override public int hashCode() {\n        int total = 0;\n        for (int i=0; i<array.length; i++) {\n            total += array[i];\n        }\n        System.out.println(total);\n        return total;\n    }\n\"SillyArray\" also provides \"setChar\", which makes it possible to modify the characters in the array:\npublic void setChar(int i, char c) {\n    this.array[i] = c;\n}\nNow suppose we create a \"SillyArray\" and add it to a map:\nSillyArray array1 = new SillyArray(\"Word1\".toCharArray());\nmap.put(array1, 1);\nThe hash code for this array is 461. Now if we modify the contents of the array and then try to look it up, like this:\narray1.setChar(0, 'C');\nInteger value = map.get(array1);\nthe hash code after the mutation is 441. With a different hash code, there's a good chance we'll go looking in the wrong sub-map. In that case, we won't find the key, even though it is in the map. And that's bad.\nIn general, it is dangerous to use mutable objects as keys in data structures that use hashing, which includes \"MyBetterMap\" and \"HashMap\". If you can guarantee that the keys won't be modified while they are in the map, or that any changes won't affect the hash code, it might be OK. But it is probably a good idea to avoid it."], ["methods become constant time, target ) return map, key ) return map, number entries, proportional k, proportion n several core MyBetterMap, constant time public boolean containsKey Object target MyLinearMap K V map chooseMap target, target public V get Object key MyLinearMap K V map chooseMap key return key public V, constant time, object key mylinearmap k v map choosemap key return key method hashes key constant time invokes, good core method, proportional k, analyze rehash constant time linear way, target public v get object key mylinearmap k v map choosemap key return key public v, constant time public boolean containskey object target mylinearmap k v map choosemap target, amortized analysis see, proportion n several core mybettermap, number entries, reason turns constant time average series invocations", "If the number of entries in the biggest sub-map is proportional to n/k, and k grows in proportion to n, several of the core \"MyBetterMap\" methods become constant time:\n    public boolean containsKey(Object target) {\n        MyLinearMap<K, V> map = chooseMap(target);\n        return map.containsKey(target);\n    }\n    public V get(Object key) {\n        MyLinearMap<K, V> map = chooseMap(key);\n        return map.get(key);\n    }\n    public V remove(Object key) {\n        MyLinearMap<K, V> map = chooseMap(key);\n        return map.remove(key);\n    }\nEach method hashes a key, which is constant time, and then invokes a method on a sub-map, which is constant time."], ["see section ~\\ ref, section ~\\ ref, little harder, good core method, analyze rehash constant time linear way, reason turns constant time average series invocations, amortized analysis see, object key mylinearmap k v map choosemap key return key method hashes key constant time invokes, good core method, amortized analysis see, proportion n several core mybettermap, analyze rehash constant time linear way, reason turns constant time average series invocations", "So far, so good. But the other core method, \"put\", is a little harder to analyze. When we don't have to rehash, it is constant time, but when we do, it's linear. In that way, it's similar to \"ArrayList.add\", which we analyzed in Section~\\ref{classifying-add}.\nFor the same reason, \"MyHashMap.put\" turns out to be constant time if we average over a series of invocations. Again, the argument is based on amortized analysis (see Section~\\ref{classifying-add})."], ["second time also takes 1 unit, diagram also shows, takes 1 unit, Suppose initial number, load factor, much work, series keys basic unit work count number times, first time call, knock towers, space next tower result uniform height, load factor, series keys basic unit work count number times, unit work second time", "Suppose the initial number of sub-maps, k, is 2, and the load factor is 1. Now let's see how much work it takes to \"put\" a series of keys. As the basic \"unit of work\", we'll count the number of times we have to hash a key and add it to a sub-map.\nThe first time we call \"put\" it takes 1 unit of work. The second time also takes 1 unit. The third time we have to rehash, so it takes 2 units to rehash the existing keys and 1 unit to hash the new key.\nNow the size of the hash table is 4, so the next time we call \"put\", it takes 1 unit of work. But the next time we have to rehash, which takes 4 units to rehash the existing keys and 1 unit to hash the new key.\nAs the arrows suggest, if we knock down the towers, each one fills the space before the next tower. The result is a uniform height of 2 units, which shows that the average work per \"put\" is about 2 units. And that means that \"put\" is constant time on average.\nThis diagram also shows why it is important to double the number of sub-maps, k, when we rehash. If we only add to k instead of multiplying, the towers would be too close together and they would start piling up. And that would not be constant time."], ["data structures small enough, public void clear (), possibly keys ) , containsKey get, constant time, constant time, minute appreciate remarkable performance operations, much matter big hash table Well sort Remember analysis, structure fit cache, constant time, containskey get, simple model computation unit work, amount time real computers", "We've shown that \"containsKey\", \"get\", and \"remove\" are constant time, and \"put\" is constant time on average. We should take a minute to appreciate how remarkable that is. The performance of these operations is pretty much the same no matter how big the hash table is. Well, sort of.\nRemember that our analysis is based on a simple model of computation where each \"unit of work\" takes the same amount of time. Real computers are more complicated than that. In particular, they are usually fastest when working with data structures small enough to fit in cache; somewhat slower if the structure doesn't fit in cache but still fits in memory; and much slower if the structure doesn't fit in memory.\nAnother limitation of this implementation is that hashing doesn't help if we are given a value rather than a key: \"containsValue\" is linear because it has to search all of the sub-maps. And there is no particularly efficient way to look up a value and find the corresponding key (or possibly keys).\nAnd there's one more limitation: some of the methods that were constant time in \"MyLinearMap\" have become linear. For example:\n    public void clear() {\n        for (int i=0; i<maps.size(); i++) {\n            maps.get(i).clear();\n        }\n    }\n\"clear\" has to clear all of the sub-maps, and the number of sub-maps is proportional to n, so it's linear. Fortunately, this operation is not used very often, so for most applications this tradeoff is acceptable."], ["plots runtime versus problem size, get something similar, performance bug , constant time Run, compile source files, ant ProfileMapPut measures, Java range problem, versus problem size scale operation constant time total time n operations, ran code, next section track error, runtimes milliseconds, implementation constant time, ant profilemapput measures", "Before we go on, we should check whether \"MyHashMap.put\" is really constant time.\nRun \"ant build\" to compile the source files. Then run \"ant ProfileMapPut\". It measures the runtime of \"HashMap.put\" (provided by Java) with a range of problem sizes, and plots runtime versus problem size on a log-log scale. If this operation is constant time, the total time for n operations should be linear, so the result should be a straight line with slope 1. When I ran this code, the estimated slope was close to 1, which is consistent with our analysis. You should get something similar.\nModify \"ProfileMapPut.java\" so it profiles your implementation, \"MyHashMap\", instead of Java's \"HashMap\". Run the profiler again and see if the slope is near 1. You might have to adjust \"startN\" and \"endMillis\" to find a range of problem sizes where the runtimes are more than a few milliseconds, but not more than a few thousand.\nWhen I ran this code, I got a surprise: the slope was about 1.7, which suggests that this implementation is not constant time after all. It contains a \"performance bug\".\nBefore you read the next section, you should track down the error, fix it, and confirm that \"put\" is now constant time, as expected."], ["mybettermap : public int size (), key ) size -= map, key ) size -= map, problem MyHashMap size, MyBetterMap public int size int, MyLinearMap K V map maps total return total add total size iterate, increase number k number entries, proportional n size, total time, constant time, solution repository book, n keys, rather modify myhashmap", "The problem with \"MyHashMap\" is in \"size\", which is inherited from \"MyBetterMap\":\n    public int size() {\n        int total = 0;\n        for (MyLinearMap<K, V> map: maps) {\n            total += map.size();\n        }\n        return total;\n    }\nTo add up the total size it has to iterate the sub-maps. Since we increase the number of sub-maps, k, as the number of entries, n, increases, k is proportional to n, so \"size\" is linear.\nAnd that makes \"put\" linear, too, because it uses \"size\":\n    public V put(K key, V value) {\n        V oldValue = super.put(key, value);\n        if (size() > maps.size() * FACTOR) {\n            rehash();\n        }\n        return oldValue;\n    }\nEverything we did to make \"put\" constant time is wasted if \"size\" is linear!\nFortunately, there is a simple solution, and we have seen it before: we have to keep the number of entries in an instance variable and update it whenever we call a method that changes it.\nYou'll find my solution in the repository for this book, in \"MyFixedHashMap.java\".  Here's the beginning of the class definition:\npublic class MyFixedHashMap<K, V> extends MyHashMap<K, V> implements Map<K, V> {\n    private int size = 0;\n    public void clear() {\n        super.clear();\n        size = 0;\n    }\nRather than modify \"MyHashMap\", I define a new class that extends it. It adds a new instance variable, \"size\", which is initialized to zero.\nUpdating \"clear\" is straightforward; we invoke \"clear\" in the superclass (which clears the sub-maps), and then update \"size\".\nUpdating \"remove\" and \"put\" is a little more difficult because when we invoke the method on the superclass, we can't tell whether the size of the sub-map changed. Here's how I worked around that:\n    public V remove(Object key) {\n        MyLinearMap<K, V> map = chooseMap(key);\n        size -= map.size();\n        V oldValue = map.remove(key);\n        size += map.size();\n        return oldValue;\n    }\n\"remove\" uses \"chooseMap\" to find the right sub-map, then subtracts away the size of the sub-map. It invokes \"remove\" on the sub-map, which may or may not change the size of the sub-map, depending on whether it finds the key. But either way, we add the new size of the sub-map back to \"size\", so the final value of \"size\" is correct.\nThe rewritten version of \"put\" is similar:\n    public V put(K key, V value) {\n        MyLinearMap<K, V> map = chooseMap(key);\n        size -= map.size();\n        V oldValue = map.put(key, value);\n        size += map.size();\n        if (size() > maps.size() * FACTOR) {\n            size = 0;\n            rehash();\n        }\n        return oldValue;\n    }\nWe have the same problem here: when we invoke \"put\" on the sub-map, we don't know whether it added a new entry. So we use the same solution, subtracting off the old size and then adding in the new size.\nNow the implementation of the \"size\" method is simple:\n    public int size() {\n        return size;\n    }\nAnd that's pretty clearly constant time."], ["putting n keys, total time, constant time, total time, n keys, constant time, total time, constant time, key value size size factor size, rehash return oldvalue problem invoke, public void clear size, solution repository book, problem myhashmap size, increase number k number entries, n keys, little difficult invoke method superclass, old size, rather modify myhashmap", "When I profiled this solution, I found that the total time for putting n keys is proportional to n, which means that each \"put\" is constant time, as it's supposed to be."], ["software engineers often use uml class diagrams , uml class diagrams provide, see , code chapter several classes, relationships classes MyLinearMap, LinkedList implements Map MyBetterMap, many MyLinearMap, implements Map MyHashMap, diagram every class implements map uml class diagrams, implements map myfixedhashmap, arrow arrows hollow head, linkedlist implements map mybettermap, uml class diagram class", "One challenge of working with the code in this chapter is that we have several classes that depend on each other. Here are some of the relationships between the classes:\n- \"MyLinearMap\" contains a \"LinkedList\" and implements \"Map\".\n- \"MyBetterMap\" contains many \"MyLinearMap\" objects and implements \"Map\". - \"MyHashMap\" extends \"MyBetterMap\", so it also contains \"MyLinearMap\" objects, and it implements \"Map\". - \"MyFixedHashMap\" extends \"MyHashMap\" and implements \"Map\".\nTo help keep track of relationships like these, software engineers often use **UML class diagrams**. UML stands for Unified Modeling Language\n(see http://thinkdast.com/uml). A \"class diagram\" is one of several graphical standards defined by UML.\nIn a class diagram, each class is represented by a box, and relationships between classes are represented by arrows. me/.\nDifferent relationships are represented by different arrows:\n- Arrows with a solid head indicate HAS-A relationships. For example, each instance of \"MyBetterMap\" contains multiple instances of \"MyLinearMap\", so they are connected by a solid arrow.\n- Arrows with a hollow head and a solid line indicate IS-A relationships. For example, \"MyHashMap\" extends \"MyBetterMap\", so they are connected by an IS-A arrow.\n- Arrows with a hollow head and a dashed line indicate that a class implements an interface; in this diagram, every class implements \"Map\".\nUML class diagrams provide a concise way to represent a lot of information about a collection of classes. They are used during design phases to communicate about alternative designs, during implementation phases to maintain a shared mental map of the project, and during deployment to document the design."], ["binary search tree,, binary search trees work, might want another implementation, point familiar Map interface HashMap implementation, hash table understand HashMap, core methods constant time performance HashMap, implementation Map reasons, another implementation Hashing, binary search tree, core methods constant time performance hashmap, point familiar map interface hashmap implementation, binary search trees, hash table", "At this point you should be familiar with the \"Map\" interface and the \"HashMap\" implementation provided by Java. And by making your own \"Map\" using a hash table, you should understand how \"HashMap\" works and why we expect its core methods to be constant time.\nBecause of this performance, \"HashMap\" is widely used, but it is not the only implementation of \"Map\". There are a few reasons you might want another implementation:\n- Hashing can be slow, so even though \"HashMap\" operations are constant time, the \"constant\" might be big.\n- Hashing works well if the hash function distributes the keys evenly among the sub-maps.  But designing good hash functions is not easy, and if too many keys end up in the same sub-map, the performance of the \"HashMap\" may be poor.\n- The keys in a hash table are not stored in any particular order; in fact, the order might change when the table grows and the keys are rehashed. For some applications, it is necessary, or at least useful, to keep the keys in order.\nIt is hard to solve all of these problems at the same time, but Java provides an implementation called \"TreeMap\" that comes close:\n- It doesn't use a hash function, so it avoids the cost of hashing and the difficulty of choosing a hash function.\n- Inside the \"TreeMap\", the keys are are stored in a **binary search tree**, which makes it possible to traverse the keys, in order, in linear time.\n- The runtime of the core methods is proportional to log n, which is not quite as good as constant time, but it is still very good.\nIn the next section, I'll explain how binary search trees work and then you will use one to implement a \"Map\". Along the way, we'll analyze the performance of the core map methods when implemented using a tree."], ["log_2 n ≈ h, tree contains nine keys, , binary search tree BST tree node, key every node BST property node, child keys, key node node right child keys right subtree, key node figure Wikipedia page binary search trees, keys general number comparisons, sides n h means, pattern number levels, current key search, true see", "A binary search tree (BST) is a tree where each node contains a key, and every \"node\" has the \"BST property\":\n- If \"node\" has a left child, all keys in the left subtree must be less than the key in \"node\".\n- If \"node\" has a right child, all keys in the right subtree must be greater than the key in \"node\".\nThis figure is from the Wikipedia page on binary search trees at http://thinkdast.com/bst, which you might find useful while you work on this exercise.\nThe key in the root is 8, and you can confirm that all keys to the left of the root are less than 8, and all keys to the right are greater. You can also check that the other nodes have this property.\nLooking up a key in a binary search tree is fast because we don't have to search the entire tree. Starting at the root, we can use the following algorithm:\n- Compare the key you are looking for, \"target\", to the key in the current node. If they are equal, you are done.\n- If \"target\" is less than the current key, search the left tree. If there isn't one, \"target\" is not in the tree.\n- If \"target\" is greater than the current key, search the right tree. If there isn't one, \"target\" is not in the tree.\nAt each level of the tree, you only have to search one child. For example, if you look for \"target = 4\" in the previous diagram, you start at the root, which contains the key \"8\". Because \"target\" is less than \"8\", you go left. Because \"target\" is greater than \"3\" you go right. Because \"target\" is less than \"6\", you go left. And then you find the key you are looking for.\nIn this example, it takes four comparisons to find the target, even though the tree contains nine keys. In general, the number of comparisons is proportional to the height of the tree, not the number of keys in the tree.\nSo what can we say about the relationship between the height of the tree, \"h\", and the number of nodes, n? Starting small and working up:\n- If \"h=1\", the tree only contains one node, so \"n=1\".\n- If \"h=2\", we can add two more nodes, for a total of \"n=3\".\n- If \"h=3\", we can add up to four more nodes, for a total of \"n=7\".\n- If \"h=4\", we can add up to eight more nodes, for a total of \"n=15\".\nBy now you might see the pattern. If we number the levels of the tree from \"1\" to \"h\", the level with index \"i\" can have up to 2^{i-1} nodes. And the total number of nodes in \"h\" levels is 2^h-1. If we have n = 2^h - 1, we can take the logarithm base 2 of both sides: log_2 n ≈ h, which means that the height of the tree is proportional to log n, if the tree is full; that is, if each level contains the maximum number of nodes.\nSo we expect that we can look up a key in a binary search tree in time proportional to log n. This is true if the tree is full, and even if the tree is only partially full. But it is not always true, as we will see."], ["takes time proportional, log time ,, log n ) , time proportional log n, logarithmic log time belongs order growth log n, binary search tree bst tree node, key node node right child keys right subtree, tree proportional log n, logarithmic log time belongs order growth log n, current key search, time proportional log n, algorithm compare, useful work exercise key root", "An algorithm that takes time proportional to log n is called \"logarithmic\" or \"log time\", and it belongs to the order of growth O(log n)."], ["suppresswarnings ( unchecked ) comparable <?, throw new illegalargumentexception (), super k >) target, repository book, source files, previous section outlines, methods contains unit, MyTreeMap Run, java type system point exercise job fill, compare target key tree signature, exception implementations map, able compare keys, repository book", "In the repository for this book, you'll find these source files:\n- \"MyTreeMap.java\" contains the code from the previous section with outlines for the missing methods.\n- \"MyTreeMapTest.java\" contains the unit tests for \"MyTreeMap\".\nRun \"ant build\" to compile the source files. Then run \"ant MyTreeMapTest\".  Several tests should fail, because you have some work to do!\nI've provided outlines for \"get\" and \"containsKey\".  Both of them use \"findNode\", which is a private method I defined; it is not part of the \"Map\" interface. Here's how it starts:\n    private Node findNode(Object target) {\n        if (target == null) {\n            throw new IllegalArgumentException();\n        }\n        @SuppressWarnings(\"unchecked\") Comparable<? super K> k = (Comparable<? super K>) target;\n        // TODO: FILL THIS IN!\n        return null;\n    }\nThe parameter \"target\" is the key we're looking for. If \"target\" is \"null\", \"findNode\" throws an exception. Some implementations of \"Map\" can handle \"null\" as a key, but in a binary search tree, we need to be able to compare keys, so dealing with \"null\" is problematic. To keep things simple, this implementation does not allow \"null\" as a key.\nThe next lines show how we can compare \"target\" to a key in the tree. From the signature of \"get\" and \"containsKey\", the compiler considers \"target\" to be an \"Object\". But we need to be able to compare keys, so we typecast \"target\" to \"Comparable<? super K>\", which means that it is comparable to an instance of type \"K\", or any superclass of \"K\".  If you are not familiar with this use of \"type wildcards\", you can read more at http://thinkdast.com/gentut.\nFortunately, dealing with Java's type system is not the point of this exercise. Your job is to fill in the rest of \"findNode\". If it finds a node that contains \"target\" as a key, it should return the node. Otherwise it should return \"null\". When you get this working, the tests for \"get\" and \"containsKey\" should pass."], ["value ) size ++ return null, throw new illegalargumentexception (), take time proportional, Note solution search, path tree, time proportional height tree search whole tree, task fill containsValue get, method equals, null return, instance variable root otherwise, tree replaces old value new returns old value key, opposed keys, starter code", "Note that your solution should only search one path through the tree, so it should take time proportional to the height of the tree. You should not search the whole tree!\nYour next task is to fill in \"containsValue\". To get you started, I've provided a helper method, \"equals\", that compares \"target\" and a given key. Note that the values in the tree (as opposed to the keys) are not necessarily comparable, so we can't use \"compareTo\"; we have to invoke \"equals\" on \"target\".\nUnlike your previous solution for \"findNode\", your solution for \"containsValue\" does have to search the whole tree, so its runtime is proportional to the number of keys, n, not the height of the tree, \"h\".\nThe next method you should fill in is \"put\". I've provided starter code that handles the simple cases:\n    public V put(K key, V value) {\n        if (key == null) {\n            throw new IllegalArgumentException();\n        }\n        if (root == null) {\n            root = new Node(key, value);\n            size++;\n            return null;\n        }\n        return putHelper(root, key, value);\n    }\n    private V putHelper(Node node, K key, V value) {\n        // TODO: Fill this in.\n    }\nIf you try to put \"null\" as a key, \"put\" throws an exception.\nIf the tree is empty, \"put\" creates a new node and initializes the instance variable \"root\".\nOtherwise, it calls \"putHelper\", which is a private method I defined; it is not part of the \"Map\" interface.\nFill in \"putHelper\" so it searches the tree and:\n- If \"key\" is already in the tree, it replaces the old value with the new, and returns the old value.\n- If \"key\" is not in the tree, it creates a new node, finds the right place to add it, and returns \"null\".\nYour implementation of \"put\" should take time proportional to the height of the tree, h, not the number of elements, n. Ideally you should search the tree only once, but if you find it easier to search twice, you can do that; it will be slower, but it doesn't change the order of growth."], ["linkedhashset : public set, k >() return set, section ~\\ ref, documentation http method return Set, keys order, compareTo method HashSet implementation Set, exercise6 maintain order keys LinkedHashSet implementation, outline keySet, new linkedhashset k return, outline keyset, compareto method hashset implementation set, order hint, documentation http method return set, next chapter, performance core methods, returns linkedhashset public set k keyset set k, exercise6 maintain order keys linkedhashset implementation, finish method, keys order, read tree traversal http", "Finally, you should fill in the body of \"keySet\".  According to the documentation at http://thinkdast.com/mapkeyset, this method should return a \"Set\" that iterates the keys in order; that is, in increasing order according to the \"compareTo\" method.  The \"HashSet\" implementation of \"Set\", which we used in Section~\\ref{exercise6}, doesn't maintain the order of the keys, but the \"LinkedHashSet\" implementation does.  You can read about it at http://thinkdast.com/linkedhashset.\nI've provided an outline of \"keySet\" that creates and returns a \"LinkedHashSet\":\n    public Set<K> keySet() {\n        Set<K> set = new LinkedHashSet<K>();\n        return set;\n    }\nYou should finish off this method so it adds the keys from the tree to \"set\" in ascending order. HINT: you might want to write a helper method; you might want to make it recursive; and you might want to read about in-order tree traversal at http://thinkdast.com/inorder."], ["next chapter, core methods, tests, next chapter, performance core methods, new linkedhashset k return, containskey use findnode private method, keys order, next chapter, performance core methods, returns linkedhashset public set k keyset set k, able compare keys, comparable super k k comparable super k target todo fill, exercise6 maintain order keys linkedhashset implementation, documentation http method return set, mytreemap run, containskey compiler considers", "\\index{in-order}\nWhen you are done, all tests should pass. In the next chapter, I'll go over my solutions and test the performance of the core methods."], ["suppresswarnings ( unchecked ) comparable <?, findnode : private node findnode, throw new illegalargumentexception (), previous exercise, outline MyTreeMap, methods present solution, findNode private Node findNode Object target implementations, null key, comparable super k k comparable super k target actual search node node root, target target, node time loop, target null throw new illegalargumentexception something, current key move", "In the previous exercise I gave you the outline of \"MyTreeMap\" and asked you to fill in the missing methods. Now I'll present a solution, starting with \"findNode\":\nprivate Node findNode(Object target) {\n    // some implementations can handle null as a key, but not this one if (target == null) {\n            throw new IllegalArgumentException();\n    }\n    // something to make the compiler happy\n    @SuppressWarnings(\"unchecked\") Comparable<? super K> k = (Comparable<? super K>) target;\n    // the actual search Node node = root;\n    while (node != null) {\n        int cmp = k.compareTo(node.key);\n        if (cmp < 0) node = node.left;\n        else if (cmp > 0) node = node.right;\n        else return node;\n    }\n    return null;\n}\n\"findNode\" is a private method used by \"containsKey\" and \"get\"; it is not part of the \"Map\" interface. The parameter \"target\" is the key we're looking for. I explained the first part of this method in the previous exercise:\n- In this implementation, \"null\" is not a legal value for a key.\n- Before we can invoke \"compareTo\" on \"target\", we have to typecast it to some kind of \"Comparable\". The \"type wildcard\" used here is as permissive as possible; that is, it works with any type that implements \"Comparable\" and whose \"compareTo\" method accepts \"K\" or any supertype of \"K\".\nAfter all that, the actual search is relatively simple. We initialize a loop variable \"node\" so it refers to the root node. Each time through the loop, we compare the target to \"node.key\". If the target is less than the current key, we move to the left child. If it's greater, we move to the right child. And if it's equal, we return the current node."], ["tree without finding, null , tree, bottom tree, target conclude tree return null, return node return null findnode private method, first part method previous exercise implementation, legal value key invoke compareto target typecast kind comparable type wildcard, child equal return current node, target conclude tree return null, variable node refers, bottom tree", "If we get to the bottom of the tree without finding the target, we conclude that it is not in the tree and return \"null\"."], ["second case checks whether, third case makes, fourth case searches, solution recursive public boolean containsValue Object target return containsValueHelper root target private boolean containsValueHelper Node node Object target node null return false equals, true containsValueHelper target, true containsValueHelper target, true return false containsValue, target value parameter, path tree, third case, searched whole tree return false method visits every node tree, true containsvaluehelper target, another second case", "My solution is recursive:\npublic boolean containsValue(Object target) {\n    return containsValueHelper(root, target);\n}\nprivate boolean containsValueHelper(Node node, Object target) {\n    if (node == null) {\n        return false;\n    }\n    if (equals(target, node.value)) {\n        return true;\n    }\n    if (containsValueHelper(node.left, target)) {\n        return true;\n    }\n    if (containsValueHelper(node.right, target)) {\n        return true;\n    }\n    return false;\n}\n\"containsValue\" takes the target value as a parameter and immediately invokes \"containsValueHelper\", passing the root of the tree as an additional parameter.\nHere's how \"containsValueHelper\" works:\n- The first \"if\" statement checks the base case of the recursion. If \"node\" is \"null\", that means we have recursed to the bottom of the tree without finding the \"target\", so we should return \"false\". Note that this only means that the target did not appear on one path through the tree; it is still possible that it will be found on another.\n- The second case checks whether we've found what we're looking for. If so, we return \"true\". Otherwise, we have to keep going.\n- The third case makes a recursive call to search for \"target\" in the left subtree. If we find it, we can return \"true\" immediately, without searching the right subtree. Otherwise, we keep going.\n- The fourth case searches the right subtree. Again, if we find what we are looking for, we return \"true\". Otherwise, having searched the whole tree, we return \"false\".\nThis method \"visits\" every node in the tree, so it takes time proportional to the number of nodes."], ["value ) size ++ return null, value ) size ++ return null, value ) size ++ return null, tree replaces returns old value, new node, right place previous exercise, starter code public V, K key V value key, tree cmp, null new node key value, want look, key value v, child node otherwise", "The \"put\" method is a little more complicated than \"get\" because it has to deal with two cases: (1) if the given key is already in the tree, it replaces and returns the old value; (2) otherwise it has to add a new node to the tree, in the right place.\nIn the previous exercise, I provided this starter code:\npublic V put(K key, V value) {\n    if (key == null) {\n        throw new IllegalArgumentException();\n    }\n    if (root == null) {\n        root = new Node(key, value);\n        size++;\n        return null;\n    }\n    return putHelper(root, key, value);\n}\nAnd asked you to fill in \"putHelper\". Here's my solution:\nprivate V putHelper(Node node, K key, V value) {\n    Comparable<? super K> k = (Comparable<? super K>) key;\n    int cmp = k.compareTo(node.key);\n    if (cmp < 0) {\n        if (node.left == null) {\n            node.left = new Node(key, value);\n            size++;\n            return null;\n        } else {\n            return putHelper(node.left, key, value);\n        }\n    }\n    if (cmp > 0) {\n        if (node.right == null) {\n            node.right = new Node(key, value);\n            size++;\n            return null;\n        } else {\n            return putHelper(node.right, key, value);\n        }\n    }\n    V oldValue = node.value;\n    node.value = value;\n    return oldValue;\n}\nThe first parameter, \"node\", is initially the root of the tree, but each time we make a recursive call, it refers to a different subtree.  As in \"get\", we use the \"compareTo\" method to figure out which path to follow through the tree.  If \"cmp < 0\", the key we're adding is less than \"node.key\", so we want to look in the left subtree. There are two cases:\n- If the left subtree is empty, that is, if \"node.left\" is \"null\", we have reached the bottom of the tree without finding \"key\". At this point, we know that \"key\" isn't in the tree, and we know where it should go. So we create a new node and add it as the left child of \"node\".\n- Otherwise we make a recursive call to search the left subtree.\nIf \"cmp > 0\", the key we're adding is greater than \"node.key\", so we want to look in the right subtree. And we handle the same two cases as in the previous branch. Finally, if \"cmp == 0\", we found the key in the tree, so we replace and return the old value."], ["might want, method recursively, wrote, value return, null return, tree cmp, null new node key value, tree replaces returns old value, cases previous branch finally, want look, key value v, subtree cmp, child node otherwise, new node", "I wrote this method recursively to make it more readable, but it would be straightforward to rewrite it iteratively, which you might want to do as an exercise."], ["return without adding anything, method visits every node, set ) return set, last method, write keySet returns Set, order implementations Map keys, keySet particular order, capabilities tree implementation simple efficient sort keys, know elements, addinorder traverse tree, keyset particular order, order implementations map keys, advantage solution public set k keyset set k", "The last method I asked you to write is \"keySet\", which returns a \"Set\" that contains the keys from the tree in ascending order. In other implementations of \"Map\", the keys returned by \"keySet\" are in no particular order, but one of the capabilities of the tree implementation is that it is simple and efficient to sort the keys. So we should take advantage of that.\nHere's my solution:\npublic Set<K> keySet() {\n    Set<K> set = new LinkedHashSet<K>();\n    addInOrder(root, set);\n    return set;\n}\nprivate void addInOrder(Node node, Set<K> set) {\n    if (node == null) return;\n    addInOrder(node.left, set);\n    set.add(node.key);\n    addInOrder(node.right, set);\n}\nIn \"keySet\", we create a \"LinkedHashSet\", which is a \"Set\" implementation that keeps the elements in order (unlike most other \"Set\" implementations). Then we call \"addInOrder\" to traverse the tree.\nThe first parameter, \"node\", is initially the root of the tree, but as you should expect by now, we use it to traverse the tree recursively. \"addInOrder\" performs a classic \"in-order traversal\" of the tree.\nIf \"node\" is \"null\", that means the subtree is empty, so we return without adding anything to \"set\". Otherwise we:\n- Traverse the left subtree in order.\n- Add \"node.key\".\n- Traverse the right subtree in order.\nRemember that the BST property guarantees that all nodes in the left subtree are less than \"node.key\", and all nodes in the right subtree are greater. So we know that \"node.key\" has been added in the correct order.\nApplying the same argument recursively, we know that the elements from the left subtree are in order, as well as the elements from the right subtree.  And the base case is correct: if the subtree is empty, no keys are added.  So we can conclude that this method adds all keys in the correct order.\nBecause this method visits every node in the tree, like \"containsValue\", it takes time proportional to n."], ["randomuuid () tostring () map, universally unique identifier , height 14 would contain 16, MyTreeMap methods, time proportional height, previous exercise, tree full every level tree, maximum number nodes, size mytreemap final, log base, final size mytreemap, binary search trees, new mytreemap string integer int n string", "In \"MyTreeMap\", the methods \"get\" and \"put\" take time proportional to the height of the tree, h. In the previous exercise, we showed that if the tree is full --- if every level of the tree contains the maximum number of nodes --- the height of the tree is proportional to log n.\nAnd I claimed that \"get\" and \"put\" are logarithmic; that is, they take time proportional to log n. But for most applications, there's no guarantee that the tree is full. In general, the shape of the tree depends on the keys and what order they are added.\nTo see how this works out in practice, we'll test our implementation with two sample datasets: a list of random strings and a list of timestamps in increasing order.\nHere's the code that generates random strings:\nMap<String, Integer> map = new MyTreeMap<String, Integer>();\nfor (int i=0; i<n; i++) {\n    String uuid = UUID.randomUUID().toString();\n    map.put(uuid, 0);\n}\n\"UUID\" is a class in the \"java.util\" package that can generate a random \"universally unique identifier\". UUIDs are useful for a variety of applications, but in this example we're taking advantage of an easy way to generate random strings.\nI ran this code with \"n=16384\" and measured the runtime and the height of the final tree. Here's the output:\nTime in milliseconds = 151 Final size of MyTreeMap = 16384 log base 2 of size of MyTreeMap = 14.0 Final height of MyTreeMap = 33\nI included \"log base 2 of size of MyTreeMap\" to see how tall the tree would be if it were full. The result indicates that a full tree with height 14 would contain 16,384 nodes.\nThe actual tree of random strings has height 33, which is substantially more than the theoretical minimum, but not too bad. To find one key in a collection of 16,384, we only have to make 33 comparisons. Compared to a linear search, that's almost 500 times faster.\nThis performance is typical with random strings or other keys that are added in no particular order. The final height of the tree might be 2-3 times the theoretical minimum, but it is still proportional to log n, which is much less than n. In fact, log n grows so slowly as n increases, it can be difficult to distinguish logarithmic time from constant time in practice.\nHowever, binary search trees don't always behave so well. Let's see what happens when we add keys in increasing order. Here's an example that measures timestamps in nanoseconds and uses them as keys:\nMyTreeMap<String, Integer> map = new MyTreeMap<String, Integer>();\nfor (int i=0; i<n; i++) {\n    String timestamp = Long.toString(System.nanoTime());\n    map.put(timestamp, 0);\n}\n\"System.nanoTime\" returns an integer with type \"long\" that indicates elapsed time in nanoseconds. Each time we call it, we get a somewhat bigger number. When we convert these timestamps to strings, they appear in increasing alphabetical order.\nAnd let's see what happens when we run it:\nTime in milliseconds = 1158 Final size of MyTreeMap = 16384 log base 2 of size of MyTreeMap = 14.0 Final height of MyTreeMap = 16384\nThe runtime is more than seven times longer than in the previous case. longer. If you wonder why, take a look at the final height of the tree:\n16384!\nIf you think about how \"put\" works, you can figure out what's going on. Every time we add a new key, it's larger than all of the keys in the tree, so we always choose the right subtree, and always add the new node as the right child of the rightmost node. The result is an \"unbalanced\" tree that only contains right children.\nThe height of this tree is proportional to n, not log n, so the performance of \"get\" and \"put\" is linear, not logarithmic.\n  In the balanced tree, the height is 4 and the total number of nodes is 2^4-1 = 15.  In the unbalanced tree with the same number of nodes, the height is 15."], ["avl tree ( avl, could avoid adding keys, , possible solutions problem, keys Map order, keys happen order second solution, several ways common modify, become unbalanced rearranges nodes Trees capability, read trees, order keeps, avl tree avl initials inventors, example code, random strings timestamps fact timestamps", "There are two possible solutions to this problem:\n- You could avoid adding keys to the \"Map\" in order. But this is not always possible.\n- You could make a tree that does a better job of handling keys if they happen to be in order.\nThe second solution is better, and there are several ways to do it. The most common is to modify \"put\" so that it detects when the tree is starting to become unbalanced and, if so, rearranges the nodes. Trees with this capability are called \"self-balancing\". Common self-balancing trees include the AVL tree (\"AVL\" are the initials of the inventors), and the red-black tree, which is what the Java \"TreeMap\" uses.\nIn our example code, if we replace \"MyTreeMap\" with the Java \"TreeMap\", the runtimes are about the same for the random strings and the timestamps. In fact, the timestamps run faster, even though they are in order, probably because they take less time to hash.\nIn summary, a binary search tree can implement \"get\" and \"put\" in logarithmic time, but only if the keys are added in an order that keeps the tree sufficiently balanced. Self-balancing trees avoid this problem by doing some additional work each time a new key is added.\nYou can read more about self-balancing trees at http://thinkdast.com/balancing."], [" , balancing trees work, similar operations, previous exercise implement remove, node middle tree rearrange, BST property, figure read explanation, tree similar operations, tree similar operations, bst property, previous exercise implement remove, node middle tree rearrange, figure read explanation, idea trees", "In the previous exercise you didn't have to implement \"remove\", but you might want to try it. If you remove a node from the middle of the tree, you have to rearrange the remaining nodes to restore the BST property.  You can probably figure out how to do that on your own, or you can read the explanation at http://thinkdast.com/bstdel.\nRemoving a node and rebalancing a tree are similar operations: if you do this exercise, you will have a better idea of how self-balancing trees work."], ["two data structures, web page, search term, previous version indexer stores index, data structures TermCounter maps search term number times, web page Index, search term, previous version indexer stores index, data structures termcounter maps search term number times, data structures, web page index, search term", "The previous version of the indexer stores the index in two data structures: a \"TermCounter\" that maps from a search term to the number of times it appears on a web page, and an \"Index\" that maps from a search term to the set of pages where it appears."], ["entire data structure might, program last started would, writing large data structures, data structures, Java program, program stops, volatile vaporizes program, Data persists program, data structures, data structures contains values, lists strings similar java list hashes similar java map sets, file system, example due power outage changes", "These data structures are stored in the memory of a running Java program, which means that when the program stops running, the index is lost. Data stored only in the memory of a running program is called \"volatile\", because it vaporizes when the program ends.\nData that persists after the program that created it ends is called \"persistent\". In general, files stored in a file system are persistent, as well as data stored in databases.\nA simple way to make data persistent is to store it in a file. Before the program ends, it could translate its data structures into a format like JSON (http://thinkdast.com/json) and then write them into a file. When it starts again, it could read the file and rebuild the data structures.\nBut there are several problems with this solution:\n- Reading and writing large data structures (like a Web index) would be slow.\n- The entire data structure might not fit into the memory of a single running program.\n- If a program ends unexpectedly (for example, due to a power outage), any changes made since the program last started would be lost.\nA better alternative is a database that provides persistent storage and the ability to read and write parts of the database without reading and writing the whole thing.\nThere are many kinds of database management systems (DBMS) that provide different capabilities. You can read an overview at http://thinkdast.com/database.\nThe database I recommend for this exercise is Redis, which provides persistent data structures that are similar to Java data structures. Specifically, it provides:\n- Lists of strings, similar to Java \"List\".\n- Hashes, similar to Java \"Map\".\n- Sets of strings, similar to Java \"Set\".\nRedis is a \"key-value database\", which means that the data structures it contains (the values) are identified by unique strings (the keys). A key in Redis plays the same role as a reference in Java: it identifies an object. We'll see some examples soon."], ["remote dictionary server , virtual machine running, , remote service fact name, REmote DIctionary Server use Redis run Redis server, Redis client many ways, many clients, exercise recommend Rather install, create instance virtual machine, free plan, redis detailed instructions, configuration page make note url, classes methods", "Redis is usually run as a remote service; in fact, the name stands for \"REmote DIctionary Server\". To use Redis, you have to run the Redis server somewhere and then connect to it using a Redis client. There are many ways to set up a server and many clients you could use. For this exercise, I recommend:\n- Rather than install and run the server yourself, consider using a service like RedisToGo (http://thinkdast.com/redistogo), which runs Redis in the cloud. They offer a free plan with enough resources for the exercise.\n- For the client I recommend Jedis, which is a Java library that provides classes and methods for working with Redis.\nHere are more detailed instructions to help you get started:\n- Create an account on RedisToGo, at http://thinkdast.com/redissign, and select the plan you want (probably the free plan to get started).\n- Create an \"instance\", which is a virtual machine running the Redis server. If you click on the \"Instances\" tab, you should see your new instance, identified by a host name and a port number. For example, I have an instance named \"dory-10534\".\n- Click on the instance name to get the configuration page. Make a note of the URL near the top of the page, which looks like this:\n  \\begin{verbatim}\n  redis://redistogo:1234567feedfacebeefa1e1234567@dory.redistogo.com:10534\n\\end{itemize}\nThis URL contains the server's host name, \"dory.redistogo.com\", the port number, \"10534\", and the password you will need to connect to the server, which is the long string of letters and numbers in the middle. You will need this information for the next step."], ["lindex ( mylist , 1 )) // hash jedis, main : public static void main, value ) // set jedis, repository book, source files, example code, Redis server, Jedis methods, creates jedis, password redis server, impossible put file repo, previous exercises implements index, ant build compile source files", "In the repository for this book, you'll find the source files for this exercise:\n- \"JedisMaker.java\" contains example code for connecting to a Redis server and running a few Jedis methods.\n- \"JedisIndex.java\" contains starter code for this exercise.\n- \"JedisIndexTest.java\" contains test code for \"JedisIndex\".\n- \"WikiFetcher.java\" contains the code we saw in previous exercises to read web pages and parse them using jsoup.\nYou will also need these files, which you worked on in previous exercises:\n- \"Index.java\" implements an index using Java data structures.\n- \"TermCounter.java\" represents a map from terms to their frequencies.\n- \"WikiNodeIterable.java\" iterates through the nodes in a DOM tree produced by jsoup.\nIf you have working versions of these files, you can use them for this exercise.  If you didn't do the previous exercises, or you are not confident in your solutions, you can copy my solutions from the solutions folder.\nThe first step is to use Jedis to connect to your Redis server. \"RedisMaker.java\" shows how to do this. It reads information about your Redis server from a file, connects to it and logs in using your password, then returns a \"Jedis\" object you can use to perform Redis operations.\nIf you open \"JedisMaker.java\", you should see the \"JedisMaker\" class, which is a helper class that provides one static method, \"make\", which creates a \"Jedis\" object. Once this object is authenticated, you can use it to communicate with your Redis database.\n\"JedisMaker\" reads information about your Redis server from a file named \"redis_url.txt\", which you should put in the directory \"src/resources\":\n- Use a text editor to create end edit \"ThinkDataStructures/code/src/resources/redis_url.txt\".\n- Paste in the URL of your server. If you are using RedisToGo, the URL will look like this:\n\"redis://redistogo:1234567feedfacebeefa1e1234567@dory.redistogo.com:10534\"\nBecause this file contains the password for your Redis server, you should not put this file in a public repository. To help you avoid doing that by accident, the repository contains a .gitignore file that makes it harder (but not impossible) to put this file in your repo.\nNow run \"ant build\" to compile the source files and \"ant JedisMaker\" to run the example code in \"main\":\n    public static void main(String[] args) {\n        Jedis jedis = make();\n        // String jedis.set(\"mykey\", \"myvalue\");\n        String value = jedis.get(\"mykey\");\n        System.out.println(\"Got value: \" + value);\n        // Set jedis.sadd(\"myset\", \"element1\", \"element2\", \"element3\");\n        System.out.println(\"element2 is member: \" +\n                           jedis.sismember(\"myset\", \"element2\"));\n        // List jedis.rpush(\"mylist\", \"element1\", \"element2\", \"element3\");\n        System.out.println(\"element at index 1: \" +\n                           jedis.lindex(\"mylist\", 1));\n        // Hash jedis.hset(\"myhash\", \"word1\", Integer.toString(2));\n        jedis.hincrBy(\"myhash\", \"word2\", 1);\n        System.out.println(\"frequency of word1: \" +\n                           jedis.hget(\"myhash\", \"word1\"));\n        System.out.println(\"frequency of word1: \" +\n                            jedis.hget(\"myhash\", \"word2\"));\n        jedis.close();\n    }\nThis example demonstrates the data types and methods you are most likely to use for this exercise. When you run it, the output should be:\nGot value: myvalue element2 is member: true element at index 1: element2 frequency of word1: 2 frequency of word2: 1\nIn the next section, I'll explain how the code works."], ["lindex ( mylist , 1 ), might help keep things straight, element3 ) boolean flag, keys strings values, several data types basic Redis data type string write Redis types italics distinguish Java types, database use similar parameters, value look, value use mykey myvalue, myhash identifies particular hash field, elements example, value look, value mykey example key mykey value myvalue redis, membership constant time operations redis", "Redis is basically a map from keys, which are strings, to values, which can be one of several data types. The most basic Redis data type is a \\redis{string}.  I will write Redis types in italics to distinguish them from Java types.\nTo add a \\redis{string} to the database, use \"jedis.set\", which is similar to \"Map.put\"; the parameters are the new key and the corresponding value. To look up a key and get its value, use \"jedis.get\":\n        jedis.set(\"mykey\", \"myvalue\");\n        String value = jedis.get(\"mykey\");\nIn this example, the key is \"\"mykey\"\" and the value is \"\"myvalue\"\".\nRedis provides a \\redis{set} structure, which is similar to a Java \"Set<String>\". To add elements to a Redis \\redis{set}, you choose a key to identify the \\redis{set} and then use \"jedis.sadd\":\n        jedis.sadd(\"myset\", \"element1\", \"element2\", \"element3\");\n        boolean flag = jedis.sismember(\"myset\", \"element2\");\nYou don't have to create the \\redis{set} as a separate step. If it doesn't exist, Redis creates it. In this case, it creates a \\redis{set} named \"myset\" that contains three elements.\nThe method \"jedis.sismember\" checks whether an element is in a\n\\redis{set}. Adding elements and checking membership are constant time operations.\nRedis also provides a \\redis{list} structure, which is similar to a Java \"List<String>\". The method \"jedis.rpush\" adds elements to the end (right side) of a\n\\redis{list}:\n        jedis.rpush(\"mylist\", \"element1\", \"element2\", \"element3\");\n        String element = jedis.lindex(\"mylist\", 1);\nAgain, you don't have to create the structure before you start adding elements. This example creates a \\redis{list} named \"mylist\" that contains three elements.\nThe method \"jedis.lindex\" takes an integer index and returns the indicated element of a \\redis{list}. Adding and accessing elements are constant time operations.\nFinally, Redis provides a \\redis{hash} structure, which is similar to a Java \"Map<String, String>\". The method \"jedis.hset\" adds a new entry to the \\redis{hash}:\n        jedis.hset(\"myhash\", \"word1\", Integer.toString(2));\n        String value = jedis.hget(\"myhash\", \"word1\");\nThis example creates a \\redis{hash} named \"myhash\" that contains one entry, which maps from the key \"word1\" to the value \"\"2\"\".\nThe keys and values are \\redis{string}s, so if we want to store an \"Integer\", we have to convert it to a \"String\" before we call \"hset\". And when we look up the value using \"hget\", the result is a \"String\", so we might have to convert it back to \"Integer\".\nWorking with Redis \\redis{hash}es can be confusing, because we use a key to identify which \\redis{hash} we want, and then another key to identify a value in the \\redis{hash}. In the context of Redis, the second key is called a \"field\", which might help keep things straight. So a \"key\" like \"myhash\" identifies a particular \\redis{hash}, and then a \"field\" like \"word1\" identifies a value in the \\redis{hash}.\nFor many applications, the values in a Redis \\redis{hash} are integers, so Redis provides a few special methods, like \"hincrby\", that treat the values as numbers:\n        jedis.hincrBy(\"myhash\", \"word2\", 1);\nThis method accesses \"myhash\", gets the current value associated with \"word2\" (or 0 if it doesn't already exist), increments it by 1, and writes the result back to the \\redis{hash}.\nSetting, getting, and incrementing entries in a \\redis{hash} are constant time operations.\nYou can read more about Redis data types at http://thinkdast.com/redistypes."], ["deleteallkeys : public void deleteallkeys (), public void deleteallkeys (), database might change every time, point information, exercise get, ready suggestions, guidance previous exercises, design decisions particular figure divide problem pieces, long time, key keys key time, persistent data persistent structures, public void deleteallkeys set string keys transaction string key keys key returns transaction, series operations transaction example simple version deleteallkeys public void deleteallkeys set string keys", "At this point you have all the information you need to do the exercise, so you can get started if you are ready. But I have a few suggestions you might want to read first:\n- For this exercise I provide less guidance than in previous exercises.  You will have to make some design decisions; in particular, you will have to figure out how to divide the problem into pieces that you can test one at a time, and then assemble the pieces into a complete solution. If you try to write the whole thing at once, without testing smaller pieces, it might take a very long time to debug.\n- One of the challenges of working with persistent data is that it is persistent. The structures stored in the database might change every time you run the program. If you mess something up in the database, you will have to fix it or start over before you can proceed. To help you keep things under control, I've provided methods called \"deleteURLSets\", \"deleteTermCounters\", and \"deleteAllKeys\", which you can use to clean out the database and start fresh. You can also use \"printIndex\" to print the contents of the index.\n- Each time you invoke a \"Jedis\" method, your client sends a message to the server, then the server performs the action you requested and sends back a message. If you perform many small operations, it will probably take a long time. You can improve performance by grouping a series of operations into a \"Transaction\".\nFor example, here's a simple version of \"deleteAllKeys\":\n    public void deleteAllKeys() {\n        Set<String> keys = jedis.keys(\"*\");\n        for (String key: keys) {\n            jedis.del(key);\n        }\n    }\nEach time you invoke \"del\" requires a round-trip from the client to the server and back. If the index contains more than a few pages, this method would take a long time to run. We can speed it up with a \"Transaction\" object:\n    public void deleteAllKeys() {\n        Set<String> keys = jedis.keys(\"*\");\n        Transaction t = jedis.multi();\n        for (String key: keys) {\n            t.del(key);\n        }\n        t.exec();\n    }\n\"jedis.multi\" returns a \"Transaction\" object, which provides all the methods of a \"Jedis\" object. But when you invoke a method on a \"Transaction\", it doesn't run the operation immediately, and it doesn't communicate with the server. It saves up a batch of operations until you invoke \"exec\". Then it sends all of the saved operations to the server at the same time, which is usually much faster."], ["start working, get stuck, get started, work adds url, associated term public void, urls public set string geturls, term termcounter tc looks search term returns, url public integer getcount string url string term pushes contents termcounter redis public list object pushtermcountertoredis termcounter tc methods, tests first figure test method, good luck, term returns number times, way divide things, basic redis commands", "Now you really have all the information you need; you should start working on the exercise. But if you get stuck, or if you really don't know how to get started, you can come back for a few more hints."], ["basic redis commands, test code, java } , basic Redis commands, work adds url, associated term public void, basic redis commands, urls public set string geturls, url public integer getcount string url string term pushes contents termcounter redis public list object pushtermcountertoredis termcounter tc methods, term returns number times, way divide things, good luck, tests first figure test method, term termcounter tc looks search term returns", "\\textbf{Don't read the following until you have run the test code, tried out some basic Redis commands, and written a few methods in \"JedisIndex.java\"}."], [" public void add, public integer getcount, public list, work Adds URL, associated term public void, term TermCounter tc Looks search term returns, URLs public Set String getURLs, term Returns number times, work adds url, associated term public void, basic redis commands, urls public set string geturls, url public integer getcount string url string term pushes contents termcounter redis public list object pushtermcountertoredis termcounter tc methods, term returns number times, way divide things, tests first figure test method, good luck, term termcounter tc looks search term returns", "OK, if you are really stuck, here are some methods you might want to work on:\n    /** * Adds a URL to the set associated with term. */\n    public void add(String term, TermCounter tc) {}\n    /** * Looks up a search term and returns a set of URLs. */\n    public Set<String> getURLs(String term) {}\n    /** * Returns the number of times the given term appears at the given URL. */\n    public Integer getCount(String url, String term) {}\n    /** * Pushes the contents of the TermCounter to Redis. */\n    public List<Object> pushTermCounterToRedis(TermCounter tc) {}\nThese are the methods I used in my solution, but they are certainly not the only way to divide things up. So please take these suggestions if they help, but ignore them if they don't."], ["often get ideas, tests first, consider writing, tests first figure test method, work adds url, associated term public void, basic redis commands, urls public set string geturls, tests first figure test method, good luck, term termcounter tc looks search term returns", "For each method, consider writing the tests first. When you figure out how to test a method, you often get ideas about how to write it."], ["good luck, Good luck, work adds url, urls public set string geturls, term termcounter tc looks search term returns, tests first figure test method, good luck, basic redis commands", "Good luck!"], ["method uses two helper methods, urlset : private string urlsetkey, termcounter : private string termcounterkey, solution store, kinds structures Redis, term URLSet Redis, URLs contain search term URL TermCounter Redis hash maps, term number times, redis structures, current url page, term map string integer, index methods, term urlset redis", "In my solution, we store two kinds of structures in Redis:\n- For each search term, we have a \"URLSet\", which is a Redis \\redis{set}\n  of URLs that contain the search term.\n- For each URL, we have a \"TermCounter\", which is a Redis \\redis{hash}\n  that maps each search term to the number of times it appears.\nWe discussed these data types in the previous chapter. You can also read about Redis structures at http://thinkdast.com/redistypes\nIn \"JedisIndex\", I provide a method that takes a search term and returns the Redis key of its \"URLSet\":\nprivate String urlSetKey(String term) {\n    return \"URLSet:\" + term;\n}\nAnd a method that takes a URL and returns the Redis key of its \"TermCounter\":\nprivate String termCounterKey(String url) {\n    return \"TermCounter:\" + url;\n}\nHere's the implementation of \"indexPage\", which takes a URL and a jsoup \"Elements\" object that contains the DOM tree of the paragraphs we want to index:\npublic void indexPage(String url, Elements paragraphs) {\n    System.out.println(\"Indexing \" + url);\n    // make a TermCounter and count the terms in the paragraphs TermCounter tc = new TermCounter(url);\n    tc.processElements(paragraphs);\n    // push the contents of the TermCounter to Redis pushTermCounterToRedis(tc);\n}\nTo index a page, we\n- Make a Java \"TermCounter\" for the contents of the page, using code from a previous exercise.\n- Push the contents of the \"TermCounter\" to Redis.\nHere's the new code that pushes a \"TermCounter\" to Redis:\npublic List<Object> pushTermCounterToRedis(TermCounter tc) {\n    Transaction t = jedis.multi();\n    String url = tc.getLabel();\n    String hashname = termCounterKey(url);\n    // if this page has already been indexed, delete the old hash t.del(hashname);\n    // for each term, add an entry in the TermCounter and a new\n    // member of the index for (String term: tc.keySet()) {\n        Integer count = tc.get(term);\n        t.hset(hashname, term, count.toString());\n        t.sadd(urlSetKey(term), url);\n    }\n    List<Object> res = t.exec();\n    return res;\n}\nThis method uses a \"Transaction\" to collect the operations and send them to the server all at once, which is much faster than sending a series of small operations.\nIt loops through the terms in the \"TermCounter\". For each one it\n- Finds or creates a \"TermCounter\" on Redis, then adds a field for the new term.\n- Finds or creates a \"URLSet\" on Redis, then adds the current URL.\nIf the page has already been indexed, we delete its old \"TermCounter\" before pushing the new contents.\nThat's it for indexing new pages.\nThe second part of the exercise asked you to write \"getCounts\", which takes a search term and returns a map from each URL where the term appears to the number of times it appears there. Here is my solution:\n    public Map<String, Integer> getCounts(String term) {\n        Map<String, Integer> map = new HashMap<String, Integer>();\n        Set<String> urls = getURLs(term);\n        for (String url: urls) {\n            Integer count = getCount(url, term);\n            map.put(url, count);\n        }\n        return map;\n    }\nThis method uses two helper methods:\n- \"getURLs\" takes a search term and returns the Set of URLs where the term appears.\n- \"getCount\" takes a URL and a term and returns the number of times the term appears at the given URL.\nHere are the implementations:\n    public Set<String> getURLs(String term) {\n        Set<String> set = jedis.smembers(urlSetKey(term));\n        return set;\n    }\n    public Integer getCount(String url, String term) {\n        String redisKey = termCounterKey(url);\n        String count = jedis.hget(redisKey, term);\n        return new Integer(count);\n    }\nBecause of the way we designed the index, these methods are simple and efficient."], ["takes time proportional, indexed n pages, unique search terms, Suppose indexed N pages, unique search terms, search term Think answer, search term run getCounts, map Runs getURLs get Set URLs URL Set, termcounter redis, many small operations redis, practice runtime proportional number pages, map runs geturls get set urls url set, large n, search term run getcounts, search term rare terms, search term think answer, suppose indexed n pages, small number common terms, entry hashmap geturls, loop run getcount, entry hashmap constant time operations overall complexity getcounts n, time proportional number urls, unique search terms", "Suppose we have indexed N pages and discovered M unique search terms. How long will it take to look up a search term? Think about your answer before you continue.\nTo look up a search term, we run \"getCounts\", which\n- Creates a map.\n- Runs \"getURLs\" to get a Set of URLs.\n- For each URL in the Set, runs \"getCount\" and adds an entry to a \"HashMap\".\n\"getURLs\" takes time proportional to the number of URLs that contain the search term. For rare terms, that might be a small number, but for common terms it might be as large as N."], ["sends many small operations, constant time operations, normally much less, loop run getCount, TermCounter Redis, entry HashMap constant time operations overall complexity getCounts N, practice runtime proportional number pages, algorithm efficient terms, termcounter redis, many small operations redis, practice runtime proportional number pages, map runs geturls get set urls url set, algorithm efficient terms, loop run getcount, entry hashmap constant time operations overall complexity getcounts n", "Inside the loop, we run \"getCount\", which finds a \"TermCounter\" on Redis, looks up a term, and adds an entry to a HashMap. Those are all constant time operations, so the overall complexity of \"getCounts\" is O(N) in the worst case. However, in practice the runtime is proportional to the number of pages that contain the term, which is normally much less than N.\nThis algorithm is as efficient as it can be, in terms of algorithmic complexity, but it is very slow because it sends many small operations to Redis. You can make it faster using a \"Transaction\". You might want to do that as an exercise, or you can see my solution in \"RedisIndex.java\"."], ["help identify relevant pages, probably avoid indexing, constant time operations, data structures, index page, index page traverse DOM tree, TextNode objects, strings search terms, theory page, common words first, termcounter linear number unique terms term add element urlset add element redis termcounter constant time operations total time push termcounter linear number unique search terms, termcounter proportional number words, case practice analysis", "Using the data structures we designed, how long will it take to index a page? Again, think about your answer before you continue.\nTo index a page, we traverse its DOM tree, find all the \"TextNode\" objects, and split up the strings into search terms. That all takes time proportional to the number of words on the page.\nFor each term, we increment a counter in a HashMap, which is a constant time operation. So making the \"TermCounter\" takes time proportional to the number of words on the page.\nPushing the \"TermCounter\" to Redis requires deleting a \"TermCounter\", which is linear in the number of unique terms. Then for each term we have to\n- Add an element to a \"URLSet\", and\n- Add an element to a Redis \"TermCounter\".\nBoth of these are constant time operations, so the total time to push the \"TermCounter\" is linear in the number of unique search terms.\nIn summary, making the \"TermCounter\" is proportional to the number of words on the page. Pushing the \"TermCounter\" to Redis is proportional to the number of unique terms.\nSince the number of words on the page usually exceeds the number of unique search terms, the overall complexity is proportional to the number of words on the page. In theory a page might contain all search terms in the index, so the worst case performance is O(M), but we don't expect to see the worse case in practice.\nThis analysis suggests a way to improve performance: we should probably avoid indexing very common words. First of all, they take up a lot of time and space, because they appear in almost every \"URLSet\" and \"TermCounter\". Furthermore, they are not very useful because they don't help identify relevant pages."], ["search engines avoid indexing common words, , stop words, search engines, common words, context stop words http, termcounter redis, search terms index, search engines, relevant pages, case performance, textnode objects, common words, context stop words http", "Most search engines avoid indexing common words, which are known in this context as stop words (http://thinkdast.com/stopword)."], [" , , give higher priority, Philosophy exercise getphilo, program reads Wikipedia page, first link, load next page, kind crawler people, node another familiar graphs, mean program loads, web crawler, priority pages, kind crawler people", "If you did the \"Getting to Philosophy\" exercise in Chapter~\\ref{getphilo}, you already have a program that reads a Wikipedia page, finds the first link, uses the link to load the next page, and repeats. This program is a specialized kind of crawler, but when people say \"Web crawler\" they usually mean a program that\n- Loads a starting page and indexes the contents,\n- Finds all the links on the page and adds the linked URLs to a collection, and\n- Works its way through the collection, loading pages, indexing them, and adding new URLs.\n- If it finds a URL that has already been indexed, it skips it.\nYou can think of the Web as a graph where each page is a node and each link is a directed edge from one node to another. If you are not familiar with graphs, you can read about them at http://thinkdast.com/graph.\nStarting from a source node, a crawler traverses this graph, visiting each reachable node once.\nThe collection we use to store the URLs determines what kind of traversal the crawler performs:\n- If it's a first-in-first-out (FIFO) queue, the crawler performs a breadth-first traversal.\n- If it's a last-in-first-out (LIFO) stack, the crawler performs a depth-first traversal.\n- More generally, the items in the collection might be prioritized. For example, we might want to give higher priority to pages that have not been indexed for a long time.\nYou can read more about graph traversal at http://thinkdast.com/graphtrav."], ["string >() // fetcher used, wikipedia final static wikifetcher wf, started private final string source, solution previous exercise, outline WikiCrawler job fill, reminder fields WikiCrawler class public class WikiCrawler, started private final String source index results, private JedisIndex index, element source notice implementation queue linkedlist add elements, first version loops, implementation public string crawl boolean testing, constant time, second version real work", "First, let's go over our solution to the previous exercise. I provided an outline of \"WikiCrawler\"; your job was to fill in \"crawl\". As a reminder, here are the fields in the \"WikiCrawler\" class:\npublic class WikiCrawler {\n    // keeps track of where we started private final String source;\n    // the index where the results go private JedisIndex index;\n    // queue of URLs to be indexed private Queue<String> queue = new LinkedList<String>();\n    // fetcher used to get pages from Wikipedia final static WikiFetcher wf = new WikiFetcher();\n}\nWhen we create a \"WikiCrawler\", we provide \"source\" and \"index\". Initially, \"queue\" contains only one element, \"source\".\nNotice that the implementation of \"queue\" is a \"LinkedList\", so we can add elements at the end --- and remove them from the beginning --- in constant time. By assigning a \"LinkedList\" object to a \"Queue\" variable, we limit ourselves to using methods in the \"Queue\" interface; specifically, we'll use \"offer\" to add elements and \"poll\" to remove them.\nHere's my implementation of \"WikiCrawler.crawl\":\n    public String crawl(boolean testing) throws IOException {\n        if (queue.isEmpty()) {\n            return null;\n        }\n        String url = queue.poll();\n        System.out.println(\"Crawling \" + url);\n        if (testing==false && index.isIndexed(url)) {\n            System.out.println(\"Already indexed.\");\n            return null;\n        }\n        Elements paragraphs;\n        if (testing) {\n            paragraphs = wf.readWikipedia(url);\n        } else {\n            paragraphs = wf.fetchWikipedia(url);\n        }\n        index.indexPage(url, paragraphs);\n        queueInternalLinks(paragraphs);\n        return url;\n    }\nMost of the complexity in this method is there to make it easier to test. Here's the logic:\n- If the queue is empty, it returns \"null\" to indicate that it did not index a page.\n- Otherwise it removes and stores the next URL from the queue.\n- If the URL has already been indexed, \"crawl\" doesn't index it again, unless it's in testing mode.\n- Next it reads the contents of the page: if it's in testing mode, it reads from a file; otherwise it reads from the Web.\n- It indexes the page.\n- It parses the page and adds internal links to the queue.\n- Finally, it returns the URL of the page it indexed.\nI presented an implementation of \"Index.indexPage\" in Section~\\ref{redis-indexer}. So the only new method is \"WikiCrawler.queueInternalLinks\"."], ["object containing one dom tree per paragraph, wrote two versions, single paragraph, different parameters, DOM tree, Element object, single paragraph, first version loops, second version real work, solution previous exercise, single paragraph, dom tree, different parameters, linkedlist object queue variable limit, mode reads, crawl index, element object, private jedisindex index", "I wrote two versions of this method with different parameters: one takes an \"Elements\" object containing one DOM tree per paragraph; the other takes an \"Element\" object that contains a single paragraph."], ["second version, real work, first version, first version loops, second version real work, element source notice implementation queue linkedlist add elements, first version loops, implementation public string crawl boolean testing, constant time, second version real work, solution previous exercise, single paragraph, dom tree, different parameters, linkedlist object queue variable limit, mode reads, crawl index, private jedisindex index", "The first version just loops through the paragraphs. The second version does the real work."], ["startswith (/ wiki /)), attr ( href ), href ) queue, void queueInternalLinks Elements, Element paragraph paragraphs queueInternalLinks, private void queueInternalLinks Element paragraph Elements, href Element elt elts, relURL href String absURL abs, first version loops, private void queueinternallinks element paragraph elements, link internal check, second version real work, solution previous exercise, single paragraph, dom tree, different parameters, linkedlist object queue variable limit, crawl index, mode reads, href element elt elts, url starts, void queueinternallinks elements, private jedisindex index", "    void queueInternalLinks(Elements paragraphs) {\n        for (Element paragraph: paragraphs) {\n            queueInternalLinks(paragraph);\n        }\n    }\n    private void queueInternalLinks(Element paragraph) {\n        Elements elts = paragraph.select(\"a[href]\");\n        for (Element elt: elts) {\n            String relURL = elt.attr(\"href\");\n            if (relURL.startsWith(\"/wiki/\")) {\n                String absURL = elt.attr(\"abs:href\");\n                queue.offer(absURL);\n            }\n        }\n    }\nTo determine whether a link is \"internal,\" we check whether the URL starts with \"/wiki/\". This might include some pages we don't want to index, like meta-pages about Wikipedia. And it might exclude some pages we want, like links to pages in non-English languages. But this simple test is good enough to get started."], ["might consider working, information retrieval ,, , next phase project implement search tool pieces, interface users, search terms, results lookup mechanism, search term returns pages, search results multiple search terms algorithms, search terms, http exercise focus steps, results lookup mechanism, information retrieval, interface users, next phase project implement search tool pieces, search results general term, simple version, interested building web applications", "The next phase of this project is to implement a search tool. The pieces we'll need include:\n- An interface where users can provide search terms and view results.\n- A lookup mechanism that takes each search term and returns the pages that contain it.\n- Mechanisms for combining search results from multiple search terms.\n- Algorithms for ranking and sorting search results.\nThe general term for processes like this is \"information retrieval\", which you can read more about at http://thinkdast.com/infret.\nIn this exercise, we'll focus on steps 3 and 4. We've already built a simple version of 2. If you are interested in building Web applications, you might consider working on step 1."], ["multiple search terms using boolean logic, boolean searches ,, contain search terms, search engines, boolean searches, combine results multiple search terms, boolean logic example search java programming, search terms, java contain indonesia expressions, either term, search terms operators, java programming, operations intersection union difference example suppose s1", "Most search engines can perform \"boolean searches\", which means you can combine the results from multiple search terms using boolean logic. For example:\n- The search \"java AND programming\" might return only pages that contain both search terms: \"java\" and \"programming\".\n- \"java OR programming\" might return pages that contain either term but not necessarily both.\n- \"java -indonesia\" might return pages that contain \"java\" and do not contain \"indonesia\".\nExpressions like these that contain search terms and operators are called \"queries\".\nWhen applied to search results, the boolean operators \"AND\", \"OR\", and \"-\" correspond to the set operations \"intersection\", \"union\", and \"difference\". For example, suppose\n- \"s1\" is the set of pages containing \"java\",\n- \"s2\" is the set of pages containing \"programming\", and\n- \"s3\" is the set of pages containing \"indonesia\".\nIn that case:\n- The intersection of \"s1\" and \"s2\" is the set of pages containing \"java\" AND \"programming\".\n- The union of \"s1\" and \"s2\" is the set of pages containing \"java\" OR \"programming\".\n- The difference of \"s1\" and \"s2\" is the set of pages containing \"java\" and not \"indonesia\".\nIn the next section you will write a method to implement these operations."], ["compareto : public int compareto, override public int compare, public class card implements comparable, repository book, list Card, class definition public class Card, Comparable Card private final int rank private final int suit public Card int rank int suit rank suit Card, integer fields, comparable card, int rank1 getrankacehigh card1 int rank2 getrankacehigh card2 rank1, http using comparator, experiment exercise, code section", "The repository for this book includes \"Card.java\", which demonstrates two ways to sort a list of \"Card\" objects. Here's the beginning of the class definition:\npublic class Card implements Comparable<Card> {\n    private final int rank;\n    private final int suit;\n    public Card(int rank, int suit) {\n        this.rank = rank;\n        this.suit = suit;\n    }\nA \"Card\" object has two integer fields, \"rank\" and \"suit\". \"Card\" implements \"Comparable<Card>\", which means that it provides \"compareTo\":\n    public int compareTo(Card that) {\n        if (this.suit < that.suit) {\n            return -1;\n        }\n        if (this.suit > that.suit) {\n            return 1;\n        }\n        if (this.rank < that.rank) {\n            return -1;\n        }\n        if (this.rank > that.rank) {\n            return 1;\n        }\n        return 0;\n    }\nThe specification of \"compareTo\" indicates that it should return a negative number if \"this\" is considered less than \"that\", a positive number if it is considered greater, and 0 if they are considered equal.\nIf you use the one-parameter version of \"Collections.sort\", it uses the \"compareTo\" method provided by the elements to sort them. To demonstrate, we can make a list of 52 cards like this:\n    public static List<Card> makeDeck() {\n        List<Card> cards = new ArrayList<Card>();\n        for (int suit = 0; suit <= 3; suit++) {\n            for (int rank = 1; rank <= 13; rank++) {\n                Card card = new Card(rank, suit);\n                cards.add(card);\n            }\n        }\n        return cards;\n    }\nAnd sort them like this:\n        Collections.sort(cards);\nThis version of \"sort\" puts the elements in what's called their \"natural order\" because it's determined by the objects themselves.\nBut it is possible to impose a different ordering by providing a \"Comparator\" object. For example, the natural order of \"Card\" objects treats Aces as the lowest rank, but in some card games they have the highest rank. We can define a \"Comparator\" that considers \"Aces high\", like this:\n        Comparator<Card> comparator = new Comparator<Card>() {\n            @Override public int compare(Card card1, Card card2) {\n                if (card1.getSuit() < card2.getSuit()) {\n                    return -1;\n                }\n                if (card1.getSuit() > card2.getSuit()) {\n                    return 1;\n                }\n                int rank1 = getRankAceHigh(card1);\n                int rank2 = getRankAceHigh(card2);\n                if (rank1 < rank2) {\n                    return -1;\n                }\n                if (rank1 > rank2) {\n                    return 1;\n                }\n                return 0;\n            }\n            private int getRankAceHigh(Card card) {\n                int rank = card.getRank();\n                if (rank == 1) {\n                    return 14;\n                } else {\n                    return rank;\n                }\n            }\n        };\nThis code defines an anonymous class that implements \"compare\", as required. Then it creates an instance of the newly-defined, unnamed class. If you are not familiar with anonymous classes in Java, you can read about them at http://thinkdast.com/anonclass.\nUsing this \"Comparator\", we can invoke \"sort\" like this:\n        Collections.sort(cards, comparator);\nIn this ordering, the Ace of Spades is considered the highest class in the deck; the two of Clubs is the lowest.\nThe code in this section is in \"Card.java\" if you want to experiment with it. As an exercise, you might want to write a comparator that sorts by \"rank\" first and then by \"suit\", so all the Aces should be together, and all the twos, etc."], ["deploying web applications using java, see , , basic version exercise, work optional exercises Read, document frequencies total number times term, pages index, search term total relevance page, search terms, results sort relevance display urls consider, search term total relevance page, heroku simple option, relevance term think simple version", "If you get a basic version of this exercise working, you might want to work on these optional exercises:\n- Read about TF-IDF at http://thinkdast.com/tfidf and implement it. You might have to modify \"JavaIndex\" to compute document frequencies; that is, the total number of times each term appears on all pages in the index.\n- For queries with more than one search term, the total relevance for each page is currently the sum of the relevance for each term. Think about when this simple version might not work well, and try out some alternatives.\n- Build a user interface that allows users to enter queries with boolean operators. Parse the queries, generate the results, then sort them by relevance and display the highest-scoring URLs. Consider generating \"snippets\" that show where the search terms appeared on the page. If you want to make a Web application for your user interface, consider using Heroku as a simple option for developing and deploying Web applications using Java.  See http://thinkdast.com/heroku."], ["inner loop never runs, insertion sort wikipedia page, override public int compare, start insertion sort, simple describe, read insertion sort Wikipedia page http, examples Come, general idea implementation insertion sort Java public class ListSorter public void insertionSort List list Comparator comparator int int j j, method list integer, last time inner loop, break j j j define class listsorter container sort, type parameter, integer elt2 return", "We'll start with insertion sort, mostly because it is simple to describe and implement. It is not very efficient, but it has some redeeming qualities, as we'll see.\nRather than explain the algorithm here, I suggest you read the insertion sort Wikipedia page at http://thinkdast.com/insertsort, which includes pseudocode and animated examples. Come back when you get the general idea.\nHere's an implementation of insertion sort in Java:\npublic class ListSorter<T> {\n    public void insertionSort(List<T> list, Comparator<T> comparator) {\n        for (int i=1; i < list.size(); i++) {\n            T elt_i = list.get(i);\n            int j = i;\n            while (j > 0) {\n                T elt_j = list.get(j-1);\n                if (comparator.compare(elt_i, elt_j) >= 0) {\n                    break;\n                }\n                list.set(j, elt_j);\n                j--;\n            }\n            list.set(j, elt_i);\n        }\n    }\n}\nI define a class, \"ListSorter\", as a container for sort algorithms. By using the type parameter, \"T\", we can write methods that work on lists containing any object type.\n\"insertionSort\" takes two parameters, a \"List\" of any kind and a \"Comparator\" that knows how to compare type \"T\" objects. It sorts the list \"in place\", which means it modifies the existing list and does not have to allocate any new space.\nThe following example shows how to call this method with a \"List\" of \"Integer\" objects:\n        List<Integer> list = new ArrayList<Integer>( Arrays.asList(3, 5, 1, 4, 2));\n        Comparator<Integer> comparator = new Comparator<Integer>() {\n            @Override public int compare(Integer elt1, Integer elt2) {\n                return elt1.compareTo(elt2);\n            }\n        };\n        ListSorter<Integer> sorter = new ListSorter<Integer>();\n        sorter.insertionSort(list, comparator);\n        System.out.println(list);\n\"insertionSort\" has two nested loops, so you might guess that its runtime is quadratic. In this case, that turns out to be correct, but before you jump to that conclusion, you have to check that the number of times each loop runs is proportional to n, the size of the array.\nThe outer loop iterates from 1 to \"list.size()\", so it is linear in the size of the list, n. The inner loop iterates from \"i\" to 0, so it is also linear in n. Therefore, the total number of times the inner loop runs is quadratic.\nIf you are not sure about that, here's the argument:\n- The first time through, i=1 and the inner loop runs at most once.\n- The second time, i=2 and the inner loop runs at most twice.\n- The last time, i=n-1 and the inner loop runs at most n-1 times.\nSo the total number of times the inner loop runs is the sum of the series 1, 2, ... , n-1, which is n (n-1) / 2. And the leading term of that expression (the one with the highest exponent) is n^2.\nIn the worst case, insertion sort is quadratic. However:\n- If the elements are already sorted, or nearly so, insertion sort is linear. Specifically, if each element is no more than k locations away from where it should be, the inner loop never runs more than k times, and the total runtime is O(kn).\n- Because the implementation is simple, the overhead is low; that is, although the runtime is a n^2, the coefficient of the leading term, a, is probably small.\nSo if we know that the array is nearly sorted, or is not very big, insertion sort might be a good choice. But for large arrays, we can do better. In fact, much better."], ["see , third step also copies, make two new arrays, classify runtime merge sort, terms levels recursion, level Suppose, list contains n elements steps, new arrays, n total time n log, constant factor logarithms order growth algorithms, lists elements, many times, half elements sort", "To classify the runtime of merge sort, it helps to think in terms of levels of recursion and how much work is done on each level. Suppose we start with a list that contains n elements. Here are the steps of the algorithm:\n- Make two new arrays and copy half of the elements into each.\n- Sort the two halves.\n- Merge the halves.\nThe first step copies each of the elements once, so it is linear. The third step also copies each element once, so it is also linear. Now we need to figure out the complexity of step 2. To do that, it helps to looks at a different picture of the computation, which shows the levels of recursion, as in\nAt the top level, we have 1 list with n elements. For simplicity, let's assume n is a power of 2. At the next level there are 2 lists with n/2 elements. Then 4 lists with n/4 elements, and so on until we get to n lists with 1 element.\nOn every level we have a total of n elements. On the way down, we have to split the arrays in half, which takes time proportional to n on every level. On the way back up, we have to merge a total of n elements, which is also linear.\nIf the number of levels is h, the total amount of work for the algorithm is O(nh). So how many levels are there? There are two ways to think about that:\n- How many times do we have to cut n in half to get to 1?\n- Or, how many times do we have to double 1 before we get to n?\nAnother way to ask the second question is \"What power of 2 is n?\"\n2^h = n\nTaking the log_2 of both sides yields\nh = log_2 n\nSo the total time is O(n log n). I didn't bother to write the base of the logarithm because logarithms with different bases differ by a constant factor, so all logarithms are in the same order of growth.\nAlgorithms in O(n log n) are sometimes called \"linearithmic\", but most people just say \"n log n\".\nIt turns out that O(n log n) is the theoretical lower bound for sort algorithms that work by comparing elements to each other. That means there is no \"comparison sort\" whose order of growth is better than n log n.  See http://thinkdast.com/compsort."], ["take linear time, next section, comparison sorts, next section sorts, linear time, n half, lists elements, different picture computation, many times, different bases, next section sorts, linear time", "But as we'll see in the next section, there are non-comparison sorts that take linear time!"], ["chief executive eric schmidt jokingly asked, 2008 united states presidential campaign, even among quadratic sort algorithms, United States Presidential Campaign candidate Barack Obama, perform impromptu algorithm analysis, Google Chief executive Eric Schmidt, efficient way sort, integers Obama, integers obama, perform impromptu algorithm analysis, google chief executive eric schmidt, watch video http obama right bubble sort, quadratic sort algorithms performance good see http, bubble sort, efficient way sort, united states presidential campaign candidate barack obama, simple runtime", "During the 2008 United States Presidential Campaign, candidate Barack Obama was asked to perform an impromptu algorithm analysis when he visited Google. Chief executive Eric Schmidt jokingly asked him for \"the most efficient way to sort a million 32-bit integers.\" Obama had apparently been tipped off, because he quickly replied, \"I think the bubble sort would be the wrong way to go.\" You can watch the video at http://thinkdast.com/obama.\nObama was right: bubble sort is conceptually simple but its runtime is quadratic; and even among quadratic sort algorithms, its performance is not very good.  See http://thinkdast.com/bubble."], ["comparison sort algorithm, bucket contains one element, top row shows, answer Schmidt, radix sort sort algorithm, size elements, stack index cards card, cards Make, pass cards, optional exercise consider, b divide bucket, third letter point bucket, first pass words bucket", "The answer Schmidt was probably looking for is \"radix sort\", which is a **non-comparison** sort algorithm that works if the size of the elements is bounded, like a 32-bit integer or a 20-character string.\nTo see how this works, imagine you have a stack of index cards where each card contains a three-letter word. Here's how you could sort the cards:\n- Make one pass through the cards and divide them into buckets based on the first letter. So words starting with \"a\" should be in one bucket, followed by words starting with \"b\", and so on.\n- Divide each bucket again based on the second letter. So words starting with \"aa\" should be together, followed by words starting with \"ab\", and so on. Of course, not all buckets will be full, but that's OK.\n- Divide each bucket again based on the third letter.\nAt this point each bucket contains one element, and the buckets are sorted in ascending order.\nThe top row shows the unsorted words. The second row shows what the buckets look like after the first pass. The words in each bucket begin with the same letter.\nAfter the second pass, the words in each bucket begin with the same two letters. After the third pass, there can be only one word in each bucket, and the buckets are in order.\nDuring each pass, we iterate through the elements and add them to buckets. As long as the buckets allow addition in constant time, each pass is linear.\nThe number of passes, which I'll call w, depends on the \"width\" of the words, but it doesn't depend on the number of words, n. So the order of growth is O(wn), which is linear in n.\nThere are many variations on radix sort, and many ways to implement each one. You can read more about them at http://thinkdast.com/radix. As an optional exercise, consider writing a version of radix sort."], ["queue takes n log n time, takes log n time, takes log n time, addition radix sort applies things, heap sort, heap sort, large dataset, report Top, element queue root, x nodes, web service handles, queue elements, heap sort", "In addition to radix sort, which applies when the things you want to sort are bounded in size, there is one other special-purpose sorting algorithm you might encounter: bounded heap sort. Bounded heap sort is useful if you are working with a very large dataset and you want to report the \"Top 10\" or \"Top k\" for some value of k much smaller than n.\nFor example, suppose you are monitoring a Web service that handles a billion transactions per day. At the end of each day, you want to report the k biggest transactions (or slowest, or any other superlative). One option is to store all transactions, sort them at the end of the day, and select the top k. That would take time proportional to n log n, and it would be very slow because we probably can't fit a billion transactions in the memory of a single program. We would have to use an \"out of core\" sort algorithm. You can read about external sorting at http://thinkdast.com/extsort.\nUsing a bounded heap, we can do much better! Here's how we will proceed:\n- I'll explain (unbounded) heap sort.\n- You'll implement it.\n- I'll explain bounded heap sort and analyze it.\nTo understand heap sort, you have to understand a heap, which is a data structure similar to a binary search tree (BST). Here are the differences:\n- In a BST, every node, \"x\", has the \"BST property\": all nodes in the left subtree of \"x\" are less than \"x\" and all nodes in the right subtree are greater than \"x\".\n- In a heap, every node, \"x\", has the \"heap property\": all nodes in both subtrees of \"x\" are greater than \"x\".\n- Heaps are like balanced BSTs; when you add or remove elements, they do some extra work to rebalance the tree.  As a result, they can be implemented efficiently using an array of elements.\nThe smallest element in a heap is always at the root, so we can find it in constant time. Adding and removing elements from a heap takes time proportional to the height of the tree h. And because the heap is always balanced, h is proportional to log n.  You can read more about heaps at http://thinkdast.com/heap.\nThe Java \"PriorityQueue\" is implemented using a heap. \"PriorityQueue\" provides the methods specified in the \"Queue\" interface, including \"offer\" and \"poll\":\n- \"offer\": Adds an element to the queue, updating the heap so that every node has the \"heap property\". Takes log n time.\n- \"poll\": Removes the smallest element in the queue from the root and updates the heap. Takes log n time.\nGiven a \"PriorityQueue\", you can easily sort of a collection of n elements like this:\n- Add all elements of the collection to a \"PriorityQueue\" using \"offer\".\n- Remove the elements from the queue using \"poll\" and add them to a \"List\".\nBecause \"poll\" returns the smallest element remaining in the queue, the elements are added to the \"List\" in ascending order. This way of sorting is called heap sort (see http://thinkdast.com/heapsort).\nAdding n elements to the queue takes n log n time. So does removing n elements. So the runtime for heap sort is O(n log n)."], ["method called, heapsort , ant listsortertest, repository book, outline method, heapSort Fill, ant ListSorterTest confirm works, http adding n elements, heap sort n, heap takes log n time given priorityqueue, heapsort fill, ant listsortertest confirm works, repository book, outline method, heap every node heap property takes log n time poll, heap sort", "In the repository for this book, in \"ListSorter.java\" you'll find the outline of a method called \"heapSort\". Fill it in and then run \"ant ListSorterTest\" to confirm that it works."], ["k largest elements like, n log k ),, always run branch 3, heap heap, contain k elements n elements, track k, empty element x Branch, heap full add x, k worst case elements, adding element heap log, contain k elements n elements, element heap, k elements", "A bounded heap is a heap that is limited to contain at most k elements. If you have n elements, you can keep track of the k largest elements like this:\nInitially, the heap is empty.  For each element, \"x\":\n- Branch 1: If the heap is not full, add \"x\" to the heap.\n- Branch 2: If the heap is full, compare \"x\" to the smallest element in the heap. If \"x\" is smaller, it cannot be one of the largest k elements, so you can discard it.\n- Branch 3: If the heap is full and \"x\" is greater than the smallest element in the heap, remove the smallest element from the heap and add \"x\".\nUsing a heap with the smallest element at the top, we can keep track of the largest k elements. Let's analyze the performance of this algorithm. For each element, we perform one of:\n- Branch 1: Adding an element to the heap is O(log k).\n- Branch 2: Finding the smallest element in the heap is O(1).\n- Branch 3: Removing the smallest element is O(log k). Adding \"x\" is also O(log k).\nIn the worst case, if the elements appear in ascending order, we always run Branch 3. In that case, the total time to process n elements is O(n log k), which is linear in n.\nIn \"ListSorter.java\" you'll find the outline of a method called \"topK\" that takes a \"List\", a \"Comparator\", and an integer k. It should return the k largest elements in the \"List\" in ascending order. Fill it in and then run \"ant ListSorterTest\" to confirm that it works."], ["uses less space might make better use, run time often increases dramatically, see , lot runtime analysis many algorithms, space example, merge sort, copies data implementation total amount space, n log, contrast insertion sort copy data sorts elements, clever implementation, large datasets space important example dataset fit memory, space example, space many applications", "Until now we have talked a lot about runtime analysis, but for many algorithms we are also concerned about space. For example, one of the drawbacks of merge sort is that it makes copies of the data. In our implementation, the total amount of space it allocates is O(n log n). With a more clever implementation, you can get the space requirement down to O(n).\nIn contrast, insertion sort doesn't copy the data because it sorts the elements in place. It uses temporary variables to compare two elements at a time, and it uses a few other local variables. But its space use doesn't depend on n.\nOur implementation of heap sort creates a new \"PriorityQueue\" to store the elements, so the space is O(n); but if you are allowed to sort the list in place, you can run heap sort with O(1) space.\nOne of the benefits of the bounded heap algorithm you just implemented is that it only needs space proportional to k (the number of elements we want to keep), and k is often much smaller than n.\nSoftware developers tend to pay more attention to runtime than space, and for many applications, that's appropriate. But for large datasets, space can be just as important or more so. For example:\n- If a dataset doesn't fit into the memory of one program, the run time often increases dramatically, or it might not run at all. If you choose an algorithm that needs less space, and that makes it possible to fit the computation into memory, it might run much faster. In the same vein, a program that uses less space might make better use of CPU caches and run faster (see http://thinkdast.com/cache).\n- On a server that runs many programs at the same time, if you can reduce the space needed for each program, you might be able to run more programs on the same server, which reduces hardware and energy costs.\nSo those are some reasons you should know at least a little bit about the space needs of algorithms."]]}